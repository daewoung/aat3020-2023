{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jdasam/aat3020-2023/blob/main/notebooks/4_Machine_translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80c87a7a",
      "metadata": {
        "id": "80c87a7a"
      },
      "source": [
        "# Machine Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "Y6OfzjCACzNM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6OfzjCACzNM",
        "outputId": "2c858787-d5c5-4232-8d25-859740ff8886"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n"
          ]
        }
      ],
      "source": [
        "# install HuggingFace\n",
        "!pip install transformers tokenizers\n",
        "\n",
        "# If you are not using Colab, install below to read xlsx file\n",
        "# !pip install openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "518b10da",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "518b10da",
        "outputId": "d0f0b8f2-b766-48fb-bc41-a69b0e7f7cf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1GMFWREWBVcD5vJdwNFadHmzVStclcyKf\n",
            "To: /content/nia-aihub-korean-english-txt.zip\n",
            "100% 449M/449M [00:05<00:00, 74.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Download dataset (originally from NIA AI-Hub)\n",
        "'''\n",
        "\n",
        "# !gdown 1CpsqOuuuB3I_PG5DbuqH1ssCFVerU46g\n",
        "# !unzip -q nia-aihub-korean-english.zip\n",
        "\n",
        "!gdown 1GMFWREWBVcD5vJdwNFadHmzVStclcyKf\n",
        "!unzip -q nia-aihub-korean-english-txt.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dWNSKjhdJ1jM",
      "metadata": {
        "id": "dWNSKjhdJ1jM"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca202b9d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "ca202b9d",
        "outputId": "e415e08d-0eca-4d52-ecc3-414e60d7cff6",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\ndataset_dir = Path('nia_korean_english')\\ndata_list = sorted(list(dataset_dir.glob('*.xlsx')))\\nfor path in data_list:\\n  df = pd.read_excel(path)\\n  kor_text_path = path.parent / (path.stem+'_kor.txt') \\n  eng_text_path = path.parent / (path.stem+'_eng.txt') \\n  with open(kor_text_path, 'w', encoding='utf8') as f:\\n      f.write('\\n'.join(df['원문']))\\n  with open(eng_text_path, 'w', encoding='utf8') as f:\\n      f.write('\\n'.join(df['번역문']))\\n\""
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# You don't have to run this\n",
        "'''\n",
        "dataset_dir = Path('nia_korean_english')\n",
        "data_list = sorted(list(dataset_dir.glob('*.xlsx')))\n",
        "for path in data_list:\n",
        "  df = pd.read_excel(path)\n",
        "  kor_text_path = path.parent / (path.stem+'_kor.txt') \n",
        "  eng_text_path = path.parent / (path.stem+'_eng.txt') \n",
        "  with open(kor_text_path, 'w', encoding='utf8') as f:\n",
        "      f.write('\\n'.join(df['원문']))\n",
        "  with open(eng_text_path, 'w', encoding='utf8') as f:\n",
        "      f.write('\\n'.join(df['번역문']))\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fbc2c73",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fbc2c73",
        "outputId": "f948c9f0-78dd-4ce2-b669-c2ed92bbc6e0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[PosixPath('nia_korean_english/1_구어체(1).xlsx'),\n",
              " PosixPath('nia_korean_english/1_구어체(2).xlsx'),\n",
              " PosixPath('nia_korean_english/2_대화체.xlsx'),\n",
              " PosixPath('nia_korean_english/3_문어체_뉴스(1)_200226.xlsx'),\n",
              " PosixPath('nia_korean_english/3_문어체_뉴스(2).xlsx'),\n",
              " PosixPath('nia_korean_english/3_문어체_뉴스(3).xlsx'),\n",
              " PosixPath('nia_korean_english/3_문어체_뉴스(4).xlsx'),\n",
              " PosixPath('nia_korean_english/4_문어체_한국문화.xlsx'),\n",
              " PosixPath('nia_korean_english/5_문어체_조례.xlsx'),\n",
              " PosixPath('nia_korean_english/6_문어체_지자체웹사이트.xlsx')]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_dir = Path('nia_korean_english')\n",
        "data_list = sorted(list(dataset_dir.glob('*.xlsx')))\n",
        "\n",
        "data_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c767468",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "6c767468",
        "outputId": "be64a757-971e-49f5-e0eb-458aeaedaf6f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/teo/.local/share/virtualenvs/aat3020-2023-E1AG9i7b/lib/python3.8/site-packages/openpyxl/styles/stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
            "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
            "/home/teo/.local/share/virtualenvs/aat3020-2023-E1AG9i7b/lib/python3.8/site-packages/openpyxl/styles/stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
            "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
            "/home/teo/.local/share/virtualenvs/aat3020-2023-E1AG9i7b/lib/python3.8/site-packages/openpyxl/styles/stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
            "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
            "/home/teo/.local/share/virtualenvs/aat3020-2023-E1AG9i7b/lib/python3.8/site-packages/openpyxl/styles/stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
            "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "dfs = [pd.read_excel(path) for path in data_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e9f316f",
      "metadata": {
        "id": "2e9f316f"
      },
      "outputs": [],
      "source": [
        "df = pd.concat(dfs, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gKyXWsJjjrLR",
      "metadata": {
        "id": "gKyXWsJjjrLR"
      },
      "outputs": [],
      "source": [
        "with open(\"nia_korean_english/1_구어체(1)_kor.txt\", 'w', encoding='utf8') as f:\n",
        "    f.write('\\n'.join(df['원문']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff8b8786",
      "metadata": {
        "id": "ff8b8786"
      },
      "outputs": [],
      "source": [
        "df['원문'][10000:10050], df['번역문'][10000:10050]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3df6e584",
      "metadata": {
        "id": "3df6e584"
      },
      "source": [
        "## Huggingface Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c6f1769",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c6f1769",
        "outputId": "f1f0dfad-d93b-482d-8c92-d46bc4177580"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['hugging_eng_32000/vocab.txt']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tokenizers import BertWordPieceTokenizer\n",
        "tokenizer = BertWordPieceTokenizer(strip_accents=False, lowercase=False)\n",
        "\n",
        "num_files = len(data_list)\n",
        "\n",
        "corpus_file   =  [str(path.parent / (path.stem + '_kor.txt')) for path in data_list[:num_files]]\n",
        "# output_dir   = Path('hugging_kor_%d'%(vocab_size))\n",
        "en_corpus_file   =  [str(path.parent / (path.stem + '_eng.txt')) for path in data_list[:num_files]]\n",
        "# output_dir   = Path('hugging_eng_%d'%(vocab_size))\n",
        "\n",
        "vocab_size    = 32000  # Number of maximum size of the vocabulary\n",
        "limit_alphabet= 6000   \n",
        "output_dir    = Path('hugging_kor_%d'%(vocab_size))\n",
        "en_output_dir = Path('hugging_eng_%d'%(vocab_size))\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "en_output_dir.mkdir(exist_ok=True)\n",
        "min_frequency = 5 \n",
        "\n",
        "tokenizer.train(files=corpus_file,\n",
        "               vocab_size=vocab_size,\n",
        "               min_frequency=min_frequency,\n",
        "               limit_alphabet=limit_alphabet, \n",
        "               show_progress=True)\n",
        "\n",
        "tokenizer.save_model(str(output_dir))\n",
        "\n",
        "en_tokenizer = BertWordPieceTokenizer(strip_accents=False, lowercase=False)\n",
        "en_tokenizer.train(files=en_corpus_file,\n",
        "                vocab_size=vocab_size,\n",
        "                min_frequency=min_frequency,\n",
        "                limit_alphabet=limit_alphabet,\n",
        "                show_progress=True)\n",
        "en_tokenizer.save_model(str(en_output_dir))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "410eb97a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "410eb97a",
        "outputId": "b196db6e-e9e5-409c-cad4-2added16d3e1",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1977c52dec34>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                                                        lowercase=False) \n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtokenized_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_src\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'원문'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'원문'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizerFast\n",
        "\n",
        "tokenizer_src = BertTokenizerFast.from_pretrained('hugging_kor_32000',\n",
        "                                                       strip_accents=False,\n",
        "                                                       lowercase=False) \n",
        "tokenizer_tgt = BertTokenizerFast.from_pretrained('hugging_eng_32000',\n",
        "                                                       strip_accents=False,\n",
        "                                                       lowercase=False) \n",
        "\n",
        "tokenized_data = tokenizer_src(df['원문'].iloc[10])\n",
        "print(df['원문'].iloc[10])\n",
        "print(tokenizer_src.decode(tokenized_data['input_ids']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q9GJ7cifk5tg",
      "metadata": {
        "id": "q9GJ7cifk5tg"
      },
      "outputs": [],
      "source": [
        "tokenized_data['input_ids']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "340f55c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "340f55c0",
        "outputId": "15b66d7d-e619-4672-dc5c-038218608974"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 6481, 25257, 8055, 4330, 2537, 10839, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "tokenized_ids = tokenizer_src(\"나는 서강대학교에 다닙니다\")['input_ids']\n",
        "tokenized_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5b3ea831",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "5b3ea831",
        "outputId": "04621101-cf34-4b8d-e1b4-a747c445e8c3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] 나는 서강대학교에 다닙니다 [SEP]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "tokenizer_src.decode(tokenized_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26964dcb",
      "metadata": {
        "id": "26964dcb"
      },
      "source": [
        "## Divide Train / Validate/ Test Set\n",
        "- using `np.random.choice`\n",
        "    - To always get the same random shuffling result, you have to use `np.random.seed()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B2zJgvXPlh95",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "B2zJgvXPlh95",
        "outputId": "33f24480-2a88-418e-d95d-b01f5b6050d6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d659cf52-06c5-4062-a100-2c32cbe09de2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>지자체</th>\n",
              "      <th>원문</th>\n",
              "      <th>번역문</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>261811</td>\n",
              "      <td>경기도</td>\n",
              "      <td>\"경기도가 말산업 육성을 위해 총예산 245,193천원으로 2013년 경기도 용인시...</td>\n",
              "      <td>\"The Gyeonggi provincial government announced ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>409852</td>\n",
              "      <td>경기도</td>\n",
              "      <td>\"경기도가 주최하고 경기FTA활용지원센터와 코트라가 주관한 이번 시장개척단은 지난 ...</td>\n",
              "      <td>\"Organized by Gyeonggi provincial government a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>352671</td>\n",
              "      <td>경기도</td>\n",
              "      <td>\"경기도가 주최하고 경기도비정규직지원센터가 주관한 이번 교육은 공공 부문이 직·간접...</td>\n",
              "      <td>\"Organized by Gyeonggi provincial government a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>308191</td>\n",
              "      <td>경기도</td>\n",
              "      <td>\"경기도가 주최하고 사단법인 한국장애인복지시설협회 경기도협회(대표자:정권)에서 주관...</td>\n",
              "      <td>\"Organized by Gyeonggi provincial government a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>185424</td>\n",
              "      <td>경기도</td>\n",
              "      <td>\"경기도가 주최하고 인구보건복지협회 경기지회, 아이낳기좋은세상 경기운동본부가 주관하...</td>\n",
              "      <td>\"Hosted by Gyeonggi provincial government, org...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100082</th>\n",
              "      <td>73961</td>\n",
              "      <td>서울시 중구</td>\n",
              "      <td>힐튼호텔 건너편에서 회현동주민센터로 내려가는 길에는 칙칙한 회색의 석축옹벽이 이어져...</td>\n",
              "      <td>Across from the Hilton Hotel, the path to Hoeh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100083</th>\n",
              "      <td>462950</td>\n",
              "      <td>경기도</td>\n",
              "      <td>힘든 분들이 많이 계시지만 조금이나마 어렵지 않도록 도움이 될 수 있도록 저희가 최...</td>\n",
              "      <td>There are a lot of people who are having a har...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100084</th>\n",
              "      <td>333006</td>\n",
              "      <td>경기도</td>\n",
              "      <td>힘든 역사 속에서 대한민국을 불과 50여년 만에 빛나는 나라로 만들고 업적을 만들 ...</td>\n",
              "      <td>It was possible to make the Republic of Korea ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100085</th>\n",
              "      <td>454399</td>\n",
              "      <td>경기도</td>\n",
              "      <td>힘든 일을 하는 데는 무엇보다 정부가 큰 관심을 갖고 있다는 자부심을 갖도록 해야 한다.</td>\n",
              "      <td>As for doing hard work, most of all, they shou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100086</th>\n",
              "      <td>507679</td>\n",
              "      <td>경기도</td>\n",
              "      <td>힘을 합쳐 세계 일류통일강국으로 대한민국을 우뚝 세우는 역할을 이 자리의 모든 지도...</td>\n",
              "      <td>I would like to thank all the leaders here for...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100087 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d659cf52-06c5-4062-a100-2c32cbe09de2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d659cf52-06c5-4062-a100-2c32cbe09de2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d659cf52-06c5-4062-a100-2c32cbe09de2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "            ID     지자체                                                 원문  \\\n",
              "0       261811     경기도  \"경기도가 말산업 육성을 위해 총예산 245,193천원으로 2013년 경기도 용인시...   \n",
              "1       409852     경기도  \"경기도가 주최하고 경기FTA활용지원센터와 코트라가 주관한 이번 시장개척단은 지난 ...   \n",
              "2       352671     경기도  \"경기도가 주최하고 경기도비정규직지원센터가 주관한 이번 교육은 공공 부문이 직·간접...   \n",
              "3       308191     경기도  \"경기도가 주최하고 사단법인 한국장애인복지시설협회 경기도협회(대표자:정권)에서 주관...   \n",
              "4       185424     경기도  \"경기도가 주최하고 인구보건복지협회 경기지회, 아이낳기좋은세상 경기운동본부가 주관하...   \n",
              "...        ...     ...                                                ...   \n",
              "100082   73961  서울시 중구  힐튼호텔 건너편에서 회현동주민센터로 내려가는 길에는 칙칙한 회색의 석축옹벽이 이어져...   \n",
              "100083  462950     경기도  힘든 분들이 많이 계시지만 조금이나마 어렵지 않도록 도움이 될 수 있도록 저희가 최...   \n",
              "100084  333006     경기도  힘든 역사 속에서 대한민국을 불과 50여년 만에 빛나는 나라로 만들고 업적을 만들 ...   \n",
              "100085  454399     경기도  힘든 일을 하는 데는 무엇보다 정부가 큰 관심을 갖고 있다는 자부심을 갖도록 해야 한다.   \n",
              "100086  507679     경기도  힘을 합쳐 세계 일류통일강국으로 대한민국을 우뚝 세우는 역할을 이 자리의 모든 지도...   \n",
              "\n",
              "                                                      번역문  \n",
              "0       \"The Gyeonggi provincial government announced ...  \n",
              "1       \"Organized by Gyeonggi provincial government a...  \n",
              "2       \"Organized by Gyeonggi provincial government a...  \n",
              "3       \"Organized by Gyeonggi provincial government a...  \n",
              "4       \"Hosted by Gyeonggi provincial government, org...  \n",
              "...                                                   ...  \n",
              "100082  Across from the Hilton Hotel, the path to Hoeh...  \n",
              "100083  There are a lot of people who are having a har...  \n",
              "100084  It was possible to make the Republic of Korea ...  \n",
              "100085  As for doing hard work, most of all, they shou...  \n",
              "100086  I would like to thank all the leaders here for...  \n",
              "\n",
              "[100087 rows x 4 columns]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "boOZlijtlpgv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boOZlijtlpgv",
        "outputId": "9ce04f9d-ee41-43a0-9463-c1d444496e01"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1602418"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NdDy0EHNlv60",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdDy0EHNlv60",
        "outputId": "3180be90-6287-4e22-b28f-887e43166aab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ID                                                507679\n",
              "지자체                                                  경기도\n",
              "원문     힘을 합쳐 세계 일류통일강국으로 대한민국을 우뚝 세우는 역할을 이 자리의 모든 지도...\n",
              "번역문    I would like to thank all the leaders here for...\n",
              "Name: 100086, dtype: object"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.iloc[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "AGA1i06oMD1N",
      "metadata": {
        "id": "AGA1i06oMD1N"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_excel(\"/content/nia_korean_english/1_구어체(1).xlsx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba4a138a",
      "metadata": {
        "id": "ba4a138a"
      },
      "source": [
        "## Define Dataset\n",
        "- Each datasample has to return source sentence and target sentence\n",
        "- You need a Tokenizer to get the tokenized result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b088f662",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b088f662",
        "outputId": "3eae71a5-3bbf-462e-e13d-4f2fcb1e2c81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([    2,    11,    70,  4665,  5209, 13306,    71, 12901,  9565, 12435,\n",
            "           11,  3546, 14567,  4325,  8934,  8407,  7400,  4154,  3252,  6420,\n",
            "        12985,  4996,  3397,  6461,    18,     3])\n",
            "tensor([    2, 26268, 23067,    11,  1056,    69, 23067,  2803,  1067,  5155,\n",
            "         1117,  1042,  2405,  4024,  5520,  1039,  1023, 26268,    18])\n",
            "tensor([26268, 23067,    11,  1056,    69, 23067,  2803,  1067,  5155,  1117,\n",
            "         1042,  2405,  4024,  5520,  1039,  1023, 26268,    18,     3])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "class Dataset:\n",
        "  def __init__(self, df, src_tokenizer, tgt_tokenizer):\n",
        "    self.data = df\n",
        "    self.src_tokenizer = src_tokenizer\n",
        "    self.tgt_tokenizer = tgt_tokenizer\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    selected_row = self.data.iloc[idx]\n",
        "    source = selected_row['원문']\n",
        "    target = selected_row['번역문']\n",
        "\n",
        "    source_enc = self.src_tokenizer(source)['input_ids']\n",
        "    target_enc = self.tgt_tokenizer(target)['input_ids']\n",
        "\n",
        "    return torch.LongTensor(source_enc), torch.LongTensor(target_enc[:-1]), torch.LongTensor(target_enc[1:])\n",
        "  \n",
        "dataset = Dataset(df, tokenizer_src, tokenizer_tgt)\n",
        "\n",
        "source, target, shifted_target = dataset[0]\n",
        "print(source)\n",
        "print(target)\n",
        "print(shifted_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kMaE7cK2EurA",
      "metadata": {
        "id": "kMaE7cK2EurA"
      },
      "source": [
        "## Split Dataset\n",
        "- using ``torch.utils.data.random_split(dataset)``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "Lb8fmwGbEhU1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lb8fmwGbEhU1",
        "outputId": "561b6db8-50e4-4e85-9dba-15441a1ff7c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "160000 20000 20000\n"
          ]
        }
      ],
      "source": [
        "num_data = len(dataset)\n",
        "# num_data\n",
        "num_train = int(num_data * 0.8)\n",
        "num_valid = int(num_data * 0.1)\n",
        "# num_test = int(num_data * 0.1)\n",
        "num_test = num_data - (num_train + num_valid)\n",
        "\n",
        "train_set, valid_set, test_set = torch.utils.data.random_split(dataset, [num_train, num_valid, num_test])\n",
        "print(len(train_set), len(valid_set), len(test_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "76933a49",
      "metadata": {
        "id": "76933a49"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "oTmu4Yy_N7vt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "oTmu4Yy_N7vt",
        "outputId": "6ca2beb7-3087-4271-ccc6-aa6984b20ded"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-6f710a1b7bb1>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Handle `CustomType` automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \"\"\"\n\u001b[0;32m--> 264\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcollate_fn_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcollate_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typed_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [8] at entry 0 and [17] at entry 1"
          ]
        }
      ],
      "source": [
        "train_loader = DataLoader(train_set, batch_size=4, shuffle=True)\n",
        "\n",
        "batch = next(iter(train_loader))\n",
        "batch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4a9f0ec",
      "metadata": {
        "id": "f4a9f0ec"
      },
      "source": [
        "## Define Collate function\n",
        "- After implementing Dataset, we have to declare DataLoader that groups several training samples as a single batch\n",
        "- However, we cannot batchify the melodies in straightforward way, because the length of each melody is different\n",
        "- In this problem, you will learn about how to handle sequences of different length as a batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "456ff9f2",
      "metadata": {
        "id": "456ff9f2"
      },
      "outputs": [],
      "source": [
        "\n",
        "'''\n",
        "This cell will make error, because the length of each sample is different to each other\n",
        "'''\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=8)\n",
        "batch = next(iter(train_loader))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "afc533f7",
      "metadata": {
        "id": "afc533f7",
        "outputId": "b13b1a4f-2d3a-4fc0-8307-8ca21ca2208a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([tensor([    2,  7110,  7249,  6455, 14377, 18637, 15237,  8251, 25509,  4337,\n",
              "          18905, 13630,  9107,  9135,    18,     3]),\n",
              "  tensor([    2,  6832,  9081, 15420, 27164, 11105, 17601,  6412,  3160,  3252,\n",
              "           6414,    18,     3]),\n",
              "  tensor([    2,  3600,  6741, 12614,  6678, 28240,  3191,  4657,  4336, 12553,\n",
              "          15888,    18,     3]),\n",
              "  tensor([    2,  6730, 15523, 28173,  6625,  7334,  6507,    18,     3]),\n",
              "  tensor([    2,  8543,  7009, 16955, 23111,  7637, 22306,    18,     3]),\n",
              "  tensor([    2,  2284, 19570,  2426, 20003,  4325,  8756,  4421, 10833,  8419,\n",
              "             18,     3]),\n",
              "  tensor([    2, 30025,  7175,  2714,  4563, 12935,  6749,  7323, 20796,  4331,\n",
              "           4173,  4417,  4694,    16,  3967,  4540,  4770,  7576, 15011,  7616,\n",
              "          30310, 16615,  7280,  4421, 11548,  9583,  7023,    18,     3]),\n",
              "  tensor([    2,  6481, 26684,  4330,  6642,  9186,  8398, 11003,  8827,  3301,\n",
              "          12469, 10145,  4387,  6582,  3309,  4352, 28808,    18,     3])],\n",
              " [tensor([    2,  1287,  1109, 13814,  1042,  1065,  4666,  1061,    69,  1434,\n",
              "             17,  2323,  8814,  1562,  1169, 18279,  2481,  1061,  4393,  2032,\n",
              "             18]),\n",
              "  tensor([   2, 1089,   11,   87, 1161, 1863, 1042, 1652, 1089, 7485, 1693, 1287,\n",
              "          1195, 2834, 1089, 1039, 1023, 1521,   18]),\n",
              "  tensor([    2,  1393,  6185,  8166,  1109, 24398,  3651,  1407,    77,    11,\n",
              "             81,  6278,    18]),\n",
              "  tensor([   2, 1287, 1159,   69, 1911, 1034, 1023, 1889, 7332,   18]),\n",
              "  tensor([   2,   77, 3134, 1067,   77, 1314, 1589, 1042, 1159, 1333, 6200,   18]),\n",
              "  tensor([   2, 1067, 1352, 1106, 1393, 3170,   11, 8849, 1352,   18]),\n",
              "  tensor([    2, 30352,   781,  1056,    69,  7960,  1369,  1606, 13185,  1592,\n",
              "           1099,  1837,  8210,    16, 22125,  1045, 31295,  5034,  1816,  1068,\n",
              "           2319, 23122,    18]),\n",
              "  tensor([   2,   77, 1612, 1042, 1406, 1042, 1023, 8082, 8270, 1634, 1407,   77,\n",
              "          1612, 1042, 1159, 1674, 2468, 1251, 8082,   18])],\n",
              " [tensor([ 1287,  1109, 13814,  1042,  1065,  4666,  1061,    69,  1434,    17,\n",
              "           2323,  8814,  1562,  1169, 18279,  2481,  1061,  4393,  2032,    18,\n",
              "              3]),\n",
              "  tensor([1089,   11,   87, 1161, 1863, 1042, 1652, 1089, 7485, 1693, 1287, 1195,\n",
              "          2834, 1089, 1039, 1023, 1521,   18,    3]),\n",
              "  tensor([ 1393,  6185,  8166,  1109, 24398,  3651,  1407,    77,    11,    81,\n",
              "           6278,    18,     3]),\n",
              "  tensor([1287, 1159,   69, 1911, 1034, 1023, 1889, 7332,   18,    3]),\n",
              "  tensor([  77, 3134, 1067,   77, 1314, 1589, 1042, 1159, 1333, 6200,   18,    3]),\n",
              "  tensor([1067, 1352, 1106, 1393, 3170,   11, 8849, 1352,   18,    3]),\n",
              "  tensor([30352,   781,  1056,    69,  7960,  1369,  1606, 13185,  1592,  1099,\n",
              "           1837,  8210,    16, 22125,  1045, 31295,  5034,  1816,  1068,  2319,\n",
              "          23122,    18,     3]),\n",
              "  tensor([  77, 1612, 1042, 1406, 1042, 1023, 8082, 8270, 1634, 1407,   77, 1612,\n",
              "          1042, 1159, 1674, 2468, 1251, 8082,   18,    3])])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "'''\n",
        "To handle that problem, you have to make your collate function \n",
        "'''\n",
        "def your_collate_function(raw_batch):\n",
        "  '''\n",
        "  You can make your own function to handle the batch\n",
        "  '''\n",
        "  src = [x[0] for x in raw_batch]\n",
        "  tgt = [x[1] for x in raw_batch]\n",
        "  shifted_tgt = [x[2] for x in raw_batch]\n",
        "  \n",
        "  return src, tgt, shifted_tgt # This returns the first sample of each batch. So it will avoid the error, but it doesn't do proper batchifying\n",
        "\n",
        "batch_size = 8\n",
        "raw_batch = [train_set[i] for i in range(batch_size)] # This is the input for the collate function\n",
        "batch = your_collate_function(raw_batch)\n",
        "\n",
        "'''\n",
        "This is what the 'collate_fn' does in DataLoader\n",
        "'''\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, collate_fn=your_collate_function)\n",
        "batch_by_loader = next(iter(train_loader))\n",
        "batch_by_loader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_batch = [train_set[0], train_set[1], train_set[2]] # this is a sample input for collate function\n",
        "raw_batch"
      ],
      "metadata": {
        "id": "XO7cn02WqvFx",
        "outputId": "6a94cd55-814e-48e6-c251-6a87ff0e9a8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "XO7cn02WqvFx",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(tensor([    2,  7110,  7249,  6455, 14377, 18637, 15237,  8251, 25509,  4337,\n",
              "          18905, 13630,  9107,  9135,    18,     3]),\n",
              "  tensor([    2,  1287,  1109, 13814,  1042,  1065,  4666,  1061,    69,  1434,\n",
              "             17,  2323,  8814,  1562,  1169, 18279,  2481,  1061,  4393,  2032,\n",
              "             18]),\n",
              "  tensor([ 1287,  1109, 13814,  1042,  1065,  4666,  1061,    69,  1434,    17,\n",
              "           2323,  8814,  1562,  1169, 18279,  2481,  1061,  4393,  2032,    18,\n",
              "              3])),\n",
              " (tensor([    2,  6832,  9081, 15420, 27164, 11105, 17601,  6412,  3160,  3252,\n",
              "           6414,    18,     3]),\n",
              "  tensor([   2, 1089,   11,   87, 1161, 1863, 1042, 1652, 1089, 7485, 1693, 1287,\n",
              "          1195, 2834, 1089, 1039, 1023, 1521,   18]),\n",
              "  tensor([1089,   11,   87, 1161, 1863, 1042, 1652, 1089, 7485, 1693, 1287, 1195,\n",
              "          2834, 1089, 1039, 1023, 1521,   18,    3])),\n",
              " (tensor([    2,  3600,  6741, 12614,  6678, 28240,  3191,  4657,  4336, 12553,\n",
              "          15888,    18,     3]),\n",
              "  tensor([    2,  1393,  6185,  8166,  1109, 24398,  3651,  1407,    77,    11,\n",
              "             81,  6278,    18]),\n",
              "  tensor([ 1393,  6185,  8166,  1109, 24398,  3651,  1407,    77,    11,    81,\n",
              "           6278,    18,     3]))]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_by_loader # [train_set[0], train_set[1], train_set[2]]"
      ],
      "metadata": {
        "id": "2iG_wY4Zqo-W",
        "outputId": "fc67856e-dabb-4997-fcbc-275da43e8ea0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2iG_wY4Zqo-W",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(tensor([    2,  7110,  7249,  6455, 14377, 18637, 15237,  8251, 25509,  4337,\n",
              "          18905, 13630,  9107,  9135,    18,     3]),\n",
              "  tensor([    2,  1287,  1109, 13814,  1042,  1065,  4666,  1061,    69,  1434,\n",
              "             17,  2323,  8814,  1562,  1169, 18279,  2481,  1061,  4393,  2032,\n",
              "             18]),\n",
              "  tensor([ 1287,  1109, 13814,  1042,  1065,  4666,  1061,    69,  1434,    17,\n",
              "           2323,  8814,  1562,  1169, 18279,  2481,  1061,  4393,  2032,    18,\n",
              "              3])),\n",
              " (tensor([    2,  6832,  9081, 15420, 27164, 11105, 17601,  6412,  3160,  3252,\n",
              "           6414,    18,     3]),\n",
              "  tensor([   2, 1089,   11,   87, 1161, 1863, 1042, 1652, 1089, 7485, 1693, 1287,\n",
              "          1195, 2834, 1089, 1039, 1023, 1521,   18]),\n",
              "  tensor([1089,   11,   87, 1161, 1863, 1042, 1652, 1089, 7485, 1693, 1287, 1195,\n",
              "          2834, 1089, 1039, 1023, 1521,   18,    3])),\n",
              " (tensor([    2,  3600,  6741, 12614,  6678, 28240,  3191,  4657,  4336, 12553,\n",
              "          15888,    18,     3]),\n",
              "  tensor([    2,  1393,  6185,  8166,  1109, 24398,  3651,  1407,    77,    11,\n",
              "             81,  6278,    18]),\n",
              "  tensor([ 1393,  6185,  8166,  1109, 24398,  3651,  1407,    77,    11,    81,\n",
              "           6278,    18,     3])),\n",
              " (tensor([    2,  6730, 15523, 28173,  6625,  7334,  6507,    18,     3]),\n",
              "  tensor([   2, 1287, 1159,   69, 1911, 1034, 1023, 1889, 7332,   18]),\n",
              "  tensor([1287, 1159,   69, 1911, 1034, 1023, 1889, 7332,   18,    3])),\n",
              " (tensor([    2,  8543,  7009, 16955, 23111,  7637, 22306,    18,     3]),\n",
              "  tensor([   2,   77, 3134, 1067,   77, 1314, 1589, 1042, 1159, 1333, 6200,   18]),\n",
              "  tensor([  77, 3134, 1067,   77, 1314, 1589, 1042, 1159, 1333, 6200,   18,    3])),\n",
              " (tensor([    2,  2284, 19570,  2426, 20003,  4325,  8756,  4421, 10833,  8419,\n",
              "             18,     3]),\n",
              "  tensor([   2, 1067, 1352, 1106, 1393, 3170,   11, 8849, 1352,   18]),\n",
              "  tensor([1067, 1352, 1106, 1393, 3170,   11, 8849, 1352,   18,    3])),\n",
              " (tensor([    2, 30025,  7175,  2714,  4563, 12935,  6749,  7323, 20796,  4331,\n",
              "           4173,  4417,  4694,    16,  3967,  4540,  4770,  7576, 15011,  7616,\n",
              "          30310, 16615,  7280,  4421, 11548,  9583,  7023,    18,     3]),\n",
              "  tensor([    2, 30352,   781,  1056,    69,  7960,  1369,  1606, 13185,  1592,\n",
              "           1099,  1837,  8210,    16, 22125,  1045, 31295,  5034,  1816,  1068,\n",
              "           2319, 23122,    18]),\n",
              "  tensor([30352,   781,  1056,    69,  7960,  1369,  1606, 13185,  1592,  1099,\n",
              "           1837,  8210,    16, 22125,  1045, 31295,  5034,  1816,  1068,  2319,\n",
              "          23122,    18,     3])),\n",
              " (tensor([    2,  6481, 26684,  4330,  6642,  9186,  8398, 11003,  8827,  3301,\n",
              "          12469, 10145,  4387,  6582,  3309,  4352, 28808,    18,     3]),\n",
              "  tensor([   2,   77, 1612, 1042, 1406, 1042, 1023, 8082, 8270, 1634, 1407,   77,\n",
              "          1612, 1042, 1159, 1674, 2468, 1251, 8082,   18]),\n",
              "  tensor([  77, 1612, 1042, 1406, 1042, 1023, 8082, 8270, 1634, 1407,   77, 1612,\n",
              "          1042, 1159, 1674, 2468, 1251, 8082,   18,    3]))]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22db0716",
      "metadata": {
        "id": "22db0716"
      },
      "source": [
        "#### Pad Sequence and Pack Sequence\n",
        "In PyTorch, there are two ways to batchify a group of sequence with different length.\n",
        "- `torch.nn.utils.rnn.pad_sequence`\n",
        "    - This function takes list of tensors with different length and return padded sequence\n",
        "    - Padding is adding some constant number as a PAD token to match the length of short sequence to the maximum length\n",
        "        - e.g. If there are sequence of length (3,7,4), we can add 4 zeros to sequence with length 3, 3 zeros to sequence with length 4 to make them length 7\n",
        "    - In default, we use 0 for padding (zero padding)\n",
        "    - The result \n",
        "- `torch.nn.utils.rnn.pack_sequence`\n",
        "    - pad_sequence "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c8cce88",
      "metadata": {
        "id": "1c8cce88"
      },
      "source": [
        "Cells below show the example of `pad_sequence`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "fe229f85",
      "metadata": {
        "id": "fe229f85",
        "outputId": "b9e9292f-63b7-4f09-b518-82e8b812f250",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 3., 27., 15.],\n",
              "        [ 2., 26., 14.],\n",
              "        [ 1., 25., 13.],\n",
              "        [ 0., 24., 12.],\n",
              "        [ 0., 23., 11.],\n",
              "        [ 0., 22., 10.],\n",
              "        [ 0., 21.,  0.],\n",
              "        [ 0., 20.,  0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence, pack_sequence, PackedSequence\n",
        "short = torch.arange(3, -1, -1).float() # [3, 2, 1, 0]\n",
        "long = torch.arange(27,19, -1).float()\n",
        "middle = torch.arange(15,9, -1).float()\n",
        "\n",
        "pad_sequence([short, long, middle], batch_first=False)  # T x N "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "0b2a0920",
      "metadata": {
        "id": "0b2a0920",
        "outputId": "1e5cc308-9354-4336-8bf0-7dc50ed72061",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 3.,  2.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
              "        [27., 26., 25., 24., 23., 22., 21., 20.],\n",
              "        [15., 14., 13., 12., 11., 10.,  0.,  0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# Default value of batch_first in pad_sequence is False.\n",
        "# So you have to always be careful not to miss batch_first=True in pad_sequence, if you use batch_first=True for your RNN layer.\n",
        "pad_sequence([short, long, middle], batch_first=True)  # N x T "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fb5e0dd",
      "metadata": {
        "id": "4fb5e0dd"
      },
      "source": [
        "1) However, the problem is that you can't figure out whether the 0 at the end of each sequence is a padded one, or was included in the original sequence\n",
        "- e.g. `[2, 3, 4, 3, 0]` becomes `[ 2,  3,  4,  5,  0,  0,  0]`. Now we don't know how many zeros were added for padding\n",
        "\n",
        "2) Also, if you run RNN for this padded sequence, RNN will calculate for the padded part also.\n",
        "- RNN doesn't know whether it is padded data, or existing data\n",
        "- This makes computation slower\n",
        "\n",
        "3) If you want to use bi-directional, which also reads the sequence from backward, paddings can make the result different.\n",
        "\n",
        "To solve this issue, we use PackedSequence, by using `pack_sequence`/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "69086d43",
      "metadata": {
        "id": "69086d43",
        "outputId": "0091d96c-8bc5-4323-9a23-78935a457cb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PackedSequence(data=tensor([27., 15.,  3., 26., 14.,  2., 25., 13.,  1., 24., 12.,  0., 23., 11.,\n",
              "        22., 10., 21., 20.]), batch_sizes=tensor([3, 3, 3, 3, 2, 2, 1, 1]), sorted_indices=tensor([1, 2, 0]), unsorted_indices=tensor([2, 0, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "packed_sequence = pack_sequence([short, long, middle], enforce_sorted=False)\n",
        "packed_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cd50747",
      "metadata": {
        "id": "4cd50747"
      },
      "source": [
        "`PackedSequence` has `data` and `batch_sizes`\n",
        "- `data` contains the flattened value of given batch\n",
        "    - To optimize the computation, the sequences have to be sorted by descending of length\n",
        "- `batch_sizes` represents how many valid batch sample exists for each time step\n",
        "    - `[3, 3, 3, 2, 2, 1, 1]` means that there are 3 sequences for first three time steps, and then 2 sequences for next two steps, and then only 1 sequence for next two steps.\n",
        "- `sorted_indices` shows how the sorted sequences can be converted to original order.\n",
        "    - `[1,2,0]` means that \n",
        "        - the 0th sequence in the sorted sequences (the longest one) was indexed as 1 in the original input batch\n",
        "        - the 1st sequence in the sorted sequences (`middle`) was indexed as 2 in the original input batch\n",
        "        - the 2nd sequence in the sorted sequences (`short`) was index as 0 in the original input batch\n",
        "- `unsorted_indices` shows how the original sequences are sorted.\n",
        "    - `[2,0,1]` means that\n",
        "        - the 0th sequence in the original input was sorted as 2nd in the sorted sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "0500f200",
      "metadata": {
        "id": "0500f200",
        "outputId": "f78e0501-2311-4907-bf8c-9926c26f7da4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of output of RNN for PackedSequence: <class 'torch.nn.utils.rnn.PackedSequence'>\n",
            "Type of last_hidden of RNN for PackedSequence: <class 'torch.Tensor'>\n"
          ]
        }
      ],
      "source": [
        "rnn_layer = nn.GRU(1, 1)\n",
        "packed_sequence = pack_sequence([short.unsqueeze(1), long.unsqueeze(1), middle.unsqueeze(1)], enforce_sorted=False)\n",
        "out, last_hidden = rnn_layer(packed_sequence)\n",
        "\n",
        "print(f\"Type of output of RNN for PackedSequence: {type(out)}\")\n",
        "print(f\"Type of last_hidden of RNN for PackedSequence: {type(last_hidden)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f2e3e44",
      "metadata": {
        "id": "8f2e3e44"
      },
      "source": [
        "- RNN or its family of PyTorch can automatically handle `PackedSequence`\n",
        "- However, other layers like `nn.Embedding` or `nn.Linear` cannot take `PackedSequence` as its input\n",
        "- There are two ways to feed `PackedSequence` to these layers\n",
        "    - First, convert PackedSequence to ordinary torch.Tensor by `torch.nn.utils.rnn.pad_packed_sequence`\n",
        "        - This will convert PackedSequence to a tensor of sequneces with same length but different padding\n",
        "    - The other way is to feed only PackedSequence.data, and then declaring new PackedSequence with the output as `data`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "packed_sequence"
      ],
      "metadata": {
        "id": "3XBGEIV9sTJ3",
        "outputId": "a1dfa2b2-9289-4b9d-e36f-beeebff82c26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "3XBGEIV9sTJ3",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PackedSequence(data=tensor([[27.],\n",
              "        [15.],\n",
              "        [ 3.],\n",
              "        [26.],\n",
              "        [14.],\n",
              "        [ 2.],\n",
              "        [25.],\n",
              "        [13.],\n",
              "        [ 1.],\n",
              "        [24.],\n",
              "        [12.],\n",
              "        [ 0.],\n",
              "        [23.],\n",
              "        [11.],\n",
              "        [22.],\n",
              "        [10.],\n",
              "        [21.],\n",
              "        [20.]]), batch_sizes=tensor([3, 3, 3, 3, 2, 2, 1, 1]), sorted_indices=tensor([1, 2, 0]), unsorted_indices=tensor([2, 0, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "b08620c8",
      "metadata": {
        "id": "b08620c8",
        "outputId": "cfd89d73-ff95-4355-aefe-1983a840d1fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-7c62b9259c97>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m '''\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_linear_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtest_linear_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not PackedSequence"
          ]
        }
      ],
      "source": [
        "'''\n",
        "This will make error, because other layers cannot handle PackedSequence\n",
        "'''\n",
        "test_linear_layer = nn.Linear(in_features=1, out_features=2)\n",
        "test_linear_layer(packed_sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "ac9efa09",
      "metadata": {
        "id": "ac9efa09",
        "outputId": "6e266ee3-8dca-4314-fe10-91ee15a0def3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The padded sequence generated from packed sequence (squeezed for printing): \n",
            " tensor([[ 3., 27., 15.],\n",
            "        [ 2., 26., 14.],\n",
            "        [ 1., 25., 13.],\n",
            "        [ 0., 24., 12.],\n",
            "        [ 0., 23., 11.],\n",
            "        [ 0., 22., 10.],\n",
            "        [ 0., 21.,  0.],\n",
            "        [ 0., 20.,  0.]])\n",
            "\"pad_packed_sequence\" also returns \"batch_lengths\", to clarify the original length before the padding: \n",
            " tensor([4, 8, 6])\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "One way to to this is using torch.nn.utils.rnn.pad_packed_sequence to convert PackedSequence to ordinary padded tensor\n",
        "'''\n",
        "\n",
        "from torch.nn.utils.rnn import pad_packed_sequence\n",
        "padded_sequence, batch_lengths = pad_packed_sequence(packed_sequence)\n",
        "print(f'The padded sequence generated from packed sequence (squeezed for printing): \\n {padded_sequence.squeeze()}')\n",
        "print(f'\"pad_packed_sequence\" also returns \"batch_lengths\", to clarify the original length before the padding: \\n {batch_lengths}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "8e302fe4",
      "metadata": {
        "id": "8e302fe4",
        "outputId": "af6d0e8e-3e46-426a-f23f-ce6e534dff67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output of feeding padded_sequence to a linear layer: tensor([[[ 3.3898,  1.8743],\n",
            "         [24.5948, 12.2365],\n",
            "         [13.9923,  7.0554]],\n",
            "\n",
            "        [[ 2.5063,  1.4426],\n",
            "         [23.7113, 11.8048],\n",
            "         [13.1088,  6.6237]],\n",
            "\n",
            "        [[ 1.6227,  1.0108],\n",
            "         [22.8277, 11.3730],\n",
            "         [12.2252,  6.1919]],\n",
            "\n",
            "        [[ 0.7392,  0.5791],\n",
            "         [21.9442, 10.9413],\n",
            "         [11.3417,  5.7602]],\n",
            "\n",
            "        [[ 0.7392,  0.5791],\n",
            "         [21.0606, 10.5095],\n",
            "         [10.4581,  5.3284]],\n",
            "\n",
            "        [[ 0.7392,  0.5791],\n",
            "         [20.1771, 10.0777],\n",
            "         [ 9.5746,  4.8966]],\n",
            "\n",
            "        [[ 0.7392,  0.5791],\n",
            "         [19.2936,  9.6460],\n",
            "         [ 0.7392,  0.5791]],\n",
            "\n",
            "        [[ 0.7392,  0.5791],\n",
            "         [18.4100,  9.2142],\n",
            "         [ 0.7392,  0.5791]]], grad_fn=<ViewBackward0>)\n",
            "Caution that it returns non-zero values for timestep with zero padding, because linear layer has a bias\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Now you can feed padded sequence to linear layer.\n",
        "'''\n",
        "\n",
        "linear_output = test_linear_layer(padded_sequence)\n",
        "print(f\"Output of feeding padded_sequence to a linear layer: {linear_output}\")\n",
        "print(\"Caution that it returns non-zero values for timestep with zero padding, because linear layer has a bias\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "b1c0e150",
      "metadata": {
        "id": "b1c0e150",
        "outputId": "8cb4f6cd-0d17-4eda-f300-85bc96e46004",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PackedSequence(data=tensor([[24.5948, 12.2365],\n",
              "        [13.9923,  7.0554],\n",
              "        [ 3.3898,  1.8743],\n",
              "        [23.7113, 11.8048],\n",
              "        [13.1088,  6.6237],\n",
              "        [ 2.5063,  1.4426],\n",
              "        [22.8277, 11.3730],\n",
              "        [12.2252,  6.1919],\n",
              "        [ 1.6227,  1.0108],\n",
              "        [21.9442, 10.9413],\n",
              "        [11.3417,  5.7602],\n",
              "        [ 0.7392,  0.5791],\n",
              "        [21.0606, 10.5095],\n",
              "        [10.4581,  5.3284],\n",
              "        [20.1771, 10.0777],\n",
              "        [ 9.5746,  4.8966],\n",
              "        [19.2936,  9.6460],\n",
              "        [18.4100,  9.2142]], grad_fn=<PackPaddedSequenceBackward0>), batch_sizes=tensor([3, 3, 3, 3, 2, 2, 1, 1]), sorted_indices=tensor([1, 2, 0]), unsorted_indices=tensor([2, 0, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "'''\n",
        "You can make the output as a PackedSequence, by using torch.nn.utils.rnn.pack_padded_sequence\n",
        "'''\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "re_packed_sequence = pack_padded_sequence(linear_output, batch_lengths, enforce_sorted=False)\n",
        "re_packed_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "22117c41",
      "metadata": {
        "id": "22117c41",
        "outputId": "9f1bbb93-b344-46c3-dc04-e41d059ad0b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PackedSequence(data=tensor([[24.5948, 12.2365],\n",
              "        [13.9923,  7.0554],\n",
              "        [ 3.3898,  1.8743],\n",
              "        [23.7113, 11.8048],\n",
              "        [13.1088,  6.6237],\n",
              "        [ 2.5063,  1.4426],\n",
              "        [22.8277, 11.3730],\n",
              "        [12.2252,  6.1919],\n",
              "        [ 1.6227,  1.0108],\n",
              "        [21.9442, 10.9413],\n",
              "        [11.3417,  5.7602],\n",
              "        [ 0.7392,  0.5791],\n",
              "        [21.0606, 10.5095],\n",
              "        [10.4581,  5.3284],\n",
              "        [20.1771, 10.0777],\n",
              "        [ 9.5746,  4.8966],\n",
              "        [19.2936,  9.6460],\n",
              "        [18.4100,  9.2142]], grad_fn=<AddmmBackward0>), batch_sizes=tensor([3, 3, 3, 3, 2, 2, 1, 1]), sorted_indices=tensor([1, 2, 0]), unsorted_indices=tensor([2, 0, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "'''\n",
        "Another way to do it is using PackedSequence.data\n",
        "'''\n",
        "\n",
        "linear_out_pack = test_linear_layer(packed_sequence.data)\n",
        "packed_sequence_after_linear = PackedSequence(linear_out_pack, packed_sequence.batch_sizes, packed_sequence.sorted_indices, packed_sequence.unsorted_indices)\n",
        "packed_sequence_after_linear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "81dd119c",
      "metadata": {
        "id": "81dd119c",
        "outputId": "129118ba-729b-46cf-e314-38d152b8658b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(PackedSequence(data=tensor([    2,     2,     2,     2,     2,     2,     2,     2, 30025,  6481,\n",
              "          7110,  6832,  3600,  2284,  6730,  8543,  7175, 26684,  7249,  9081,\n",
              "          6741, 19570, 15523,  7009,  2714,  4330,  6455, 15420, 12614,  2426,\n",
              "         28173, 16955,  4563,  6642, 14377, 27164,  6678, 20003,  6625, 23111,\n",
              "         12935,  9186, 18637, 11105, 28240,  4325,  7334,  7637,  6749,  8398,\n",
              "         15237, 17601,  3191,  8756,  6507, 22306,  7323, 11003,  8251,  6412,\n",
              "          4657,  4421,    18,    18, 20796,  8827, 25509,  3160,  4336, 10833,\n",
              "             3,     3,  4331,  3301,  4337,  3252, 12553,  8419,  4173, 12469,\n",
              "         18905,  6414, 15888,    18,  4417, 10145, 13630,    18,    18,     3,\n",
              "          4694,  4387,  9107,     3,     3,    16,  6582,  9135,  3967,  3309,\n",
              "            18,  4540,  4352,     3,  4770, 28808,  7576,    18, 15011,     3,\n",
              "          7616, 30310, 16615,  7280,  4421, 11548,  9583,  7023,    18,     3]), batch_sizes=tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 6, 6, 6, 5, 3, 3, 3, 2, 2, 2, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1]), sorted_indices=tensor([6, 7, 0, 1, 2, 5, 3, 4]), unsorted_indices=tensor([2, 3, 4, 6, 7, 5, 0, 1])),\n",
              " PackedSequence(data=tensor([    2,     2,     2,     2,     2,     2,     2,     2, 30352,  1287,\n",
              "            77,  1089,  1393,    77,  1287,  1067,   781,  1109,  1612,    11,\n",
              "          6185,  3134,  1159,  1352,  1056, 13814,  1042,    87,  8166,  1067,\n",
              "            69,  1106,    69,  1042,  1406,  1161,  1109,    77,  1911,  1393,\n",
              "          7960,  1065,  1042,  1863, 24398,  1314,  1034,  3170,  1369,  4666,\n",
              "          1023,  1042,  3651,  1589,  1023,    11,  1606,  1061,  8082,  1652,\n",
              "          1407,  1042,  1889,  8849, 13185,    69,  8270,  1089,    77,  1159,\n",
              "          7332,  1352,  1592,  1434,  1634,  7485,    11,  1333,    18,    18,\n",
              "          1099,    17,  1407,  1693,    81,  6200,  1837,  2323,    77,  1287,\n",
              "          6278,    18,  8210,  8814,  1612,  1195,    18,    16,  1562,  1042,\n",
              "          2834, 22125,  1169,  1159,  1089,  1045, 18279,  1674,  1039, 31295,\n",
              "          2481,  2468,  1023,  5034,  1061,  1251,  1521,  1816,  4393,  8082,\n",
              "            18,  1068,  2032,    18,  2319,    18, 23122,    18]), batch_sizes=tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 6, 6, 5, 4, 4, 4, 4, 4, 4, 3, 2, 1, 1]), sorted_indices=tensor([6, 0, 7, 1, 2, 4, 3, 5]), unsorted_indices=tensor([1, 3, 4, 6, 5, 7, 0, 2])),\n",
              " PackedSequence(data=tensor([30352,  1287,    77,  1089,  1393,    77,  1287,  1067,   781,  1109,\n",
              "          1612,    11,  6185,  3134,  1159,  1352,  1056, 13814,  1042,    87,\n",
              "          8166,  1067,    69,  1106,    69,  1042,  1406,  1161,  1109,    77,\n",
              "          1911,  1393,  7960,  1065,  1042,  1863, 24398,  1314,  1034,  3170,\n",
              "          1369,  4666,  1023,  1042,  3651,  1589,  1023,    11,  1606,  1061,\n",
              "          8082,  1652,  1407,  1042,  1889,  8849, 13185,    69,  8270,  1089,\n",
              "            77,  1159,  7332,  1352,  1592,  1434,  1634,  7485,    11,  1333,\n",
              "            18,    18,  1099,    17,  1407,  1693,    81,  6200,     3,     3,\n",
              "          1837,  2323,    77,  1287,  6278,    18,  8210,  8814,  1612,  1195,\n",
              "            18,     3,    16,  1562,  1042,  2834,     3, 22125,  1169,  1159,\n",
              "          1089,  1045, 18279,  1674,  1039, 31295,  2481,  2468,  1023,  5034,\n",
              "          1061,  1251,  1521,  1816,  4393,  8082,    18,  1068,  2032,    18,\n",
              "             3,  2319,    18,     3, 23122,     3,    18,     3]), batch_sizes=tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 6, 6, 5, 4, 4, 4, 4, 4, 4, 3, 2, 1, 1]), sorted_indices=tensor([6, 0, 7, 1, 2, 4, 3, 5]), unsorted_indices=tensor([1, 3, 4, 6, 5, 7, 0, 2])))"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "def pack_collate(raw_batch):\n",
        "  source, target, shifted_target = zip(*raw_batch)\n",
        "  return pack_sequence(source, enforce_sorted=False), pack_sequence(target, enforce_sorted=False), pack_sequence(shifted_target, enforce_sorted=False)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=8, collate_fn=pack_collate)\n",
        "batch = next(iter(train_loader))\n",
        "batch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "357c61f4",
      "metadata": {
        "id": "357c61f4"
      },
      "source": [
        "## Define Model\n",
        "![image](https://raw.githubusercontent.com/tensorflow/nmt/master/nmt/g3doc/img/seq2seq.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0af0ca29",
      "metadata": {
        "id": "0af0ca29"
      },
      "outputs": [],
      "source": [
        "class Seq2seq(nn.Module):\n",
        "  def __init__(self, enc_vocab, dec_vocab, hidden_size):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(enc_vocab, hidden_size)\n",
        "    self.decoder = Decoder(dec_vocab, hidden_size)\n",
        "\n",
        "  def forward(self, src, tgt):\n",
        "    enc_out = self.encoder(src)\n",
        "    dec_out = self.decoder(tgt, enc_out)\n",
        "    return dec_out \n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, num_vocab, hidden_size, num_layers=2):\n",
        "    super().__init__()\n",
        "    self.emb = nn.Embedding(num_vocab, hidden_size)\n",
        "    self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True, num_layers=num_layers)\n",
        "    # batch_first True: it takes (Num_samples_in_batch, num_timesteps, num_dim)\n",
        "    # batch_first False: it takes (num_timesteps, Num_samples_in_batch, num_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    if isinstance(x, PackedSequence):\n",
        "      emb = PackedSequence(self.emb(x.data), batch_sizes=x.batch_sizes, sorted_indices=x.sorted_indices, unsorted_indices=x.unsorted_indices)\n",
        "    else:\n",
        "      emb = self.emb(x)\n",
        "    out, last_hidden = self.rnn(emb)\n",
        "    return last_hidden\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, num_vocab, hidden_size, num_layers=2):\n",
        "    super().__init__()\n",
        "    self.emb = nn.Embedding(num_vocab, hidden_size)\n",
        "    self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True, num_layers=num_layers)\n",
        "    self.proj = nn.Linear(hidden_size, num_vocab)\n",
        "\n",
        "\n",
        "  def forward(self, x, enc_output):\n",
        "    if isinstance(x, PackedSequence):\n",
        "      emb = PackedSequence(self.emb(x.data), batch_sizes=x.batch_sizes, sorted_indices=x.sorted_indices, unsorted_indices=x.unsorted_indices)\n",
        "      out, last_hidden = self.rnn(emb, enc_output)\n",
        "      logit = PackedSequence(self.proj(out.data), batch_sizes=out.batch_sizes, sorted_indices=out.sorted_indices, unsorted_indices=out.unsorted_indices)\n",
        "    else:\n",
        "      emb = self.emb(x)\n",
        "      out, last_hidden = self.rnn(emb, enc_output)\n",
        "      logit = self.proj(out)\n",
        "    return logit\n",
        "\n",
        "hidden_size = 64\n",
        "model = Seq2seq(tokenizer_src.vocab_size, tokenizer_tgt.vocab_size, hidden_size)\n",
        "encoder = Encoder(tokenizer_src.vocab_size, hidden_size)\n",
        "decoder = Decoder(tokenizer_tgt.vocab_size, hidden_size)\n",
        "\n",
        "src, tgt, shifted_tgt = batch\n",
        "\n",
        "enc_out = encoder(src)\n",
        "dec_out = decoder(tgt, enc_out)\n",
        "\n",
        "logit = model(src, tgt)\n",
        "# logit.shape, logit.softmax(dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WMGUwt10Whf5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMGUwt10Whf5",
        "outputId": "53bf8946-490b-41cd-ca60-8eaa32b28fe9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([1, 10, 30679]),\n",
              " torch.Size([1, 10]),\n",
              " tensor([[  76,   10,  746,  810,  906,  335,  412, 2590,   17,    3]]))"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "logit.shape, shifted_tgt.shape, shifted_tgt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WgvICWU0Wg0j",
      "metadata": {
        "id": "WgvICWU0Wg0j"
      },
      "outputs": [],
      "source": [
        "def nll_loss(pred, target, eps=1e-8):\n",
        "  if pred.ndim == 3:\n",
        "    pred = pred.flatten(0, 1)\n",
        "  if target.ndim == 2:\n",
        "    target = target.flatten(0, 1)\n",
        "  assert pred.ndim == 2\n",
        "  assert target.ndim == 1\n",
        "\n",
        "  \n",
        "  return -torch.log(pred[torch.arange(len(target)), target] + eps).mean()\n",
        "\n",
        "# loss = nll_loss(logit.softmax(dim=-1), shifted_tgt)\n",
        "# loss, loss.shape, shifted_tgt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13ffb21b",
      "metadata": {
        "id": "13ffb21b"
      },
      "source": [
        "## Define Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64a3758c",
      "metadata": {
        "id": "64a3758c"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "class Trainer:\n",
        "  def __init__(self, model, optimizer, loss_fn, train_loader, valid_loader, device):\n",
        "    self.model = model\n",
        "    self.optimizer = optimizer\n",
        "    self.loss_fn = loss_fn\n",
        "    self.train_loader = train_loader\n",
        "    self.valid_loader = valid_loader\n",
        "    \n",
        "    self.model.to(device)\n",
        "    \n",
        "    self.best_valid_accuracy = 0\n",
        "    self.device = device\n",
        "    \n",
        "    self.training_loss = []\n",
        "    self.validation_loss = []\n",
        "    self.validation_acc = []\n",
        "\n",
        "  def save_model(self, path='kor_eng_translator_model.pt'):\n",
        "    torch.save({'model':self.model.state_dict(), 'optim':self.optimizer.state_dict()}, path)\n",
        "    \n",
        "  def train_by_num_epoch(self, num_epochs):\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "      self.model.train()\n",
        "      for batch in tqdm(self.train_loader, leave=False):\n",
        "        loss_value = self._train_by_single_batch(batch)\n",
        "        self.training_loss.append(loss_value)\n",
        "      self.model.eval()\n",
        "      validation_loss, validation_acc = self.validate()\n",
        "      self.validation_loss.append(validation_loss)\n",
        "      self.validation_acc.append(validation_acc)\n",
        "      \n",
        "      if validation_acc > self.best_valid_accuracy:\n",
        "        print(f\"Saving the model with best validation accuracy: Epoch {epoch+1}, Acc: {validation_acc:.4f} \")\n",
        "        self.save_model('kor_eng_translator_model_best.pt')\n",
        "      else:\n",
        "        self.save_model('kor_eng_translator_model_last.pt')\n",
        "      self.best_valid_accuracy = max(validation_acc, self.best_valid_accuracy)\n",
        "\n",
        "      \n",
        "  def _train_by_single_batch(self, batch):\n",
        "    '''\n",
        "    This method updates self.model's parameter with a given batch\n",
        "    \n",
        "    batch (tuple): (batch_of_input_text, batch_of_label)\n",
        "    \n",
        "    You have to use variables below:\n",
        "    \n",
        "    self.model (SentimentModel/torch.nn.Module): A neural network model\n",
        "    self.optimizer (torch.optim.adam.Adam): Adam optimizer that optimizes model's parameter\n",
        "    self.loss_fn (function): function for calculating BCE loss for a given prediction and target\n",
        "    self.device (str): 'cuda' or 'cpu'\n",
        "\n",
        "    output: loss (float): Mean binary cross entropy value for every sample in the training batch\n",
        "    The model's parameters, optimizer's steps has to be updated inside this method\n",
        "\n",
        "    TODO: Complete this method \n",
        "    '''\n",
        "    src, tgt, shifted_tgt = batch\n",
        "    src = src.to(self.device)\n",
        "    tgt = tgt.to(self.device)\n",
        "    shifted_tgt = shifted_tgt.to(self.device)\n",
        "\n",
        "    logit = self.model(src, tgt)\n",
        "\n",
        "    if isinstance(logit, PackedSequence):\n",
        "      prob = logit.data.softmax(dim=-1)\n",
        "      loss = self.loss_fn(prob, shifted_tgt.data)\n",
        "    else:\n",
        "      loss = self.loss_fn(prob, shifted_tgt)\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "    self.optimizer.zero_grad\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "    \n",
        "  def validate(self, external_loader=None):\n",
        "    '''\n",
        "    This method calculates accuracy and loss for given data loader.\n",
        "    It can be used for validation step, or to get test set result\n",
        "    \n",
        "    input:\n",
        "      data_loader: If there is no data_loader given, use self.valid_loader as default.\n",
        "      \n",
        "    \n",
        "    output: \n",
        "      validation_loss (float): Mean Binary Cross Entropy value for every sample in validation set\n",
        "      validation_accuracy (float): Mean Accuracy value for every sample in validation set\n",
        "      \n",
        "    TODO: Complete this method \n",
        "\n",
        "    '''\n",
        "    \n",
        "    ### Don't change this part\n",
        "    if external_loader and isinstance(external_loader, DataLoader):\n",
        "      loader = external_loader\n",
        "      print('An arbitrary loader is used instead of Validation loader')\n",
        "    else:\n",
        "      loader = self.valid_loader\n",
        "      \n",
        "    self.model.eval()\n",
        "    \n",
        "    '''\n",
        "    Write your code from here, using loader, self.model, self.loss_fn.\n",
        "    '''\n",
        "\n",
        "hidden_size = 128\n",
        "model = Seq2seq(tokenizer_src.vocab_size, tokenizer_tgt.vocab_size, hidden_size)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_fn = nll_loss\n",
        "train_loader = DataLoader(train_set, batch_size=64, shuffle=True, collate_fn=pack_collate)\n",
        "valid_loader = DataLoader(valid_set, batch_size=128, shuffle=False, collate_fn=pack_collate)\n",
        "device = 'cuda'\n",
        "\n",
        "trainer = Trainer(model, optimizer, loss_fn, train_loader, valid_loader, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-zNWE0znYwwf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zNWE0znYwwf",
        "outputId": "06c9a0c2-c5b5-43a7-ed71-fcd4b5caa521"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[10.386933326721191, 10.32657241821289, 10.268647193908691, 10.210345268249512, 10.149768829345703, 10.084954261779785, 10.013751029968262, 9.933724403381348, 9.842076301574707, 9.735479354858398, 9.609955787658691, 9.460831642150879, 9.283170700073242, 9.072586059570312, 8.827017784118652, 8.548736572265625, 8.245342254638672, 7.928269863128662, 7.609827041625977, 7.30034065246582, 7.006803512573242, 6.732766628265381, 6.479230880737305, 6.245960235595703, 6.0324931144714355, 5.8385162353515625, 5.663750171661377, 5.507776737213135, 5.369976997375488, 5.249426364898682, 5.144853115081787, 5.054716110229492, 4.977362632751465, 4.911262035369873, 4.85506534576416, 4.807579517364502, 4.767537593841553, 4.733421802520752, 4.70345401763916, 4.676103115081787, 4.650460720062256, 4.626137733459473, 4.6028008460998535, 4.580265998840332, 4.558821201324463, 4.539508819580078, 4.523560523986816, 4.511585235595703, 4.503292083740234, 4.497927665710449, 4.4949469566345215, 4.494128704071045, 4.495345592498779, 4.498366832733154, 4.502806663513184, 4.508152484893799, 4.513828754425049, 4.519322395324707, 4.524385452270508, 4.529078960418701, 4.533546447753906, 4.537725925445557, 4.5412163734436035, 4.543583393096924, 4.544646263122559, 4.544658660888672, 4.544276237487793, 4.544242858886719, 4.545022964477539, 4.5467000007629395, 4.549046993255615, 4.5516510009765625, 4.554001331329346, 4.55544900894165, 4.555344581604004, 4.553051471710205, 4.548077583312988, 4.540017127990723, 4.528607368469238, 4.513759136199951, 4.495539665222168, 4.474140644073486, 4.449749946594238, 4.422365188598633, 4.391932010650635, 4.35894775390625, 4.324995994567871, 4.2922515869140625, 4.262956142425537, 4.2383713722229, 4.2186713218688965, 4.203605651855469, 4.192801475524902, 4.185894966125488, 4.182514667510986, 4.182295799255371, 4.184702396392822, 4.189136981964111, 4.195082664489746, 4.2021050453186035]\n"
          ]
        }
      ],
      "source": [
        "loss_records = []\n",
        "\n",
        "for i in range(100):\n",
        "  loss = trainer._train_by_single_batch(batch)\n",
        "  loss_records.append(loss)\n",
        "print(loss_records)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mQx4K_3DZS-I",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "7e0c64845c244720b529a238fb7f03fc",
            "2ce4571ca8c3486db82cf5111b49b979",
            "d01cea5d86be431fa519656159469c88",
            "50f476d7d8e84e8a81539a9a3549c1d4",
            "601a23ba484145af8eec1946779a53d8",
            "0eb7936f98394185a72a8eb807798fe4",
            "4f4102090d5d4f079c0a23a218648049",
            "87be8f11d7304eea98f48b2f49448a24",
            "e718ea3aae714c89aaee1f1f79374cab",
            "4d0a96248ad549d6989d9cc4a58bc10f",
            "ccdb06e71d874d1f8210776f1f74ea28",
            "5daec47eff3040788002690b9d9ff559",
            "fa45e83d15cb442fb7890f9ccedac956",
            "a1f02abecc824b7b9f94ba2546ea7041",
            "3e01a4e4ef104f42a255d072e865d0ec",
            "496ec14697464ddf922fb72560efeabc",
            "bb4843917abc42d68b3df2fbb3d90519",
            "29011e908581492ebcb84b39c77fcc06",
            "91e2fea28b8e4386903bd5dbd39c9a2f",
            "f63757eee857429c83bf8bba81a36ff6",
            "c202c1da2c3549b1b759e9bc8609ffcd",
            "11f296f0327c43378b8e2cca2ac83ce8"
          ]
        },
        "id": "mQx4K_3DZS-I",
        "outputId": "efdf1f48-9588-49c3-9b80-c6e4a1cdb54d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e0c64845c244720b529a238fb7f03fc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5daec47eff3040788002690b9d9ff559",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/160000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer.train_by_num_epoch(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d79bbee3",
      "metadata": {
        "id": "d79bbee3"
      },
      "outputs": [],
      "source": [
        "!gdown 10PR89HB2h2dKiXw-4GWQyhGvAOds3WKZ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1916a074",
      "metadata": {
        "id": "1916a074"
      },
      "outputs": [],
      "source": [
        "model = Seq2seq(tokenizer_src.vocab_size, tokenizer_tgt.vocab_size, hidden_size, num_layers=3)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0eb7936f98394185a72a8eb807798fe4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11f296f0327c43378b8e2cca2ac83ce8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29011e908581492ebcb84b39c77fcc06": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ce4571ca8c3486db82cf5111b49b979": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0eb7936f98394185a72a8eb807798fe4",
            "placeholder": "​",
            "style": "IPY_MODEL_4f4102090d5d4f079c0a23a218648049",
            "value": "  0%"
          }
        },
        "3e01a4e4ef104f42a255d072e865d0ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c202c1da2c3549b1b759e9bc8609ffcd",
            "placeholder": "​",
            "style": "IPY_MODEL_11f296f0327c43378b8e2cca2ac83ce8",
            "value": " 14326/160000 [02:02&lt;19:28, 124.68it/s]"
          }
        },
        "496ec14697464ddf922fb72560efeabc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d0a96248ad549d6989d9cc4a58bc10f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f4102090d5d4f079c0a23a218648049": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "50f476d7d8e84e8a81539a9a3549c1d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d0a96248ad549d6989d9cc4a58bc10f",
            "placeholder": "​",
            "style": "IPY_MODEL_ccdb06e71d874d1f8210776f1f74ea28",
            "value": " 0/1 [00:00&lt;?, ?it/s]"
          }
        },
        "5daec47eff3040788002690b9d9ff559": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fa45e83d15cb442fb7890f9ccedac956",
              "IPY_MODEL_a1f02abecc824b7b9f94ba2546ea7041",
              "IPY_MODEL_3e01a4e4ef104f42a255d072e865d0ec"
            ],
            "layout": "IPY_MODEL_496ec14697464ddf922fb72560efeabc"
          }
        },
        "601a23ba484145af8eec1946779a53d8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e0c64845c244720b529a238fb7f03fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2ce4571ca8c3486db82cf5111b49b979",
              "IPY_MODEL_d01cea5d86be431fa519656159469c88",
              "IPY_MODEL_50f476d7d8e84e8a81539a9a3549c1d4"
            ],
            "layout": "IPY_MODEL_601a23ba484145af8eec1946779a53d8"
          }
        },
        "87be8f11d7304eea98f48b2f49448a24": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91e2fea28b8e4386903bd5dbd39c9a2f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1f02abecc824b7b9f94ba2546ea7041": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91e2fea28b8e4386903bd5dbd39c9a2f",
            "max": 160000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f63757eee857429c83bf8bba81a36ff6",
            "value": 14326
          }
        },
        "bb4843917abc42d68b3df2fbb3d90519": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c202c1da2c3549b1b759e9bc8609ffcd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccdb06e71d874d1f8210776f1f74ea28": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d01cea5d86be431fa519656159469c88": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87be8f11d7304eea98f48b2f49448a24",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e718ea3aae714c89aaee1f1f79374cab",
            "value": 0
          }
        },
        "e718ea3aae714c89aaee1f1f79374cab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f63757eee857429c83bf8bba81a36ff6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fa45e83d15cb442fb7890f9ccedac956": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb4843917abc42d68b3df2fbb3d90519",
            "placeholder": "​",
            "style": "IPY_MODEL_29011e908581492ebcb84b39c77fcc06",
            "value": "  9%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}