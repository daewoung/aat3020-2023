{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jdasam/aat3020-2023/blob/main/notebooks/4_Machine_translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80c87a7a",
      "metadata": {
        "id": "80c87a7a"
      },
      "source": [
        "# Machine Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "Y6OfzjCACzNM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6OfzjCACzNM",
        "outputId": "6fde0333-6409-4585-d854-ff34b0f83ec1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m115.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n"
          ]
        }
      ],
      "source": [
        "# install HuggingFace\n",
        "!pip install transformers tokenizers\n",
        "\n",
        "# If you are not using Colab, install below to read xlsx file\n",
        "# !pip install openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "518b10da",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "518b10da",
        "outputId": "b3646180-0c9d-46c0-a9f7-9ec218d98cd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=13CGLEULYccogSLByHXPAxSveLZTtnj8c\n",
            "To: /content/nia_korean_english_csv.zip\n",
            "100% 190M/190M [00:04<00:00, 45.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Download dataset (originally from NIA AI-Hub)\n",
        "'''\n",
        "\n",
        "# !gdown 1CpsqOuuuB3I_PG5DbuqH1ssCFVerU46g\n",
        "# !unzip -q nia-aihub-korean-english.zip\n",
        "\n",
        "# !gdown 1GMFWREWBVcD5vJdwNFadHmzVStclcyKf\n",
        "# !unzip -q nia-aihub-korean-english-txt.zip\n",
        "\n",
        "!gdown 13CGLEULYccogSLByHXPAxSveLZTtnj8c\n",
        "!unzip -q nia_korean_english_csv.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tPBCvjfiMXSq"
      },
      "id": "tPBCvjfiMXSq",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "dWNSKjhdJ1jM",
      "metadata": {
        "id": "dWNSKjhdJ1jM"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"nia_korean_english.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "f7x-RgZcNk9I",
        "outputId": "7638e5b3-7aaf-4e18-cf7b-ccf3786470b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "id": "f7x-RgZcNk9I",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                        원문  \\\n",
              "0        'Bible Coloring'은 성경의 아름다운 이야기를 체험 할 수 있는 컬러링 ...   \n",
              "1                                             씨티은행에서 일하세요?   \n",
              "2                    푸리토의 베스트셀러는 해외에서 입소문만으로 4차 완판을 기록하였다.   \n",
              "3         11장에서는 예수님이 이번엔 나사로를 무덤에서 불러내어 죽은 자 가운데서 살리셨습니다.   \n",
              "4           6.5, 7, 8 사이즈가 몇 개나 더 재입고 될지 제게 알려주시면 감사하겠습니다.   \n",
              "...                                                    ...   \n",
              "1602413  힐튼호텔 건너편에서 회현동주민센터로 내려가는 길에는 칙칙한 회색의 석축옹벽이 이어져...   \n",
              "1602414  힘든 분들이 많이 계시지만 조금이나마 어렵지 않도록 도움이 될 수 있도록 저희가 최...   \n",
              "1602415  힘든 역사 속에서 대한민국을 불과 50여년 만에 빛나는 나라로 만들고 업적을 만들 ...   \n",
              "1602416  힘든 일을 하는 데는 무엇보다 정부가 큰 관심을 갖고 있다는 자부심을 갖도록 해야 한다.   \n",
              "1602417  힘을 합쳐 세계 일류통일강국으로 대한민국을 우뚝 세우는 역할을 이 자리의 모든 지도...   \n",
              "\n",
              "                                                       번역문  \n",
              "0        Bible Coloring' is a coloring application that...  \n",
              "1                              Do you work at a City bank?  \n",
              "2        PURITO's bestseller, which recorded 4th rough ...  \n",
              "3        In Chapter 11 Jesus called Lazarus from the to...  \n",
              "4        I would feel grateful to know how many stocks ...  \n",
              "...                                                    ...  \n",
              "1602413  Across from the Hilton Hotel, the path to Hoeh...  \n",
              "1602414  There are a lot of people who are having a har...  \n",
              "1602415  It was possible to make the Republic of Korea ...  \n",
              "1602416  As for doing hard work, most of all, they shou...  \n",
              "1602417  I would like to thank all the leaders here for...  \n",
              "\n",
              "[1602418 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-21156792-f76d-450e-8b12-4b42146b05e9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>원문</th>\n",
              "      <th>번역문</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>'Bible Coloring'은 성경의 아름다운 이야기를 체험 할 수 있는 컬러링 ...</td>\n",
              "      <td>Bible Coloring' is a coloring application that...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>씨티은행에서 일하세요?</td>\n",
              "      <td>Do you work at a City bank?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>푸리토의 베스트셀러는 해외에서 입소문만으로 4차 완판을 기록하였다.</td>\n",
              "      <td>PURITO's bestseller, which recorded 4th rough ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11장에서는 예수님이 이번엔 나사로를 무덤에서 불러내어 죽은 자 가운데서 살리셨습니다.</td>\n",
              "      <td>In Chapter 11 Jesus called Lazarus from the to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6.5, 7, 8 사이즈가 몇 개나 더 재입고 될지 제게 알려주시면 감사하겠습니다.</td>\n",
              "      <td>I would feel grateful to know how many stocks ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1602413</th>\n",
              "      <td>힐튼호텔 건너편에서 회현동주민센터로 내려가는 길에는 칙칙한 회색의 석축옹벽이 이어져...</td>\n",
              "      <td>Across from the Hilton Hotel, the path to Hoeh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1602414</th>\n",
              "      <td>힘든 분들이 많이 계시지만 조금이나마 어렵지 않도록 도움이 될 수 있도록 저희가 최...</td>\n",
              "      <td>There are a lot of people who are having a har...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1602415</th>\n",
              "      <td>힘든 역사 속에서 대한민국을 불과 50여년 만에 빛나는 나라로 만들고 업적을 만들 ...</td>\n",
              "      <td>It was possible to make the Republic of Korea ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1602416</th>\n",
              "      <td>힘든 일을 하는 데는 무엇보다 정부가 큰 관심을 갖고 있다는 자부심을 갖도록 해야 한다.</td>\n",
              "      <td>As for doing hard work, most of all, they shou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1602417</th>\n",
              "      <td>힘을 합쳐 세계 일류통일강국으로 대한민국을 우뚝 세우는 역할을 이 자리의 모든 지도...</td>\n",
              "      <td>I would like to thank all the leaders here for...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1602418 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-21156792-f76d-450e-8b12-4b42146b05e9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-21156792-f76d-450e-8b12-4b42146b05e9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-21156792-f76d-450e-8b12-4b42146b05e9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca202b9d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "ca202b9d",
        "outputId": "e415e08d-0eca-4d52-ecc3-414e60d7cff6",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\ndataset_dir = Path('nia_korean_english')\\ndata_list = sorted(list(dataset_dir.glob('*.xlsx')))\\nfor path in data_list:\\n  df = pd.read_excel(path)\\n  kor_text_path = path.parent / (path.stem+'_kor.txt') \\n  eng_text_path = path.parent / (path.stem+'_eng.txt') \\n  with open(kor_text_path, 'w', encoding='utf8') as f:\\n      f.write('\\n'.join(df['원문']))\\n  with open(eng_text_path, 'w', encoding='utf8') as f:\\n      f.write('\\n'.join(df['번역문']))\\n\""
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# You don't have to run this\n",
        "'''\n",
        "dataset_dir = Path('nia_korean_english')\n",
        "data_list = sorted(list(dataset_dir.glob('*.xlsx')))\n",
        "for path in data_list:\n",
        "  df = pd.read_excel(path)\n",
        "  kor_text_path = path.parent / (path.stem+'_kor.txt') \n",
        "  eng_text_path = path.parent / (path.stem+'_eng.txt') \n",
        "  with open(kor_text_path, 'w', encoding='utf8') as f:\n",
        "      f.write('\\n'.join(df['원문']))\n",
        "  with open(eng_text_path, 'w', encoding='utf8') as f:\n",
        "      f.write('\\n'.join(df['번역문']))\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fbc2c73",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fbc2c73",
        "outputId": "f948c9f0-78dd-4ce2-b669-c2ed92bbc6e0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[PosixPath('nia_korean_english/1_구어체(1).xlsx'),\n",
              " PosixPath('nia_korean_english/1_구어체(2).xlsx'),\n",
              " PosixPath('nia_korean_english/2_대화체.xlsx'),\n",
              " PosixPath('nia_korean_english/3_문어체_뉴스(1)_200226.xlsx'),\n",
              " PosixPath('nia_korean_english/3_문어체_뉴스(2).xlsx'),\n",
              " PosixPath('nia_korean_english/3_문어체_뉴스(3).xlsx'),\n",
              " PosixPath('nia_korean_english/3_문어체_뉴스(4).xlsx'),\n",
              " PosixPath('nia_korean_english/4_문어체_한국문화.xlsx'),\n",
              " PosixPath('nia_korean_english/5_문어체_조례.xlsx'),\n",
              " PosixPath('nia_korean_english/6_문어체_지자체웹사이트.xlsx')]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_dir = Path('nia_korean_english')\n",
        "data_list = sorted(list(dataset_dir.glob('*.xlsx')))\n",
        "\n",
        "data_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c767468",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "6c767468",
        "outputId": "be64a757-971e-49f5-e0eb-458aeaedaf6f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/teo/.local/share/virtualenvs/aat3020-2023-E1AG9i7b/lib/python3.8/site-packages/openpyxl/styles/stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
            "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
            "/home/teo/.local/share/virtualenvs/aat3020-2023-E1AG9i7b/lib/python3.8/site-packages/openpyxl/styles/stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
            "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
            "/home/teo/.local/share/virtualenvs/aat3020-2023-E1AG9i7b/lib/python3.8/site-packages/openpyxl/styles/stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
            "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
            "/home/teo/.local/share/virtualenvs/aat3020-2023-E1AG9i7b/lib/python3.8/site-packages/openpyxl/styles/stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
            "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "dfs = [pd.read_excel(path) for path in data_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e9f316f",
      "metadata": {
        "id": "2e9f316f"
      },
      "outputs": [],
      "source": [
        "df = pd.concat(dfs, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gKyXWsJjjrLR",
      "metadata": {
        "id": "gKyXWsJjjrLR"
      },
      "outputs": [],
      "source": [
        "with open(\"nia_korean_english/1_구어체(1)_kor.txt\", 'w', encoding='utf8') as f:\n",
        "    f.write('\\n'.join(df['원문']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff8b8786",
      "metadata": {
        "id": "ff8b8786"
      },
      "outputs": [],
      "source": [
        "df['원문'][10000:10050], df['번역문'][10000:10050]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3df6e584",
      "metadata": {
        "id": "3df6e584"
      },
      "source": [
        "## Huggingface Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c6f1769",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c6f1769",
        "outputId": "f1f0dfad-d93b-482d-8c92-d46bc4177580"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['hugging_eng_32000/vocab.txt']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tokenizers import BertWordPieceTokenizer\n",
        "tokenizer = BertWordPieceTokenizer(strip_accents=False, lowercase=False)\n",
        "\n",
        "num_files = len(data_list)\n",
        "\n",
        "corpus_file   =  [str(path.parent / (path.stem + '_kor.txt')) for path in data_list[:num_files]]\n",
        "# output_dir   = Path('hugging_kor_%d'%(vocab_size))\n",
        "en_corpus_file   =  [str(path.parent / (path.stem + '_eng.txt')) for path in data_list[:num_files]]\n",
        "# output_dir   = Path('hugging_eng_%d'%(vocab_size))\n",
        "\n",
        "vocab_size    = 32000  # Number of maximum size of the vocabulary\n",
        "limit_alphabet= 6000   \n",
        "output_dir    = Path('hugging_kor_%d'%(vocab_size))\n",
        "en_output_dir = Path('hugging_eng_%d'%(vocab_size))\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "en_output_dir.mkdir(exist_ok=True)\n",
        "min_frequency = 5 \n",
        "\n",
        "tokenizer.train(files=corpus_file,\n",
        "               vocab_size=vocab_size,\n",
        "               min_frequency=min_frequency,\n",
        "               limit_alphabet=limit_alphabet, \n",
        "               show_progress=True)\n",
        "\n",
        "tokenizer.save_model(str(output_dir))\n",
        "\n",
        "en_tokenizer = BertWordPieceTokenizer(strip_accents=False, lowercase=False)\n",
        "en_tokenizer.train(files=en_corpus_file,\n",
        "                vocab_size=vocab_size,\n",
        "                min_frequency=min_frequency,\n",
        "                limit_alphabet=limit_alphabet,\n",
        "                show_progress=True)\n",
        "en_tokenizer.save_model(str(en_output_dir))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "410eb97a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "410eb97a",
        "outputId": "f5f6eb80-3ea4-440c-b132-c6d8bcd75124",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "나는 친구에게 그 철학자의 책을 선물해 주겠다고 말했습니다.\n",
            "[CLS] 나는 친구에게 그 철학자의 책을 선물해 주겠다고 말했습니다. [SEP]\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizerFast\n",
        "\n",
        "tokenizer_src = BertTokenizerFast.from_pretrained('hugging_kor_32000',\n",
        "                                                       strip_accents=False,\n",
        "                                                       lowercase=False) \n",
        "tokenizer_tgt = BertTokenizerFast.from_pretrained('hugging_eng_32000',\n",
        "                                                       strip_accents=False,\n",
        "                                                       lowercase=False) \n",
        "\n",
        "tokenized_data = tokenizer_src(df['원문'].iloc[10])\n",
        "print(df['원문'].iloc[10])\n",
        "print(tokenizer_src.decode(tokenized_data['input_ids']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q9GJ7cifk5tg",
      "metadata": {
        "id": "q9GJ7cifk5tg"
      },
      "outputs": [],
      "source": [
        "tokenized_data['input_ids']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "340f55c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "340f55c0",
        "outputId": "15b66d7d-e619-4672-dc5c-038218608974"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[2, 6481, 25257, 8055, 4330, 2537, 10839, 3]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_ids = tokenizer_src(\"나는 서강대학교에 다닙니다\")['input_ids']\n",
        "tokenized_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b3ea831",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "5b3ea831",
        "outputId": "04621101-cf34-4b8d-e1b4-a747c445e8c3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[CLS] 나는 서강대학교에 다닙니다 [SEP]'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer_src.decode(tokenized_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26964dcb",
      "metadata": {
        "id": "26964dcb"
      },
      "source": [
        "## Divide Train / Validate/ Test Set\n",
        "- using `np.random.choice`\n",
        "    - To always get the same random shuffling result, you have to use `np.random.seed()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B2zJgvXPlh95",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "B2zJgvXPlh95",
        "outputId": "33f24480-2a88-418e-d95d-b01f5b6050d6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d659cf52-06c5-4062-a100-2c32cbe09de2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>지자체</th>\n",
              "      <th>원문</th>\n",
              "      <th>번역문</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>261811</td>\n",
              "      <td>경기도</td>\n",
              "      <td>\"경기도가 말산업 육성을 위해 총예산 245,193천원으로 2013년 경기도 용인시...</td>\n",
              "      <td>\"The Gyeonggi provincial government announced ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>409852</td>\n",
              "      <td>경기도</td>\n",
              "      <td>\"경기도가 주최하고 경기FTA활용지원센터와 코트라가 주관한 이번 시장개척단은 지난 ...</td>\n",
              "      <td>\"Organized by Gyeonggi provincial government a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>352671</td>\n",
              "      <td>경기도</td>\n",
              "      <td>\"경기도가 주최하고 경기도비정규직지원센터가 주관한 이번 교육은 공공 부문이 직·간접...</td>\n",
              "      <td>\"Organized by Gyeonggi provincial government a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>308191</td>\n",
              "      <td>경기도</td>\n",
              "      <td>\"경기도가 주최하고 사단법인 한국장애인복지시설협회 경기도협회(대표자:정권)에서 주관...</td>\n",
              "      <td>\"Organized by Gyeonggi provincial government a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>185424</td>\n",
              "      <td>경기도</td>\n",
              "      <td>\"경기도가 주최하고 인구보건복지협회 경기지회, 아이낳기좋은세상 경기운동본부가 주관하...</td>\n",
              "      <td>\"Hosted by Gyeonggi provincial government, org...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100082</th>\n",
              "      <td>73961</td>\n",
              "      <td>서울시 중구</td>\n",
              "      <td>힐튼호텔 건너편에서 회현동주민센터로 내려가는 길에는 칙칙한 회색의 석축옹벽이 이어져...</td>\n",
              "      <td>Across from the Hilton Hotel, the path to Hoeh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100083</th>\n",
              "      <td>462950</td>\n",
              "      <td>경기도</td>\n",
              "      <td>힘든 분들이 많이 계시지만 조금이나마 어렵지 않도록 도움이 될 수 있도록 저희가 최...</td>\n",
              "      <td>There are a lot of people who are having a har...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100084</th>\n",
              "      <td>333006</td>\n",
              "      <td>경기도</td>\n",
              "      <td>힘든 역사 속에서 대한민국을 불과 50여년 만에 빛나는 나라로 만들고 업적을 만들 ...</td>\n",
              "      <td>It was possible to make the Republic of Korea ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100085</th>\n",
              "      <td>454399</td>\n",
              "      <td>경기도</td>\n",
              "      <td>힘든 일을 하는 데는 무엇보다 정부가 큰 관심을 갖고 있다는 자부심을 갖도록 해야 한다.</td>\n",
              "      <td>As for doing hard work, most of all, they shou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100086</th>\n",
              "      <td>507679</td>\n",
              "      <td>경기도</td>\n",
              "      <td>힘을 합쳐 세계 일류통일강국으로 대한민국을 우뚝 세우는 역할을 이 자리의 모든 지도...</td>\n",
              "      <td>I would like to thank all the leaders here for...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100087 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d659cf52-06c5-4062-a100-2c32cbe09de2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d659cf52-06c5-4062-a100-2c32cbe09de2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d659cf52-06c5-4062-a100-2c32cbe09de2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "            ID     지자체                                                 원문  \\\n",
              "0       261811     경기도  \"경기도가 말산업 육성을 위해 총예산 245,193천원으로 2013년 경기도 용인시...   \n",
              "1       409852     경기도  \"경기도가 주최하고 경기FTA활용지원센터와 코트라가 주관한 이번 시장개척단은 지난 ...   \n",
              "2       352671     경기도  \"경기도가 주최하고 경기도비정규직지원센터가 주관한 이번 교육은 공공 부문이 직·간접...   \n",
              "3       308191     경기도  \"경기도가 주최하고 사단법인 한국장애인복지시설협회 경기도협회(대표자:정권)에서 주관...   \n",
              "4       185424     경기도  \"경기도가 주최하고 인구보건복지협회 경기지회, 아이낳기좋은세상 경기운동본부가 주관하...   \n",
              "...        ...     ...                                                ...   \n",
              "100082   73961  서울시 중구  힐튼호텔 건너편에서 회현동주민센터로 내려가는 길에는 칙칙한 회색의 석축옹벽이 이어져...   \n",
              "100083  462950     경기도  힘든 분들이 많이 계시지만 조금이나마 어렵지 않도록 도움이 될 수 있도록 저희가 최...   \n",
              "100084  333006     경기도  힘든 역사 속에서 대한민국을 불과 50여년 만에 빛나는 나라로 만들고 업적을 만들 ...   \n",
              "100085  454399     경기도  힘든 일을 하는 데는 무엇보다 정부가 큰 관심을 갖고 있다는 자부심을 갖도록 해야 한다.   \n",
              "100086  507679     경기도  힘을 합쳐 세계 일류통일강국으로 대한민국을 우뚝 세우는 역할을 이 자리의 모든 지도...   \n",
              "\n",
              "                                                      번역문  \n",
              "0       \"The Gyeonggi provincial government announced ...  \n",
              "1       \"Organized by Gyeonggi provincial government a...  \n",
              "2       \"Organized by Gyeonggi provincial government a...  \n",
              "3       \"Organized by Gyeonggi provincial government a...  \n",
              "4       \"Hosted by Gyeonggi provincial government, org...  \n",
              "...                                                   ...  \n",
              "100082  Across from the Hilton Hotel, the path to Hoeh...  \n",
              "100083  There are a lot of people who are having a har...  \n",
              "100084  It was possible to make the Republic of Korea ...  \n",
              "100085  As for doing hard work, most of all, they shou...  \n",
              "100086  I would like to thank all the leaders here for...  \n",
              "\n",
              "[100087 rows x 4 columns]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "boOZlijtlpgv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boOZlijtlpgv",
        "outputId": "9ce04f9d-ee41-43a0-9463-c1d444496e01"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1602418"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NdDy0EHNlv60",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdDy0EHNlv60",
        "outputId": "3180be90-6287-4e22-b28f-887e43166aab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ID                                                507679\n",
              "지자체                                                  경기도\n",
              "원문     힘을 합쳐 세계 일류통일강국으로 대한민국을 우뚝 세우는 역할을 이 자리의 모든 지도...\n",
              "번역문    I would like to thank all the leaders here for...\n",
              "Name: 100086, dtype: object"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.iloc[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AGA1i06oMD1N",
      "metadata": {
        "id": "AGA1i06oMD1N"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_excel(\"/content/nia_korean_english/1_구어체(1).xlsx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba4a138a",
      "metadata": {
        "id": "ba4a138a"
      },
      "source": [
        "## Define Dataset\n",
        "- Each datasample has to return source sentence and target sentence\n",
        "- You need a Tokenizer to get the tokenized result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b088f662",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b088f662",
        "outputId": "f8773ed4-aafa-4a22-e8b5-4917d431390f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([    2,    11,    70,  4665,  5209, 13306,    71, 12901,  9565, 12435,\n",
            "           11,  3546, 14567,  4325,  8934,  8407,  7400,  4154,  3252,  6420,\n",
            "        12985,  4996,  3397,  6461,    18,     3])\n",
            "tensor([    2, 26268, 23067,    11,  1056,    69, 23067,  2803,  1067,  5155,\n",
            "         1117,  1042,  2405,  4024,  5520,  1039,  1023, 26268,    18])\n",
            "tensor([26268, 23067,    11,  1056,    69, 23067,  2803,  1067,  5155,  1117,\n",
            "         1042,  2405,  4024,  5520,  1039,  1023, 26268,    18,     3])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "class Dataset:\n",
        "  def __init__(self, df, src_tokenizer, tgt_tokenizer):\n",
        "    self.data = df\n",
        "    self.src_tokenizer = src_tokenizer\n",
        "    self.tgt_tokenizer = tgt_tokenizer\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    selected_row = self.data.iloc[idx]\n",
        "    source = selected_row['원문']\n",
        "    target = selected_row['번역문']\n",
        "\n",
        "    source_enc = self.src_tokenizer(source)['input_ids']\n",
        "    target_enc = self.tgt_tokenizer(target)['input_ids']\n",
        "\n",
        "    return torch.LongTensor(source_enc), torch.LongTensor(target_enc[:-1]), torch.LongTensor(target_enc[1:])\n",
        "  \n",
        "dataset = Dataset(df, tokenizer_src, tokenizer_tgt)\n",
        "\n",
        "source, target, shifted_target = dataset[0]\n",
        "print(source)\n",
        "print(target)\n",
        "print(shifted_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kMaE7cK2EurA",
      "metadata": {
        "id": "kMaE7cK2EurA"
      },
      "source": [
        "## Split Dataset\n",
        "- using ``torch.utils.data.random_split(dataset)``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "Lb8fmwGbEhU1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lb8fmwGbEhU1",
        "outputId": "dccae782-4e6b-41ed-cbce-69be82d8e24f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1281934 160241 160243\n"
          ]
        }
      ],
      "source": [
        "num_data = len(dataset)\n",
        "# num_data\n",
        "num_train = int(num_data * 0.8)\n",
        "num_valid = int(num_data * 0.1)\n",
        "# num_test = int(num_data * 0.1)\n",
        "num_test = num_data - (num_train + num_valid)\n",
        "\n",
        "train_set, valid_set, test_set = torch.utils.data.random_split(dataset, [num_train, num_valid, num_test])\n",
        "print(len(train_set), len(valid_set), len(test_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "76933a49",
      "metadata": {
        "id": "76933a49"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "oTmu4Yy_N7vt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "oTmu4Yy_N7vt",
        "outputId": "4220e80d-e2cd-4976-a0e6-8224d249d136"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-6f710a1b7bb1>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Handle `CustomType` automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \"\"\"\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcollate_fn_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcollate_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typed_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [17] at entry 0 and [38] at entry 1"
          ]
        }
      ],
      "source": [
        "train_loader = DataLoader(train_set, batch_size=4, shuffle=True)\n",
        "\n",
        "batch = next(iter(train_loader))\n",
        "batch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4a9f0ec",
      "metadata": {
        "id": "f4a9f0ec"
      },
      "source": [
        "## Define Collate function\n",
        "- After implementing Dataset, we have to declare DataLoader that groups several training samples as a single batch\n",
        "- However, we cannot batchify the melodies in straightforward way, because the length of each melody is different\n",
        "- In this problem, you will learn about how to handle sequences of different length as a batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "456ff9f2",
      "metadata": {
        "id": "456ff9f2"
      },
      "outputs": [],
      "source": [
        "\n",
        "'''\n",
        "This cell will make error, because the length of each sample is different to each other\n",
        "'''\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=8)\n",
        "batch = next(iter(train_loader))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afc533f7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afc533f7",
        "outputId": "b13b1a4f-2d3a-4fc0-8307-8ca21ca2208a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([tensor([    2,  7110,  7249,  6455, 14377, 18637, 15237,  8251, 25509,  4337,\n",
              "          18905, 13630,  9107,  9135,    18,     3]),\n",
              "  tensor([    2,  6832,  9081, 15420, 27164, 11105, 17601,  6412,  3160,  3252,\n",
              "           6414,    18,     3]),\n",
              "  tensor([    2,  3600,  6741, 12614,  6678, 28240,  3191,  4657,  4336, 12553,\n",
              "          15888,    18,     3]),\n",
              "  tensor([    2,  6730, 15523, 28173,  6625,  7334,  6507,    18,     3]),\n",
              "  tensor([    2,  8543,  7009, 16955, 23111,  7637, 22306,    18,     3]),\n",
              "  tensor([    2,  2284, 19570,  2426, 20003,  4325,  8756,  4421, 10833,  8419,\n",
              "             18,     3]),\n",
              "  tensor([    2, 30025,  7175,  2714,  4563, 12935,  6749,  7323, 20796,  4331,\n",
              "           4173,  4417,  4694,    16,  3967,  4540,  4770,  7576, 15011,  7616,\n",
              "          30310, 16615,  7280,  4421, 11548,  9583,  7023,    18,     3]),\n",
              "  tensor([    2,  6481, 26684,  4330,  6642,  9186,  8398, 11003,  8827,  3301,\n",
              "          12469, 10145,  4387,  6582,  3309,  4352, 28808,    18,     3])],\n",
              " [tensor([    2,  1287,  1109, 13814,  1042,  1065,  4666,  1061,    69,  1434,\n",
              "             17,  2323,  8814,  1562,  1169, 18279,  2481,  1061,  4393,  2032,\n",
              "             18]),\n",
              "  tensor([   2, 1089,   11,   87, 1161, 1863, 1042, 1652, 1089, 7485, 1693, 1287,\n",
              "          1195, 2834, 1089, 1039, 1023, 1521,   18]),\n",
              "  tensor([    2,  1393,  6185,  8166,  1109, 24398,  3651,  1407,    77,    11,\n",
              "             81,  6278,    18]),\n",
              "  tensor([   2, 1287, 1159,   69, 1911, 1034, 1023, 1889, 7332,   18]),\n",
              "  tensor([   2,   77, 3134, 1067,   77, 1314, 1589, 1042, 1159, 1333, 6200,   18]),\n",
              "  tensor([   2, 1067, 1352, 1106, 1393, 3170,   11, 8849, 1352,   18]),\n",
              "  tensor([    2, 30352,   781,  1056,    69,  7960,  1369,  1606, 13185,  1592,\n",
              "           1099,  1837,  8210,    16, 22125,  1045, 31295,  5034,  1816,  1068,\n",
              "           2319, 23122,    18]),\n",
              "  tensor([   2,   77, 1612, 1042, 1406, 1042, 1023, 8082, 8270, 1634, 1407,   77,\n",
              "          1612, 1042, 1159, 1674, 2468, 1251, 8082,   18])],\n",
              " [tensor([ 1287,  1109, 13814,  1042,  1065,  4666,  1061,    69,  1434,    17,\n",
              "           2323,  8814,  1562,  1169, 18279,  2481,  1061,  4393,  2032,    18,\n",
              "              3]),\n",
              "  tensor([1089,   11,   87, 1161, 1863, 1042, 1652, 1089, 7485, 1693, 1287, 1195,\n",
              "          2834, 1089, 1039, 1023, 1521,   18,    3]),\n",
              "  tensor([ 1393,  6185,  8166,  1109, 24398,  3651,  1407,    77,    11,    81,\n",
              "           6278,    18,     3]),\n",
              "  tensor([1287, 1159,   69, 1911, 1034, 1023, 1889, 7332,   18,    3]),\n",
              "  tensor([  77, 3134, 1067,   77, 1314, 1589, 1042, 1159, 1333, 6200,   18,    3]),\n",
              "  tensor([1067, 1352, 1106, 1393, 3170,   11, 8849, 1352,   18,    3]),\n",
              "  tensor([30352,   781,  1056,    69,  7960,  1369,  1606, 13185,  1592,  1099,\n",
              "           1837,  8210,    16, 22125,  1045, 31295,  5034,  1816,  1068,  2319,\n",
              "          23122,    18,     3]),\n",
              "  tensor([  77, 1612, 1042, 1406, 1042, 1023, 8082, 8270, 1634, 1407,   77, 1612,\n",
              "          1042, 1159, 1674, 2468, 1251, 8082,   18,    3])])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "To handle that problem, you have to make your collate function \n",
        "'''\n",
        "def your_collate_function(raw_batch):\n",
        "  '''\n",
        "  You can make your own function to handle the batch\n",
        "  '''\n",
        "  src = [x[0] for x in raw_batch]\n",
        "  tgt = [x[1] for x in raw_batch]\n",
        "  shifted_tgt = [x[2] for x in raw_batch]\n",
        "  \n",
        "  return src, tgt, shifted_tgt # This returns the first sample of each batch. So it will avoid the error, but it doesn't do proper batchifying\n",
        "\n",
        "batch_size = 8\n",
        "raw_batch = [train_set[i] for i in range(batch_size)] # This is the input for the collate function\n",
        "batch = your_collate_function(raw_batch)\n",
        "\n",
        "'''\n",
        "This is what the 'collate_fn' does in DataLoader\n",
        "'''\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, collate_fn=your_collate_function)\n",
        "batch_by_loader = next(iter(train_loader))\n",
        "batch_by_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XO7cn02WqvFx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XO7cn02WqvFx",
        "outputId": "6a94cd55-814e-48e6-c251-6a87ff0e9a8f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(tensor([    2,  7110,  7249,  6455, 14377, 18637, 15237,  8251, 25509,  4337,\n",
              "          18905, 13630,  9107,  9135,    18,     3]),\n",
              "  tensor([    2,  1287,  1109, 13814,  1042,  1065,  4666,  1061,    69,  1434,\n",
              "             17,  2323,  8814,  1562,  1169, 18279,  2481,  1061,  4393,  2032,\n",
              "             18]),\n",
              "  tensor([ 1287,  1109, 13814,  1042,  1065,  4666,  1061,    69,  1434,    17,\n",
              "           2323,  8814,  1562,  1169, 18279,  2481,  1061,  4393,  2032,    18,\n",
              "              3])),\n",
              " (tensor([    2,  6832,  9081, 15420, 27164, 11105, 17601,  6412,  3160,  3252,\n",
              "           6414,    18,     3]),\n",
              "  tensor([   2, 1089,   11,   87, 1161, 1863, 1042, 1652, 1089, 7485, 1693, 1287,\n",
              "          1195, 2834, 1089, 1039, 1023, 1521,   18]),\n",
              "  tensor([1089,   11,   87, 1161, 1863, 1042, 1652, 1089, 7485, 1693, 1287, 1195,\n",
              "          2834, 1089, 1039, 1023, 1521,   18,    3])),\n",
              " (tensor([    2,  3600,  6741, 12614,  6678, 28240,  3191,  4657,  4336, 12553,\n",
              "          15888,    18,     3]),\n",
              "  tensor([    2,  1393,  6185,  8166,  1109, 24398,  3651,  1407,    77,    11,\n",
              "             81,  6278,    18]),\n",
              "  tensor([ 1393,  6185,  8166,  1109, 24398,  3651,  1407,    77,    11,    81,\n",
              "           6278,    18,     3]))]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_batch = [train_set[0], train_set[1], train_set[2]] # this is a sample input for collate function\n",
        "raw_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2iG_wY4Zqo-W",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iG_wY4Zqo-W",
        "outputId": "fc67856e-dabb-4997-fcbc-275da43e8ea0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(tensor([    2,  7110,  7249,  6455, 14377, 18637, 15237,  8251, 25509,  4337,\n",
              "          18905, 13630,  9107,  9135,    18,     3]),\n",
              "  tensor([    2,  1287,  1109, 13814,  1042,  1065,  4666,  1061,    69,  1434,\n",
              "             17,  2323,  8814,  1562,  1169, 18279,  2481,  1061,  4393,  2032,\n",
              "             18]),\n",
              "  tensor([ 1287,  1109, 13814,  1042,  1065,  4666,  1061,    69,  1434,    17,\n",
              "           2323,  8814,  1562,  1169, 18279,  2481,  1061,  4393,  2032,    18,\n",
              "              3])),\n",
              " (tensor([    2,  6832,  9081, 15420, 27164, 11105, 17601,  6412,  3160,  3252,\n",
              "           6414,    18,     3]),\n",
              "  tensor([   2, 1089,   11,   87, 1161, 1863, 1042, 1652, 1089, 7485, 1693, 1287,\n",
              "          1195, 2834, 1089, 1039, 1023, 1521,   18]),\n",
              "  tensor([1089,   11,   87, 1161, 1863, 1042, 1652, 1089, 7485, 1693, 1287, 1195,\n",
              "          2834, 1089, 1039, 1023, 1521,   18,    3])),\n",
              " (tensor([    2,  3600,  6741, 12614,  6678, 28240,  3191,  4657,  4336, 12553,\n",
              "          15888,    18,     3]),\n",
              "  tensor([    2,  1393,  6185,  8166,  1109, 24398,  3651,  1407,    77,    11,\n",
              "             81,  6278,    18]),\n",
              "  tensor([ 1393,  6185,  8166,  1109, 24398,  3651,  1407,    77,    11,    81,\n",
              "           6278,    18,     3])),\n",
              " (tensor([    2,  6730, 15523, 28173,  6625,  7334,  6507,    18,     3]),\n",
              "  tensor([   2, 1287, 1159,   69, 1911, 1034, 1023, 1889, 7332,   18]),\n",
              "  tensor([1287, 1159,   69, 1911, 1034, 1023, 1889, 7332,   18,    3])),\n",
              " (tensor([    2,  8543,  7009, 16955, 23111,  7637, 22306,    18,     3]),\n",
              "  tensor([   2,   77, 3134, 1067,   77, 1314, 1589, 1042, 1159, 1333, 6200,   18]),\n",
              "  tensor([  77, 3134, 1067,   77, 1314, 1589, 1042, 1159, 1333, 6200,   18,    3])),\n",
              " (tensor([    2,  2284, 19570,  2426, 20003,  4325,  8756,  4421, 10833,  8419,\n",
              "             18,     3]),\n",
              "  tensor([   2, 1067, 1352, 1106, 1393, 3170,   11, 8849, 1352,   18]),\n",
              "  tensor([1067, 1352, 1106, 1393, 3170,   11, 8849, 1352,   18,    3])),\n",
              " (tensor([    2, 30025,  7175,  2714,  4563, 12935,  6749,  7323, 20796,  4331,\n",
              "           4173,  4417,  4694,    16,  3967,  4540,  4770,  7576, 15011,  7616,\n",
              "          30310, 16615,  7280,  4421, 11548,  9583,  7023,    18,     3]),\n",
              "  tensor([    2, 30352,   781,  1056,    69,  7960,  1369,  1606, 13185,  1592,\n",
              "           1099,  1837,  8210,    16, 22125,  1045, 31295,  5034,  1816,  1068,\n",
              "           2319, 23122,    18]),\n",
              "  tensor([30352,   781,  1056,    69,  7960,  1369,  1606, 13185,  1592,  1099,\n",
              "           1837,  8210,    16, 22125,  1045, 31295,  5034,  1816,  1068,  2319,\n",
              "          23122,    18,     3])),\n",
              " (tensor([    2,  6481, 26684,  4330,  6642,  9186,  8398, 11003,  8827,  3301,\n",
              "          12469, 10145,  4387,  6582,  3309,  4352, 28808,    18,     3]),\n",
              "  tensor([   2,   77, 1612, 1042, 1406, 1042, 1023, 8082, 8270, 1634, 1407,   77,\n",
              "          1612, 1042, 1159, 1674, 2468, 1251, 8082,   18]),\n",
              "  tensor([  77, 1612, 1042, 1406, 1042, 1023, 8082, 8270, 1634, 1407,   77, 1612,\n",
              "          1042, 1159, 1674, 2468, 1251, 8082,   18,    3]))]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_by_loader # [train_set[0], train_set[1], train_set[2]]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22db0716",
      "metadata": {
        "id": "22db0716"
      },
      "source": [
        "#### Pad Sequence and Pack Sequence\n",
        "In PyTorch, there are two ways to batchify a group of sequence with different length.\n",
        "- `torch.nn.utils.rnn.pad_sequence`\n",
        "    - This function takes list of tensors with different length and return padded sequence\n",
        "    - Padding is adding some constant number as a PAD token to match the length of short sequence to the maximum length\n",
        "        - e.g. If there are sequence of length (3,7,4), we can add 4 zeros to sequence with length 3, 3 zeros to sequence with length 4 to make them length 7\n",
        "    - In default, we use 0 for padding (zero padding)\n",
        "    - The result \n",
        "- `torch.nn.utils.rnn.pack_sequence`\n",
        "    - pad_sequence "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c8cce88",
      "metadata": {
        "id": "1c8cce88"
      },
      "source": [
        "Cells below show the example of `pad_sequence`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe229f85",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe229f85",
        "outputId": "b9e9292f-63b7-4f09-b518-82e8b812f250"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 3., 27., 15.],\n",
              "        [ 2., 26., 14.],\n",
              "        [ 1., 25., 13.],\n",
              "        [ 0., 24., 12.],\n",
              "        [ 0., 23., 11.],\n",
              "        [ 0., 22., 10.],\n",
              "        [ 0., 21.,  0.],\n",
              "        [ 0., 20.,  0.]])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence, pack_sequence, PackedSequence\n",
        "short = torch.arange(3, -1, -1).float() # [3, 2, 1, 0]\n",
        "long = torch.arange(27,19, -1).float()\n",
        "middle = torch.arange(15,9, -1).float()\n",
        "\n",
        "pad_sequence([short, long, middle], batch_first=False)  # T x N "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b2a0920",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b2a0920",
        "outputId": "1e5cc308-9354-4336-8bf0-7dc50ed72061"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 3.,  2.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
              "        [27., 26., 25., 24., 23., 22., 21., 20.],\n",
              "        [15., 14., 13., 12., 11., 10.,  0.,  0.]])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Default value of batch_first in pad_sequence is False.\n",
        "# So you have to always be careful not to miss batch_first=True in pad_sequence, if you use batch_first=True for your RNN layer.\n",
        "pad_sequence([short, long, middle], batch_first=True)  # N x T "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fb5e0dd",
      "metadata": {
        "id": "4fb5e0dd"
      },
      "source": [
        "1) However, the problem is that you can't figure out whether the 0 at the end of each sequence is a padded one, or was included in the original sequence\n",
        "- e.g. `[2, 3, 4, 3, 0]` becomes `[ 2,  3,  4,  5,  0,  0,  0]`. Now we don't know how many zeros were added for padding\n",
        "\n",
        "2) Also, if you run RNN for this padded sequence, RNN will calculate for the padded part also.\n",
        "- RNN doesn't know whether it is padded data, or existing data\n",
        "- This makes computation slower\n",
        "\n",
        "3) If you want to use bi-directional, which also reads the sequence from backward, paddings can make the result different.\n",
        "\n",
        "To solve this issue, we use PackedSequence, by using `pack_sequence`/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69086d43",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69086d43",
        "outputId": "0091d96c-8bc5-4323-9a23-78935a457cb6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PackedSequence(data=tensor([27., 15.,  3., 26., 14.,  2., 25., 13.,  1., 24., 12.,  0., 23., 11.,\n",
              "        22., 10., 21., 20.]), batch_sizes=tensor([3, 3, 3, 3, 2, 2, 1, 1]), sorted_indices=tensor([1, 2, 0]), unsorted_indices=tensor([2, 0, 1]))"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "packed_sequence = pack_sequence([short, long, middle], enforce_sorted=False)\n",
        "packed_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cd50747",
      "metadata": {
        "id": "4cd50747"
      },
      "source": [
        "`PackedSequence` has `data` and `batch_sizes`\n",
        "- `data` contains the flattened value of given batch\n",
        "    - To optimize the computation, the sequences have to be sorted by descending of length\n",
        "- `batch_sizes` represents how many valid batch sample exists for each time step\n",
        "    - `[3, 3, 3, 2, 2, 1, 1]` means that there are 3 sequences for first three time steps, and then 2 sequences for next two steps, and then only 1 sequence for next two steps.\n",
        "- `sorted_indices` shows how the sorted sequences can be converted to original order.\n",
        "    - `[1,2,0]` means that \n",
        "        - the 0th sequence in the sorted sequences (the longest one) was indexed as 1 in the original input batch\n",
        "        - the 1st sequence in the sorted sequences (`middle`) was indexed as 2 in the original input batch\n",
        "        - the 2nd sequence in the sorted sequences (`short`) was index as 0 in the original input batch\n",
        "- `unsorted_indices` shows how the original sequences are sorted.\n",
        "    - `[2,0,1]` means that\n",
        "        - the 0th sequence in the original input was sorted as 2nd in the sorted sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0500f200",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0500f200",
        "outputId": "f78e0501-2311-4907-bf8c-9926c26f7da4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type of output of RNN for PackedSequence: <class 'torch.nn.utils.rnn.PackedSequence'>\n",
            "Type of last_hidden of RNN for PackedSequence: <class 'torch.Tensor'>\n"
          ]
        }
      ],
      "source": [
        "rnn_layer = nn.GRU(1, 1)\n",
        "packed_sequence = pack_sequence([short.unsqueeze(1), long.unsqueeze(1), middle.unsqueeze(1)], enforce_sorted=False)\n",
        "out, last_hidden = rnn_layer(packed_sequence)\n",
        "\n",
        "print(f\"Type of output of RNN for PackedSequence: {type(out)}\")\n",
        "print(f\"Type of last_hidden of RNN for PackedSequence: {type(last_hidden)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f2e3e44",
      "metadata": {
        "id": "8f2e3e44"
      },
      "source": [
        "- RNN or its family of PyTorch can automatically handle `PackedSequence`\n",
        "- However, other layers like `nn.Embedding` or `nn.Linear` cannot take `PackedSequence` as its input\n",
        "- There are two ways to feed `PackedSequence` to these layers\n",
        "    - First, convert PackedSequence to ordinary torch.Tensor by `torch.nn.utils.rnn.pad_packed_sequence`\n",
        "        - This will convert PackedSequence to a tensor of sequneces with same length but different padding\n",
        "    - The other way is to feed only PackedSequence.data, and then declaring new PackedSequence with the output as `data`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3XBGEIV9sTJ3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XBGEIV9sTJ3",
        "outputId": "a1dfa2b2-9289-4b9d-e36f-beeebff82c26"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PackedSequence(data=tensor([[27.],\n",
              "        [15.],\n",
              "        [ 3.],\n",
              "        [26.],\n",
              "        [14.],\n",
              "        [ 2.],\n",
              "        [25.],\n",
              "        [13.],\n",
              "        [ 1.],\n",
              "        [24.],\n",
              "        [12.],\n",
              "        [ 0.],\n",
              "        [23.],\n",
              "        [11.],\n",
              "        [22.],\n",
              "        [10.],\n",
              "        [21.],\n",
              "        [20.]]), batch_sizes=tensor([3, 3, 3, 3, 2, 2, 1, 1]), sorted_indices=tensor([1, 2, 0]), unsorted_indices=tensor([2, 0, 1]))"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "packed_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b08620c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "b08620c8",
        "outputId": "cfd89d73-ff95-4355-aefe-1983a840d1fe"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-7c62b9259c97>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m '''\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_linear_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtest_linear_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not PackedSequence"
          ]
        }
      ],
      "source": [
        "'''\n",
        "This will make error, because other layers cannot handle PackedSequence\n",
        "'''\n",
        "test_linear_layer = nn.Linear(in_features=1, out_features=2)\n",
        "test_linear_layer(packed_sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac9efa09",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac9efa09",
        "outputId": "6e266ee3-8dca-4314-fe10-91ee15a0def3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The padded sequence generated from packed sequence (squeezed for printing): \n",
            " tensor([[ 3., 27., 15.],\n",
            "        [ 2., 26., 14.],\n",
            "        [ 1., 25., 13.],\n",
            "        [ 0., 24., 12.],\n",
            "        [ 0., 23., 11.],\n",
            "        [ 0., 22., 10.],\n",
            "        [ 0., 21.,  0.],\n",
            "        [ 0., 20.,  0.]])\n",
            "\"pad_packed_sequence\" also returns \"batch_lengths\", to clarify the original length before the padding: \n",
            " tensor([4, 8, 6])\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "One way to to this is using torch.nn.utils.rnn.pad_packed_sequence to convert PackedSequence to ordinary padded tensor\n",
        "'''\n",
        "\n",
        "from torch.nn.utils.rnn import pad_packed_sequence\n",
        "padded_sequence, batch_lengths = pad_packed_sequence(packed_sequence)\n",
        "print(f'The padded sequence generated from packed sequence (squeezed for printing): \\n {padded_sequence.squeeze()}')\n",
        "print(f'\"pad_packed_sequence\" also returns \"batch_lengths\", to clarify the original length before the padding: \\n {batch_lengths}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e302fe4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e302fe4",
        "outputId": "af6d0e8e-3e46-426a-f23f-ce6e534dff67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output of feeding padded_sequence to a linear layer: tensor([[[ 3.3898,  1.8743],\n",
            "         [24.5948, 12.2365],\n",
            "         [13.9923,  7.0554]],\n",
            "\n",
            "        [[ 2.5063,  1.4426],\n",
            "         [23.7113, 11.8048],\n",
            "         [13.1088,  6.6237]],\n",
            "\n",
            "        [[ 1.6227,  1.0108],\n",
            "         [22.8277, 11.3730],\n",
            "         [12.2252,  6.1919]],\n",
            "\n",
            "        [[ 0.7392,  0.5791],\n",
            "         [21.9442, 10.9413],\n",
            "         [11.3417,  5.7602]],\n",
            "\n",
            "        [[ 0.7392,  0.5791],\n",
            "         [21.0606, 10.5095],\n",
            "         [10.4581,  5.3284]],\n",
            "\n",
            "        [[ 0.7392,  0.5791],\n",
            "         [20.1771, 10.0777],\n",
            "         [ 9.5746,  4.8966]],\n",
            "\n",
            "        [[ 0.7392,  0.5791],\n",
            "         [19.2936,  9.6460],\n",
            "         [ 0.7392,  0.5791]],\n",
            "\n",
            "        [[ 0.7392,  0.5791],\n",
            "         [18.4100,  9.2142],\n",
            "         [ 0.7392,  0.5791]]], grad_fn=<ViewBackward0>)\n",
            "Caution that it returns non-zero values for timestep with zero padding, because linear layer has a bias\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Now you can feed padded sequence to linear layer.\n",
        "'''\n",
        "\n",
        "linear_output = test_linear_layer(padded_sequence)\n",
        "print(f\"Output of feeding padded_sequence to a linear layer: {linear_output}\")\n",
        "print(\"Caution that it returns non-zero values for timestep with zero padding, because linear layer has a bias\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1c0e150",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1c0e150",
        "outputId": "8cb4f6cd-0d17-4eda-f300-85bc96e46004"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PackedSequence(data=tensor([[24.5948, 12.2365],\n",
              "        [13.9923,  7.0554],\n",
              "        [ 3.3898,  1.8743],\n",
              "        [23.7113, 11.8048],\n",
              "        [13.1088,  6.6237],\n",
              "        [ 2.5063,  1.4426],\n",
              "        [22.8277, 11.3730],\n",
              "        [12.2252,  6.1919],\n",
              "        [ 1.6227,  1.0108],\n",
              "        [21.9442, 10.9413],\n",
              "        [11.3417,  5.7602],\n",
              "        [ 0.7392,  0.5791],\n",
              "        [21.0606, 10.5095],\n",
              "        [10.4581,  5.3284],\n",
              "        [20.1771, 10.0777],\n",
              "        [ 9.5746,  4.8966],\n",
              "        [19.2936,  9.6460],\n",
              "        [18.4100,  9.2142]], grad_fn=<PackPaddedSequenceBackward0>), batch_sizes=tensor([3, 3, 3, 3, 2, 2, 1, 1]), sorted_indices=tensor([1, 2, 0]), unsorted_indices=tensor([2, 0, 1]))"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "You can make the output as a PackedSequence, by using torch.nn.utils.rnn.pack_padded_sequence\n",
        "'''\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "re_packed_sequence = pack_padded_sequence(linear_output, batch_lengths, enforce_sorted=False)\n",
        "re_packed_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22117c41",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22117c41",
        "outputId": "9f1bbb93-b344-46c3-dc04-e41d059ad0b6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PackedSequence(data=tensor([[24.5948, 12.2365],\n",
              "        [13.9923,  7.0554],\n",
              "        [ 3.3898,  1.8743],\n",
              "        [23.7113, 11.8048],\n",
              "        [13.1088,  6.6237],\n",
              "        [ 2.5063,  1.4426],\n",
              "        [22.8277, 11.3730],\n",
              "        [12.2252,  6.1919],\n",
              "        [ 1.6227,  1.0108],\n",
              "        [21.9442, 10.9413],\n",
              "        [11.3417,  5.7602],\n",
              "        [ 0.7392,  0.5791],\n",
              "        [21.0606, 10.5095],\n",
              "        [10.4581,  5.3284],\n",
              "        [20.1771, 10.0777],\n",
              "        [ 9.5746,  4.8966],\n",
              "        [19.2936,  9.6460],\n",
              "        [18.4100,  9.2142]], grad_fn=<AddmmBackward0>), batch_sizes=tensor([3, 3, 3, 3, 2, 2, 1, 1]), sorted_indices=tensor([1, 2, 0]), unsorted_indices=tensor([2, 0, 1]))"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "Another way to do it is using PackedSequence.data\n",
        "'''\n",
        "\n",
        "linear_out_pack = test_linear_layer(packed_sequence.data)\n",
        "packed_sequence_after_linear = PackedSequence(linear_out_pack, packed_sequence.batch_sizes, packed_sequence.sorted_indices, packed_sequence.unsorted_indices)\n",
        "packed_sequence_after_linear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "81dd119c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81dd119c",
        "outputId": "ad0c5921-f0b5-4734-cbb6-bfd700049a36"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(PackedSequence(data=tensor([    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "             2,     2, 28573,  4195, 12333,  7014,  9772,  3776,  2215, 11619,\n",
              "         23926,   147,   145,  7560, 15374, 11978, 21442, 19122,  2290, 11065,\n",
              "          7729,  7381,  9773,  6481,  6959,  8027,  6585,  6528,  6853,  3558,\n",
              "          2592,  3558,  6528, 15446,  8527,  8210,  4331, 31129, 14356, 27365,\n",
              "          4449, 24769,  9339,  3194,  6785, 10006,  8549, 15988,  4379, 31311,\n",
              "         13374,  9145,    16,  3558, 10143,  7924,  6645,    77, 13945,  3558,\n",
              "          9688,  3202,  4632, 10573,  6851, 10444,  4345,  8468,  7248, 21415,\n",
              "         15270,  3536,  7532,  6468,  4342,  4477,  7283,  9840, 12054,  4355,\n",
              "          6963,  6751,  6442,  4322,  8952, 20864,  6412,  8604, 19107,  4630,\n",
              "          4203,  8201, 14027, 15917,  4668, 23130,  6741,  8236, 11464,  8251,\n",
              "         23142,  9433,    12, 22790, 10556, 15251,  4217,  7133,  4331,  4325,\n",
              "          4416,  9197,    16,  6447,    16,    79,  6412, 11448,  7985,  5078,\n",
              "         13403,  8005, 18309,  2973,  6475,  4345,    16, 10384,  6604,  6447,\n",
              "           214,  4322,  8925,  4410, 11908,  4325,  6727,  4330,  4539,  2153,\n",
              "          7765, 19914,  6475, 17348, 10806, 21172, 29084,  5209,    71, 11473,\n",
              "          6557, 16130, 30333, 19391,  6428, 10348,  7366,  3202, 25852, 15396,\n",
              "         16102, 19211,  2170,   145,  4227, 12604, 17291,  6502,  6641,  4108,\n",
              "          4731,  4417,    16,  6959, 12821, 13349, 21442,  8107, 15647, 10100,\n",
              "          8716,  6422,  8051, 12613,  6420,  7596, 12217, 12251,  7307,  8092,\n",
              "          4320,  4336,  8846,    18, 22426,  3314, 12152, 21052,  4427,  2426,\n",
              "         14593,  4339,  7620,  7905,  6642,  6645,  4331, 12502,  4379,  7674,\n",
              "          6434,    16,  4631, 12500,  7124,  4684, 18659,  6582, 13146, 10062,\n",
              "          8980,  6476, 23137, 14168,  6887,     3,  2997,  4387,  6984,  4402,\n",
              "            13, 14144,  4443,    21,  4458,  7183,  6785,  3156, 16569,  6787,\n",
              "         15096,  6427, 11747,    82,    17, 29499,  3631, 10302,  7610,    77,\n",
              "          2592,  6699, 18284, 18535, 17784,  6419, 21062,  5479, 14632, 16994,\n",
              "          8041,  2849,  7879,  3061,  4395,  4313,   148,   146, 19281, 10373,\n",
              "          6967,  9697,  7526, 18201,  4856,    88,  8921, 11849, 12697,  5017,\n",
              "          4630, 15870,  9125,  7911,  6507,    18,    18,    18,  4336,  6480,\n",
              "         13857,  8354, 24761, 21422, 29226,    16, 10094,  6487,  7609,  6553,\n",
              "          6776, 22448,  6444,  2624,  4879, 17729, 25277,  6415, 11647,  6415,\n",
              "          6411,  4330,  8725,  6420,  9843,    18,     3,     3,     3,   215,\n",
              "         28149,  6907, 25992,  6420,  6438,   147, 13831,  4470,  3236,  2805,\n",
              "          9267,  3402,  4387, 14319,  9666,  4330,    16, 13306,  6442,  8045,\n",
              "          6549,  3382, 11452, 13825,  2192,    18,     3,  3474, 11131, 19374,\n",
              "          9056,  4058,    16,  3685,  4330,  4630,  4544,  4333, 17063, 10894,\n",
              "         21594,  4325,    16, 17440,  6428,  4387,  7092, 24287,  9563,  3252,\n",
              "          4401,  6553,  7777,     3, 15850,  7953,  6421,  6881, 10449, 23240,\n",
              "          7863,  4108,  4336,  8178, 25220,  4131, 15896,  4380, 13421, 26123,\n",
              "          4379, 17729, 11970,  6435,  4161,  6417,  7824, 13002,  6414,    18,\n",
              "          4592,  7385, 21700,  6624,  4217,  3379,  6420,  4339,  3202,  2159,\n",
              "         12063,  4807, 26198, 14958,  8385, 19122,  4227,    16,  4154,  7803,\n",
              "         11088,  6507,    18,    18,    18,     3,   214,    11, 26423,  6533,\n",
              "          4553,  4382,  7708,    22,  4727, 19656,  3975,  4582, 11959,  4330,\n",
              "          2192,  6676,    87, 21801,  3252,  4154,    18,    18,     3,     3,\n",
              "             3,  4102,  3547,  4345,   147,  8328,  6474,  4203, 29224, 10270,\n",
              "          4477,  4360,  3675,  9703, 29772,    16, 14567,  4495,  4456,  6420,\n",
              "          3252,     3,     3,  8178,  7171,  8362,  2154, 16248,  6874,  4718,\n",
              "         24542,  4084, 23285,  8620,  6423,  6635, 14360,  9993,  4325,  4496,\n",
              "         10181, 10676,  6414,  7247,  8972,  7491, 19571, 24391, 14431, 21350,\n",
              "         15443,  7550,  3194,  3060, 12854, 12455, 10979,  6440, 27499,  4342,\n",
              "         19119,  8735,    18,   215,  8673,  7025,  3828,  4387, 19610,  4416,\n",
              "         17186,  9840,  4477,   104,  4322,  4151,  4639, 21913,  6476, 22414,\n",
              "          4320,    35,     3,  2153,   145, 11608,  4410, 22133,  8234,  3063,\n",
              "          4443, 17486,  4317,  2970, 19430,  4451,  9063,  7069,  7781, 24132,\n",
              "            18,     3,  6809, 10757, 29319, 10545,  2578, 27471,  9725,  6455,\n",
              "          6767,  2602,  7442, 10421,  4710,  8535,  7112, 30583,    18,     3,\n",
              "         12688, 25831,    16,  7523,  8391,  4379,  2675,  7553, 10135,  4610,\n",
              "          4401,  4331,  6434,  8858, 13304,  6417,     3,    16,  9647,   145,\n",
              "          9136,  4648,  6877,  8507,  7614,  4381,  4744,  6785,  8954,  8402,\n",
              "          9907, 19987,  6420, 28573,    11, 11515,    24, 27608,  4355,  2565,\n",
              "         24769, 15098,  4387, 31866,  4342,  4485,  4330,  6425,  6537, 19789,\n",
              "          3474, 13157,  7320,  2189,  4901, 10301,  9976,  9655, 12161,  6803,\n",
              "         23562,  6489, 20837,    18,    18,  4345,   145,  4312,  6794,  4831,\n",
              "          4336,  6415,  4598,  7610, 12716,  8660, 19603,    18,    18,     3,\n",
              "             3,  7798, 15876,   146, 11216,  7237, 11909,  7975, 12699, 21454,\n",
              "          2578, 19569,    18,     3,     3, 20154,   104,  7609,  6555, 13300,\n",
              "          2809, 18255,  7523,    20,  3953,  6764,     3,   104, 15312,  7940,\n",
              "          9123,  3410,  4777, 15847,  4384,    18, 11438,  6485, 12228, 21281,\n",
              "         30015,  3610,  4818,  6447, 10220, 22151, 30682, 18382,    18, 20154,\n",
              "          4510,  3852, 31441, 10527,  7616,  8988,  9550, 10027,    18,     3,\n",
              "           104,    11,  4352,  6412, 14356, 14735,  4355,  7062,  6543,     3,\n",
              "          7367,  2849,  4683, 13645,  8610, 18493,  7970,  6537,    18, 28391,\n",
              "          7436,  4322,  7031,  4345,  6504,   148,    18,     3,   104, 10092,\n",
              "          3600, 16798,  2565,  6447,  2220,     3,  6688,  6810, 12590, 13511,\n",
              "          3092,  6755, 13435,  9780,  7150, 28148,  2192, 20630,    18,    18,\n",
              "         20154, 12421,  4830,   148,    18,     3,     3,  7063, 13830,  4473,\n",
              "          6673,     3, 11397,  4710,  6799,  6535,  7614,  7799, 31830,    18,\n",
              "          4131,  6414,  6416,     3,  4386,    18,    18,  4187,     3,     3,\n",
              "          4338,  5020,  4319,    16,  9435, 10657,  4319,    16, 11466,  4540,\n",
              "          4370,  4437,  8345,    16,  3537,  4449, 19525,    16, 16190, 10657,\n",
              "            16, 18078,  9780,    16, 10857,  6639,  7125,  6608,  9893,    18,\n",
              "             3]), batch_sizes=tensor([32, 32, 32, 32, 32, 32, 32, 32, 31, 31, 31, 28, 27, 26, 26, 25, 22, 20,\n",
              "         20, 20, 19, 18, 17, 16, 16, 16, 16, 14, 12, 11, 11, 11, 10,  9,  9,  8,\n",
              "          7,  7,  7,  5,  4,  4,  4,  3,  3,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "          1,  1,  1,  1]), sorted_indices=tensor([21, 17, 14,  5, 27,  8, 24, 28,  3,  9, 13, 29,  4, 16,  0, 19, 22,  2,\n",
              "         10, 20, 25, 23, 31, 15,  7, 18, 11, 12, 26, 30,  1,  6]), unsorted_indices=tensor([14, 30, 17,  8, 12,  3, 31, 24,  5,  9, 18, 26, 27, 10,  2, 23, 13,  1,\n",
              "         25, 15, 19,  0, 16, 21,  6, 20, 28,  4,  7, 11, 29, 22])),\n",
              " PackedSequence(data=tensor([    2,     2,     2,  ...,  2405, 11077,    18]), batch_sizes=tensor([32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 31, 30, 30, 29, 27, 24, 24, 21,\n",
              "         21, 21, 20, 20, 20, 20, 19, 19, 18, 17, 17, 16, 15, 15, 14, 14, 13, 13,\n",
              "         12, 12,  9,  8,  7,  7,  7,  7,  7,  7,  7,  6,  5,  5,  5,  5,  4,  4,\n",
              "          4,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1,  1,  1,  1,  1,\n",
              "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1]), sorted_indices=tensor([21,  5, 24, 13,  3, 17,  8,  4, 14, 16, 28, 27, 29,  0, 19,  9,  2, 22,\n",
              "         25, 20, 18, 23, 31, 11, 10,  7, 30, 15,  1, 12, 26,  6]), unsorted_indices=tensor([13, 28, 16,  4,  7,  1, 31, 25,  6, 15, 24, 23, 29,  3,  8, 27,  9,  5,\n",
              "         20, 14, 19,  0, 17, 21,  2, 18, 30, 11, 10, 12, 26, 22])),\n",
              " PackedSequence(data=tensor([ 1107,  1039,  7208,  ..., 11077,    18,     3]), batch_sizes=tensor([32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 31, 30, 30, 29, 27, 24, 24, 21,\n",
              "         21, 21, 20, 20, 20, 20, 19, 19, 18, 17, 17, 16, 15, 15, 14, 14, 13, 13,\n",
              "         12, 12,  9,  8,  7,  7,  7,  7,  7,  7,  7,  6,  5,  5,  5,  5,  4,  4,\n",
              "          4,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1,  1,  1,  1,  1,\n",
              "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1]), sorted_indices=tensor([21,  5, 24, 13,  3, 17,  8,  4, 14, 16, 28, 27, 29,  0, 19,  9,  2, 22,\n",
              "         25, 20, 18, 23, 31, 11, 10,  7, 30, 15,  1, 12, 26,  6]), unsorted_indices=tensor([13, 28, 16,  4,  7,  1, 31, 25,  6, 15, 24, 23, 29,  3,  8, 27,  9,  5,\n",
              "         20, 14, 19,  0, 17, 21,  2, 18, 30, 11, 10, 12, 26, 22])))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "from torch.nn.utils.rnn import pack_sequence, PackedSequence\n",
        "\n",
        "def pack_collate(raw_batch):\n",
        "  source, target, shifted_target = zip(*raw_batch)\n",
        "  return pack_sequence(source, enforce_sorted=False), pack_sequence(target, enforce_sorted=False), pack_sequence(shifted_target, enforce_sorted=False)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=32, collate_fn=pack_collate)\n",
        "batch = next(iter(train_loader))\n",
        "batch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src, tgt, shifted_tgt = batch\n",
        "src.sorted_indices, tgt.sorted_indices"
      ],
      "metadata": {
        "id": "LiWbuM8SPVop",
        "outputId": "21a36a56-2ddd-4d40-b603-223caef71a3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "LiWbuM8SPVop",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([21, 17, 14,  5, 27,  8, 24, 28,  3,  9, 13, 29,  4, 16,  0, 19, 22,  2,\n",
              "         10, 20, 25, 23, 31, 15,  7, 18, 11, 12, 26, 30,  1,  6]),\n",
              " tensor([21,  5, 24, 13,  3, 17,  8,  4, 14, 16, 28, 27, 29,  0, 19,  9,  2, 22,\n",
              "         25, 20, 18, 23, 31, 11, 10,  7, 30, 15,  1, 12, 26,  6]))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "357c61f4",
      "metadata": {
        "id": "357c61f4"
      },
      "source": [
        "## Define Model\n",
        "![image](https://raw.githubusercontent.com/tensorflow/nmt/master/nmt/g3doc/img/seq2seq.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "0af0ca29",
      "metadata": {
        "id": "0af0ca29"
      },
      "outputs": [],
      "source": [
        "class Seq2seq(nn.Module):\n",
        "  def __init__(self, enc_vocab, dec_vocab, hidden_size, num_layers=2):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(enc_vocab, hidden_size, num_layers=num_layers)\n",
        "    self.decoder = Decoder(dec_vocab, hidden_size, num_layers=num_layers)\n",
        "\n",
        "  def forward(self, src, tgt):\n",
        "    enc_out = self.encoder(src)\n",
        "    if isinstance(src, PackedSequence) and isinstance(tgt, PackedSequence):\n",
        "      enc_out = enc_out[:, src.unsorted_indices][:, tgt.sorted_indices ]\n",
        "    dec_out = self.decoder(tgt, enc_out)\n",
        "    return dec_out \n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, num_vocab, hidden_size, num_layers=2):\n",
        "    super().__init__()\n",
        "    self.emb = nn.Embedding(num_vocab, hidden_size)\n",
        "    self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True, num_layers=num_layers)\n",
        "    # batch_first True: it takes (Num_samples_in_batch, num_timesteps, num_dim)\n",
        "    # batch_first False: it takes (num_timesteps, Num_samples_in_batch, num_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    if isinstance(x, PackedSequence):\n",
        "      emb = PackedSequence(self.emb(x.data), batch_sizes=x.batch_sizes, sorted_indices=x.sorted_indices, unsorted_indices=x.unsorted_indices)\n",
        "    else:\n",
        "      emb = self.emb(x)\n",
        "    out, last_hidden = self.rnn(emb)\n",
        "    return last_hidden\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, num_vocab, hidden_size, num_layers=2):\n",
        "    super().__init__()\n",
        "    self.emb = nn.Embedding(num_vocab, hidden_size)\n",
        "    self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True, num_layers=num_layers)\n",
        "    self.proj = nn.Linear(hidden_size, num_vocab)\n",
        "\n",
        "\n",
        "  def forward(self, x, enc_output):\n",
        "    if isinstance(x, PackedSequence):\n",
        "      emb = PackedSequence(self.emb(x.data), batch_sizes=x.batch_sizes, sorted_indices=x.sorted_indices, unsorted_indices=x.unsorted_indices)\n",
        "      out, last_hidden = self.rnn(emb, enc_output)\n",
        "      logit = PackedSequence(self.proj(out.data), batch_sizes=out.batch_sizes, sorted_indices=out.sorted_indices, unsorted_indices=out.unsorted_indices)\n",
        "    else:\n",
        "      emb = self.emb(x)\n",
        "      out, last_hidden = self.rnn(emb, enc_output)\n",
        "      logit = self.proj(out)\n",
        "    return logit\n",
        "    \n",
        "hidden_size = 64\n",
        "model = Seq2seq(tokenizer_src.vocab_size, tokenizer_tgt.vocab_size, hidden_size)\n",
        "encoder = Encoder(tokenizer_src.vocab_size, hidden_size)\n",
        "decoder = Decoder(tokenizer_tgt.vocab_size, hidden_size)\n",
        "\n",
        "src, tgt, shifted_tgt = batch\n",
        "\n",
        "enc_out = encoder(src)\n",
        "dec_out = decoder(tgt, enc_out)\n",
        "\n",
        "logit = model(src, tgt)\n",
        "# logit.shape, logit.softmax(dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unsorted_enc_out = enc_out[:, src.unsorted_indices]\n",
        "target_sorted_enc_out = unsorted_enc_out[:, tgt.sorted_indices]"
      ],
      "metadata": {
        "id": "Ld6OZjJ_Qdar"
      },
      "id": "Ld6OZjJ_Qdar",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logit"
      ],
      "metadata": {
        "id": "lj7zFQPqOkNk",
        "outputId": "8e09d4dd-2978-4ad7-c4ac-6eec740df0cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "lj7zFQPqOkNk",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PackedSequence(data=tensor([[-0.1037,  0.1473,  0.0048,  ..., -0.0047, -0.1110, -0.0725],\n",
              "        [-0.1180,  0.1796,  0.0051,  ..., -0.0034, -0.1301, -0.0628],\n",
              "        [-0.1426,  0.1752,  0.0121,  ..., -0.0245, -0.0783, -0.0516],\n",
              "        ...,\n",
              "        [ 0.0704,  0.0287,  0.0330,  ...,  0.0372, -0.1235,  0.0724],\n",
              "        [ 0.0297,  0.0548, -0.0590,  ..., -0.0222, -0.1012,  0.0353],\n",
              "        [ 0.0028,  0.0724, -0.0668,  ...,  0.0017, -0.0646,  0.0580]],\n",
              "       grad_fn=<AddmmBackward0>), batch_sizes=tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 7, 7, 7, 7, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
              "        5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "        2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), sorted_indices=tensor([5, 3, 4, 0, 2, 7, 1, 6]), unsorted_indices=tensor([3, 6, 4, 1, 2, 0, 7, 5]))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WMGUwt10Whf5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMGUwt10Whf5",
        "outputId": "53bf8946-490b-41cd-ca60-8eaa32b28fe9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([1, 10, 30679]),\n",
              " torch.Size([1, 10]),\n",
              " tensor([[  76,   10,  746,  810,  906,  335,  412, 2590,   17,    3]]))"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "logit.shape, shifted_tgt.shape, shifted_tgt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "WgvICWU0Wg0j",
      "metadata": {
        "id": "WgvICWU0Wg0j"
      },
      "outputs": [],
      "source": [
        "def nll_loss(pred, target, eps=1e-8):\n",
        "  if pred.ndim == 3:\n",
        "    pred = pred.flatten(0, 1)\n",
        "  if target.ndim == 2:\n",
        "    target = target.flatten(0, 1)\n",
        "  assert pred.ndim == 2\n",
        "  assert target.ndim == 1\n",
        "\n",
        "  \n",
        "  return -torch.log(pred[torch.arange(len(target)), target] + eps).mean()\n",
        "\n",
        "# loss = nll_loss(logit.softmax(dim=-1), shifted_tgt)\n",
        "# loss, loss.shape, shifted_tgt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13ffb21b",
      "metadata": {
        "id": "13ffb21b"
      },
      "source": [
        "## Define Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "64a3758c",
      "metadata": {
        "id": "64a3758c"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "class Trainer:\n",
        "  def __init__(self, model, optimizer, loss_fn, train_loader, valid_loader, device):\n",
        "    self.model = model\n",
        "    self.optimizer = optimizer\n",
        "    self.loss_fn = loss_fn\n",
        "    self.train_loader = train_loader\n",
        "    self.valid_loader = valid_loader\n",
        "    \n",
        "    self.model.to(device)\n",
        "    \n",
        "    self.best_valid_accuracy = 0\n",
        "    self.device = device\n",
        "    \n",
        "    self.training_loss = []\n",
        "    self.validation_loss = []\n",
        "    self.validation_acc = []\n",
        "\n",
        "  def save_model(self, path='kor_eng_translator_model.pt'):\n",
        "    torch.save({'model':self.model.state_dict(), 'optim':self.optimizer.state_dict()}, path)\n",
        "    \n",
        "  def train_by_num_epoch(self, num_epochs):\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "      self.model.train()\n",
        "      for batch in tqdm(self.train_loader, leave=False):\n",
        "        loss_value = self._train_by_single_batch(batch)\n",
        "        self.training_loss.append(loss_value)\n",
        "      self.model.eval()\n",
        "      validation_loss, validation_acc = self.validate()\n",
        "      self.validation_loss.append(validation_loss)\n",
        "      self.validation_acc.append(validation_acc)\n",
        "      \n",
        "      if validation_acc > self.best_valid_accuracy:\n",
        "        print(f\"Saving the model with best validation accuracy: Epoch {epoch+1}, Acc: {validation_acc:.4f} \")\n",
        "        self.save_model('kor_eng_translator_model_best.pt')\n",
        "      else:\n",
        "        self.save_model('kor_eng_translator_model_last.pt')\n",
        "      self.best_valid_accuracy = max(validation_acc, self.best_valid_accuracy)\n",
        "\n",
        "      \n",
        "  def _train_by_single_batch(self, batch):\n",
        "    '''\n",
        "    This method updates self.model's parameter with a given batch\n",
        "    \n",
        "    batch (tuple): (batch_of_input_text, batch_of_label)\n",
        "    \n",
        "    You have to use variables below:\n",
        "    \n",
        "    self.model (SentimentModel/torch.nn.Module): A neural network model\n",
        "    self.optimizer (torch.optim.adam.Adam): Adam optimizer that optimizes model's parameter\n",
        "    self.loss_fn (function): function for calculating BCE loss for a given prediction and target\n",
        "    self.device (str): 'cuda' or 'cpu'\n",
        "\n",
        "    output: loss (float): Mean binary cross entropy value for every sample in the training batch\n",
        "    The model's parameters, optimizer's steps has to be updated inside this method\n",
        "\n",
        "    TODO: Complete this method \n",
        "    '''\n",
        "    src, tgt, shifted_tgt = batch\n",
        "    src = src.to(self.device)\n",
        "    tgt = tgt.to(self.device)\n",
        "    shifted_tgt = shifted_tgt.to(self.device)\n",
        "\n",
        "    logit = self.model(src, tgt)\n",
        "\n",
        "    if isinstance(logit, PackedSequence):\n",
        "      prob = logit.data.softmax(dim=-1)\n",
        "      loss = self.loss_fn(prob, shifted_tgt.data)\n",
        "    else:\n",
        "      loss = self.loss_fn(prob, shifted_tgt)\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "    self.optimizer.zero_grad()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "    \n",
        "  def validate(self, external_loader=None):\n",
        "    '''\n",
        "    This method calculates accuracy and loss for given data loader.\n",
        "    It can be used for validation step, or to get test set result\n",
        "    \n",
        "    input:\n",
        "      data_loader: If there is no data_loader given, use self.valid_loader as default.\n",
        "      \n",
        "    \n",
        "    output: \n",
        "      validation_loss (float): Mean Binary Cross Entropy value for every sample in validation set\n",
        "      validation_accuracy (float): Mean Accuracy value for every sample in validation set\n",
        "      \n",
        "    TODO: Complete this method \n",
        "\n",
        "    '''\n",
        "    \n",
        "    ### Don't change this part\n",
        "    if external_loader and isinstance(external_loader, DataLoader):\n",
        "      loader = external_loader\n",
        "      print('An arbitrary loader is used instead of Validation loader')\n",
        "    else:\n",
        "      loader = self.valid_loader\n",
        "      \n",
        "    self.model.eval()\n",
        "    \n",
        "    '''\n",
        "    Write your code from here, using loader, self.model, self.loss_fn.\n",
        "    '''\n",
        "\n",
        "hidden_size = 128\n",
        "model = Seq2seq(tokenizer_src.vocab_size, tokenizer_tgt.vocab_size, hidden_size)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_fn = nll_loss\n",
        "train_loader = DataLoader(train_set, batch_size=64, shuffle=True, collate_fn=pack_collate)\n",
        "valid_loader = DataLoader(valid_set, batch_size=128, shuffle=False, collate_fn=pack_collate)\n",
        "device = 'cuda'\n",
        "\n",
        "trainer = Trainer(model, optimizer, loss_fn, train_loader, valid_loader, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "-zNWE0znYwwf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zNWE0znYwwf",
        "outputId": "c80a4226-068c-4d20-d998-6cc63348d693"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10.359963417053223, 10.313287734985352, 10.263622283935547, 10.206441879272461, 10.136489868164062, 10.047164916992188, 9.930057525634766, 9.775178909301758, 9.572907447814941, 9.3184232711792, 9.017634391784668, 8.689282417297363, 8.35786247253418, 8.042617797851562, 7.753167629241943, 7.492118835449219, 7.258220672607422, 7.048532962799072, 6.860129356384277, 6.690823554992676, 6.539017677307129, 6.403351306915283, 6.282470703125, 6.174984931945801, 6.079526901245117, 5.994827747344971, 5.9197821617126465, 5.853461742401123, 5.795093536376953, 5.744019031524658, 5.699643611907959, 5.661417484283447, 5.628849506378174, 5.601459503173828, 5.578672409057617, 5.559847354888916, 5.544384956359863, 5.531750679016113, 5.521424770355225, 5.512904167175293, 5.5057172775268555, 5.49946403503418, 5.4938130378723145, 5.488489151000977, 5.483280181884766, 5.478091239929199, 5.47297477722168, 5.468082904815674, 5.463491439819336, 5.458996295928955, 5.4544267654418945, 5.449832916259766, 5.445329666137695, 5.441020488739014, 5.436959266662598, 5.4331254959106445, 5.429429531097412, 5.4257659912109375, 5.422093868255615, 5.418450832366943, 5.4149065017700195, 5.411503314971924, 5.408212184906006, 5.4049296379089355, 5.401577949523926, 5.398151874542236, 5.394686222076416, 5.3912034034729, 5.387701511383057, 5.384198188781738, 5.380713939666748, 5.377253532409668, 5.37380313873291, 5.370333671569824, 5.366819381713867, 5.363255023956299, 5.359652042388916, 5.356027126312256, 5.3523945808410645, 5.348753929138184, 5.345085144042969, 5.341372966766357, 5.337623119354248, 5.333868980407715, 5.330146789550781, 5.326449394226074, 5.322743892669678, 5.318997859954834, 5.3151960372924805, 5.311343193054199, 5.307466506958008, 5.303586959838867, 5.299708843231201, 5.295834064483643, 5.291963577270508, 5.288084030151367, 5.284177780151367, 5.280236721038818, 5.276272296905518, 5.272297382354736]\n"
          ]
        }
      ],
      "source": [
        "loss_records = []\n",
        "\n",
        "for i in range(100):\n",
        "  loss = trainer._train_by_single_batch(batch)\n",
        "  loss_records.append(loss)\n",
        "print(loss_records)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mQx4K_3DZS-I",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "7e0c64845c244720b529a238fb7f03fc",
            "2ce4571ca8c3486db82cf5111b49b979",
            "d01cea5d86be431fa519656159469c88",
            "50f476d7d8e84e8a81539a9a3549c1d4",
            "601a23ba484145af8eec1946779a53d8",
            "0eb7936f98394185a72a8eb807798fe4",
            "4f4102090d5d4f079c0a23a218648049",
            "87be8f11d7304eea98f48b2f49448a24",
            "e718ea3aae714c89aaee1f1f79374cab",
            "4d0a96248ad549d6989d9cc4a58bc10f",
            "ccdb06e71d874d1f8210776f1f74ea28",
            "5daec47eff3040788002690b9d9ff559",
            "fa45e83d15cb442fb7890f9ccedac956",
            "a1f02abecc824b7b9f94ba2546ea7041",
            "3e01a4e4ef104f42a255d072e865d0ec",
            "496ec14697464ddf922fb72560efeabc",
            "bb4843917abc42d68b3df2fbb3d90519",
            "29011e908581492ebcb84b39c77fcc06",
            "91e2fea28b8e4386903bd5dbd39c9a2f",
            "f63757eee857429c83bf8bba81a36ff6",
            "c202c1da2c3549b1b759e9bc8609ffcd",
            "11f296f0327c43378b8e2cca2ac83ce8"
          ]
        },
        "id": "mQx4K_3DZS-I",
        "outputId": "efdf1f48-9588-49c3-9b80-c6e4a1cdb54d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e0c64845c244720b529a238fb7f03fc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5daec47eff3040788002690b9d9ff559",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/160000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer.train_by_num_epoch(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a1eeaf7",
      "metadata": {
        "id": "4a1eeaf7"
      },
      "source": [
        "- You can download pre-trained weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "d79bbee3",
      "metadata": {
        "id": "d79bbee3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90e03b51-0d43-4e0d-88d9-4b97fa459cb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=127OoPv8-lF5TWeNCq9PNa8igyGQnR-Pe\n",
            "To: /content/kor_eng_translator_model_best.pt\n",
            "100% 324M/324M [00:06<00:00, 50.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 127OoPv8-lF5TWeNCq9PNa8igyGQnR-Pe"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state_dict = torch.load(\"kor_eng_translator_model_best.pt\")"
      ],
      "metadata": {
        "id": "YvQCKX1xR5hl"
      },
      "id": "YvQCKX1xR5hl",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "1916a074",
      "metadata": {
        "id": "1916a074",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "ad41631f-3f2b-463d-f625-6416b937ad80"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-cfc9547ac8bb>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeq2seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_tgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Seq2seq.__init__() got an unexpected keyword argument 'num_layers'"
          ]
        }
      ],
      "source": [
        "hidden_size = 256\n",
        "model = Seq2seq(tokenizer_src.vocab_size, tokenizer_tgt.vocab_size, hidden_size, num_layers=3)\n",
        "\n",
        "model.load_state_dict(state_dict)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0eb7936f98394185a72a8eb807798fe4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11f296f0327c43378b8e2cca2ac83ce8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29011e908581492ebcb84b39c77fcc06": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ce4571ca8c3486db82cf5111b49b979": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0eb7936f98394185a72a8eb807798fe4",
            "placeholder": "​",
            "style": "IPY_MODEL_4f4102090d5d4f079c0a23a218648049",
            "value": "  0%"
          }
        },
        "3e01a4e4ef104f42a255d072e865d0ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c202c1da2c3549b1b759e9bc8609ffcd",
            "placeholder": "​",
            "style": "IPY_MODEL_11f296f0327c43378b8e2cca2ac83ce8",
            "value": " 14326/160000 [02:02&lt;19:28, 124.68it/s]"
          }
        },
        "496ec14697464ddf922fb72560efeabc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d0a96248ad549d6989d9cc4a58bc10f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f4102090d5d4f079c0a23a218648049": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "50f476d7d8e84e8a81539a9a3549c1d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d0a96248ad549d6989d9cc4a58bc10f",
            "placeholder": "​",
            "style": "IPY_MODEL_ccdb06e71d874d1f8210776f1f74ea28",
            "value": " 0/1 [00:00&lt;?, ?it/s]"
          }
        },
        "5daec47eff3040788002690b9d9ff559": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fa45e83d15cb442fb7890f9ccedac956",
              "IPY_MODEL_a1f02abecc824b7b9f94ba2546ea7041",
              "IPY_MODEL_3e01a4e4ef104f42a255d072e865d0ec"
            ],
            "layout": "IPY_MODEL_496ec14697464ddf922fb72560efeabc"
          }
        },
        "601a23ba484145af8eec1946779a53d8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e0c64845c244720b529a238fb7f03fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2ce4571ca8c3486db82cf5111b49b979",
              "IPY_MODEL_d01cea5d86be431fa519656159469c88",
              "IPY_MODEL_50f476d7d8e84e8a81539a9a3549c1d4"
            ],
            "layout": "IPY_MODEL_601a23ba484145af8eec1946779a53d8"
          }
        },
        "87be8f11d7304eea98f48b2f49448a24": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91e2fea28b8e4386903bd5dbd39c9a2f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1f02abecc824b7b9f94ba2546ea7041": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91e2fea28b8e4386903bd5dbd39c9a2f",
            "max": 160000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f63757eee857429c83bf8bba81a36ff6",
            "value": 14326
          }
        },
        "bb4843917abc42d68b3df2fbb3d90519": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c202c1da2c3549b1b759e9bc8609ffcd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccdb06e71d874d1f8210776f1f74ea28": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d01cea5d86be431fa519656159469c88": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87be8f11d7304eea98f48b2f49448a24",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e718ea3aae714c89aaee1f1f79374cab",
            "value": 0
          }
        },
        "e718ea3aae714c89aaee1f1f79374cab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f63757eee857429c83bf8bba81a36ff6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fa45e83d15cb442fb7890f9ccedac956": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb4843917abc42d68b3df2fbb3d90519",
            "placeholder": "​",
            "style": "IPY_MODEL_29011e908581492ebcb84b39c77fcc06",
            "value": "  9%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}