{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jdasam/aat3020-2023/blob/main/notebooks/4_Machine_translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80c87a7a",
      "metadata": {
        "id": "80c87a7a"
      },
      "source": [
        "# Machine Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "518b10da",
      "metadata": {
        "id": "518b10da"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Download dataset (originally from NIA AI-Hub)\n",
        "'''\n",
        "\n",
        "!gdown 1CpsqOuuuB3I_PG5DbuqH1ssCFVerU46g"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "192d9799",
      "metadata": {
        "id": "192d9799"
      },
      "outputs": [],
      "source": [
        "!unzip -q nia-aihub-korean-english.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca202b9d",
      "metadata": {
        "scrolled": true,
        "id": "ca202b9d"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "dataset_dir = Path('nia_korean_english')\n",
        "data_list = sorted(list(dataset_dir.glob('*.xlsx')))\n",
        "for path in dataset_dir.glob('*.xlsx'):\n",
        "  df = pd.read_excel(path)\n",
        "  kor_text_path = path.parent / (path.stem+'_kor.txt') \n",
        "  eng_text_path = path.parent / (path.stem+'_eng.txt') \n",
        "  with open(kor_text_path, 'w', encoding='utf8') as f:\n",
        "      f.write('\\n'.join(df['원문']))\n",
        "  with open(eng_text_path, 'w', encoding='utf8') as f:\n",
        "      f.write('\\n'.join(df['번역문']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fbc2c73",
      "metadata": {
        "id": "1fbc2c73"
      },
      "outputs": [],
      "source": [
        "data_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c767468",
      "metadata": {
        "id": "6c767468"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "dfs = [pd.read_excel(path) for path in data_list[:2]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e9f316f",
      "metadata": {
        "id": "2e9f316f"
      },
      "outputs": [],
      "source": [
        "df = pd.concat(dfs, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff8b8786",
      "metadata": {
        "id": "ff8b8786"
      },
      "outputs": [],
      "source": [
        "df['번역문'][10000:10050]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3df6e584",
      "metadata": {
        "id": "3df6e584"
      },
      "source": [
        "## Huggingface Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c6f1769",
      "metadata": {
        "id": "6c6f1769"
      },
      "outputs": [],
      "source": [
        "from tokenizers import BertWordPieceTokenizer\n",
        "tokenizer = BertWordPieceTokenizer(strip_accents=False, lowercase=False)\n",
        "\n",
        "corpus_file   =  [str(path.parent / (path.stem + '_kor.txt')) for path in data_list[:2]]\n",
        "# output_dir   = Path('hugging_kor_%d'%(vocab_size))\n",
        "en_corpus_file   =  [str(path.parent / (path.stem + '_eng.txt')) for path in data_list[:2]]\n",
        "# output_dir   = Path('hugging_eng_%d'%(vocab_size))\n",
        "\n",
        "vocab_size    = 32000  # Number of maximum size of the vocabulary\n",
        "limit_alphabet= 6000   \n",
        "output_dir    = Path('hugging_kor_%d'%(vocab_size))\n",
        "en_output_dir = Path('hugging_eng_%d'%(vocab_size))\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "en_output_dir.mkdir(exist_ok=True)\n",
        "min_frequency = 5 \n",
        "\n",
        "tokenizer.train(files=corpus_file,\n",
        "               vocab_size=vocab_size,\n",
        "               min_frequency=min_frequency,\n",
        "               limit_alphabet=limit_alphabet, \n",
        "               show_progress=True)\n",
        "\n",
        "tokenizer.save_model(str(output_dir))\n",
        "\n",
        "en_tokenizer = BertWordPieceTokenizer(strip_accents=False, lowercase=False)\n",
        "en_tokenizer.train(files=en_corpus_file,\n",
        "                vocab_size=vocab_size,\n",
        "                min_frequency=min_frequency,\n",
        "                limit_alphabet=limit_alphabet,\n",
        "                show_progress=True)\n",
        "en_tokenizer.save_model(str(en_output_dir))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "410eb97a",
      "metadata": {
        "scrolled": true,
        "id": "410eb97a"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizerFast\n",
        "\n",
        "tokenizer_src = BertTokenizerFast.from_pretrained('hugging_kor_32000',\n",
        "                                                       strip_accents=False,\n",
        "                                                       lowercase=False) \n",
        "tokenizer_tgt = BertTokenizerFast.from_pretrained('hugging_eng_32000',\n",
        "                                                       strip_accents=False,\n",
        "                                                       lowercase=False) \n",
        "\n",
        "tokenized_data = tokenizer_src(df['원문'].iloc[10])\n",
        "print(tokenizer_src.decode(tokenized_data['input_ids']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "340f55c0",
      "metadata": {
        "id": "340f55c0"
      },
      "outputs": [],
      "source": [
        "tokenized_ids = tokenizer_src(\"나는 학교에 갑니다\")['input_ids']\n",
        "tokenized_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b3ea831",
      "metadata": {
        "id": "5b3ea831"
      },
      "outputs": [],
      "source": [
        "tokenizer_src.decode(tokenized_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3acc83d",
      "metadata": {
        "id": "c3acc83d"
      },
      "outputs": [],
      "source": [
        "df.iloc[4]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26964dcb",
      "metadata": {
        "id": "26964dcb"
      },
      "source": [
        "## Divide Train / Validate/ Test Set\n",
        "- using `np.random.choice`\n",
        "    - To always get the same random shuffling result, you have to use `np.random.seed()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b088f662",
      "metadata": {
        "id": "b088f662"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "ba4a138a",
      "metadata": {
        "id": "ba4a138a"
      },
      "source": [
        "## Define Dataset\n",
        "- Each datasample has to return source sentence and target sentence\n",
        "- You need a Tokenizer to get the tokenized result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76933a49",
      "metadata": {
        "id": "76933a49"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4a9f0ec",
      "metadata": {
        "id": "f4a9f0ec"
      },
      "source": [
        "## Define Collate function\n",
        "- After implementing Dataset, we have to declare DataLoader that groups several training samples as a single batch\n",
        "- However, we cannot batchify the melodies in straightforward way, because the length of each melody is different\n",
        "- In this problem, you will learn about how to handle sequences of different length as a batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "456ff9f2",
      "metadata": {
        "id": "456ff9f2"
      },
      "outputs": [],
      "source": [
        "\n",
        "'''\n",
        "This cell will make error, because the length of each sample is different to each other\n",
        "'''\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=8)\n",
        "batch = next(iter(train_loader))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afc533f7",
      "metadata": {
        "id": "afc533f7"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "To handle that problem, you have to make your collate function \n",
        "'''\n",
        "def your_collate_function(raw_batch):\n",
        "  '''\n",
        "  You can make your own function to handle the batch\n",
        "  '''\n",
        "  \n",
        "  return raw_batch[0] # This returns the first melody of each batch. So it will avoid the error, but it doesn't do proper batchifying\n",
        "\n",
        "batch_size = 8\n",
        "raw_batch = [train_set[i] for i in range(batch_size)] # This is the input for the collate function\n",
        "batch = your_collate_function(raw_batch)\n",
        "\n",
        "'''\n",
        "This is what the 'collate_fn' does in DataLoader\n",
        "'''\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, collate_fn=your_collate_function)\n",
        "batch_by_loader = next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22db0716",
      "metadata": {
        "id": "22db0716"
      },
      "source": [
        "#### Pad Sequence and Pack Sequence\n",
        "In PyTorch, there are two ways to batchify a group of sequence with different length.\n",
        "- `torch.nn.utils.rnn.pad_sequence`\n",
        "    - This function takes list of tensors with different length and return padded sequence\n",
        "    - Padding is adding some constant number as a PAD token to match the length of short sequence to the maximum length\n",
        "        - e.g. If there are sequence of length (3,7,4), we can add 4 zeros to sequence with length 3, 3 zeros to sequence with length 4 to make them length 7\n",
        "    - In default, we use 0 for padding (zero padding)\n",
        "    - The result \n",
        "- `torch.nn.utils.rnn.pack_sequence`\n",
        "    - pad_sequence "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c8cce88",
      "metadata": {
        "id": "1c8cce88"
      },
      "source": [
        "Cells below show the example of `pad_sequence`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe229f85",
      "metadata": {
        "id": "fe229f85"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence, pack_sequence, PackedSequence\n",
        "short = torch.arange(3, -1, -1).float() # [3, 2, 1, 0]\n",
        "long = torch.arange(27,19, -1).float()\n",
        "middle = torch.arange(15,9, -1).float()\n",
        "\n",
        "pad_sequence([short, long, middle], batch_first=False)  # T x N "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b2a0920",
      "metadata": {
        "id": "0b2a0920"
      },
      "outputs": [],
      "source": [
        "# Default value of batch_first in pad_sequence is False.\n",
        "# So you have to always be careful not to miss batch_first=True in pad_sequence, if you use batch_first=True for your RNN layer.\n",
        "pad_sequence([short, long, middle], batch_first=True)  # N x T "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fb5e0dd",
      "metadata": {
        "id": "4fb5e0dd"
      },
      "source": [
        "1) However, the problem is that you can't figure out whether the 0 at the end of each sequence is a padded one, or was included in the original sequence\n",
        "- e.g. `[2, 3, 4, 3, 0]` becomes `[ 2,  3,  4,  5,  0,  0,  0]`. Now we don't know how many zeros were added for padding\n",
        "\n",
        "2) Also, if you run RNN for this padded sequence, RNN will calculate for the padded part also.\n",
        "- RNN doesn't know whether it is padded data, or existing data\n",
        "- This makes computation slower\n",
        "\n",
        "3) If you want to use bi-directional, which also reads the sequence from backward, paddings can make the result different.\n",
        "\n",
        "To solve this issue, we use PackedSequence, by using `pack_sequence`/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69086d43",
      "metadata": {
        "id": "69086d43"
      },
      "outputs": [],
      "source": [
        "packed_sequence = pack_sequence([short, long, middle], enforce_sorted=False)\n",
        "packed_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cd50747",
      "metadata": {
        "id": "4cd50747"
      },
      "source": [
        "`PackedSequence` has `data` and `batch_sizes`\n",
        "- `data` contains the flattened value of given batch\n",
        "    - To optimize the computation, the sequences have to be sorted by descending of length\n",
        "- `batch_sizes` represents how many valid batch sample exists for each time step\n",
        "    - `[3, 3, 3, 2, 2, 1, 1]` means that there are 3 sequences for first three time steps, and then 2 sequences for next two steps, and then only 1 sequence for next two steps.\n",
        "- `sorted_indices` shows how the sorted sequences can be converted to original order.\n",
        "    - `[1,2,0]` means that \n",
        "        - the 0th sequence in the sorted sequences (the longest one) was indexed as 1 in the original input batch\n",
        "        - the 1st sequence in the sorted sequences (`middle`) was indexed as 2 in the original input batch\n",
        "        - the 2nd sequence in the sorted sequences (`short`) was index as 0 in the original input batch\n",
        "- `unsorted_indices` shows how the original sequences are sorted.\n",
        "    - `[2,0,1]` means that\n",
        "        - the 0th sequence in the original input was sorted as 2nd in the sorted sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0500f200",
      "metadata": {
        "id": "0500f200"
      },
      "outputs": [],
      "source": [
        "rnn_layer = nn.GRU(1, 1)\n",
        "packed_sequence = pack_sequence([short.unsqueeze(1), long.unsqueeze(1), middle.unsqueeze(1)], enforce_sorted=False)\n",
        "out, last_hidden = rnn_layer(packed_sequence)\n",
        "\n",
        "print(f\"Type of output of RNN for PackedSequence: {type(out)}\")\n",
        "print(f\"Type of last_hidden of RNN for PackedSequence: {type(last_hidden)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f2e3e44",
      "metadata": {
        "id": "8f2e3e44"
      },
      "source": [
        "- RNN or its family of PyTorch can automatically handle `PackedSequence`\n",
        "- However, other layers like `nn.Embedding` or `nn.Linear` cannot take `PackedSequence` as its input\n",
        "- There are two ways to feed `PackedSequence` to these layers\n",
        "    - First, convert PackedSequence to ordinary torch.Tensor by `torch.nn.utils.rnn.pad_packed_sequence`\n",
        "        - This will convert PackedSequence to a tensor of sequneces with same length but different padding\n",
        "    - The other way is to feed only PackedSequence.data, and then declaring new PackedSequence with the output as `data`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b08620c8",
      "metadata": {
        "id": "b08620c8"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This will make error, because other layers cannot handle PackedSequence\n",
        "'''\n",
        "test_linear_layer = nn.Linear(in_features=1, out_features=2)\n",
        "test_linear_layer(packed_sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac9efa09",
      "metadata": {
        "id": "ac9efa09"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "One way to to this is using torch.nn.utils.rnn.pad_packed_sequence to convert PackedSequence to ordinary tensor\n",
        "'''\n",
        "\n",
        "from torch.nn.utils.rnn import pad_packed_sequence\n",
        "padded_sequence, batch_lengths = pad_packed_sequence(packed_sequence)\n",
        "print(f'The padded sequence generated from packed sequence (squeezed for printing): \\n {padded_sequence.squeeze()}')\n",
        "print(f'\"pad_packed_sequence\" also returns \"batch_lengths\", to clarify the original length before the padding: \\n {batch_lengths}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e302fe4",
      "metadata": {
        "id": "8e302fe4"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Now you can feed padded sequence to linear layer.\n",
        "'''\n",
        "\n",
        "linear_output = test_linear_layer(padded_sequence)\n",
        "print(f\"Output of feeding padded_sequence to a linear layer: {linear_output}\")\n",
        "print(\"Caution that it returns non-zero values for timestep with zero padding, because linear layer has a bias\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1c0e150",
      "metadata": {
        "id": "b1c0e150"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "You can make the output as a PackedSequence, by using torch.nn.utils.rnn.pack_padded_sequence\n",
        "'''\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "re_packed_sequence = pack_padded_sequence(linear_output, batch_lengths, enforce_sorted=False)\n",
        "re_packed_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22117c41",
      "metadata": {
        "id": "22117c41"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Another way to do it is using PackedSequence.data\n",
        "'''\n",
        "\n",
        "linear_out_pack = test_linear_layer(packed_sequence.data)\n",
        "packed_sequence_after_linear = PackedSequence(linear_out_pack, packed_sequence.batch_sizes, packed_sequence.sorted_indices, packed_sequence.unsorted_indices)\n",
        "packed_sequence_after_linear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81dd119c",
      "metadata": {
        "id": "81dd119c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "357c61f4",
      "metadata": {
        "id": "357c61f4"
      },
      "source": [
        "## Define Model\n",
        "![image](https://raw.githubusercontent.com/tensorflow/nmt/master/nmt/g3doc/img/seq2seq.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0af0ca29",
      "metadata": {
        "id": "0af0ca29"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "13ffb21b",
      "metadata": {
        "id": "13ffb21b"
      },
      "source": [
        "## Define Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64a3758c",
      "metadata": {
        "id": "64a3758c"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "class Trainer:\n",
        "  def __init__(self, model, optimizer, loss_fn, train_loader, valid_loader, device):\n",
        "    self.model = model\n",
        "    self.optimizer = optimizer\n",
        "    self.loss_fn = loss_fn\n",
        "    self.train_loader = train_loader\n",
        "    self.valid_loader = valid_loader\n",
        "    \n",
        "    self.model.to(device)\n",
        "    \n",
        "    self.best_valid_accuracy = 0\n",
        "    self.device = device\n",
        "    \n",
        "    self.training_loss = []\n",
        "    self.validation_loss = []\n",
        "    self.validation_acc = []\n",
        "\n",
        "  def save_model(self, path='imdb_sentiment_model.pt'):\n",
        "    torch.save({'model':self.model.state_dict(), 'optim':self.optimizer.state_dict()}, path)\n",
        "    \n",
        "  def train_by_num_epoch(self, num_epochs):\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "      self.model.train()\n",
        "      for batch in self.train_loader:\n",
        "        loss_value = self._train_by_single_batch(batch)\n",
        "        self.training_loss.append(loss_value)\n",
        "      self.model.eval()\n",
        "      validation_loss, validation_acc = self.validate()\n",
        "      self.validation_loss.append(validation_loss)\n",
        "      self.validation_acc.append(validation_acc)\n",
        "      \n",
        "      if validation_acc > self.best_valid_accuracy:\n",
        "        print(f\"Saving the model with best validation accuracy: Epoch {epoch+1}, Acc: {validation_acc:.4f} \")\n",
        "        self.save_model('imdb_sentiment_model_best.pt')\n",
        "      else:\n",
        "        self.save_model('imdb_sentiment_model_last.pt')\n",
        "      self.best_valid_accuracy = max(validation_acc, self.best_valid_accuracy)\n",
        "\n",
        "      \n",
        "  def _train_by_single_batch(self, batch):\n",
        "    '''\n",
        "    This method updates self.model's parameter with a given batch\n",
        "    \n",
        "    batch (tuple): (batch_of_input_text, batch_of_label)\n",
        "    \n",
        "    You have to use variables below:\n",
        "    \n",
        "    self.model (SentimentModel/torch.nn.Module): A neural network model\n",
        "    self.optimizer (torch.optim.adam.Adam): Adam optimizer that optimizes model's parameter\n",
        "    self.loss_fn (function): function for calculating BCE loss for a given prediction and target\n",
        "    self.device (str): 'cuda' or 'cpu'\n",
        "\n",
        "    output: loss (float): Mean binary cross entropy value for every sample in the training batch\n",
        "    The model's parameters, optimizer's steps has to be updated inside this method\n",
        "\n",
        "    TODO: Complete this method \n",
        "    '''\n",
        "\n",
        "    \n",
        "    return\n",
        "\n",
        "    \n",
        "  def validate(self, external_loader=None):\n",
        "    '''\n",
        "    This method calculates accuracy and loss for given data loader.\n",
        "    It can be used for validation step, or to get test set result\n",
        "    \n",
        "    input:\n",
        "      data_loader: If there is no data_loader given, use self.valid_loader as default.\n",
        "      \n",
        "    \n",
        "    output: \n",
        "      validation_loss (float): Mean Binary Cross Entropy value for every sample in validation set\n",
        "      validation_accuracy (float): Mean Accuracy value for every sample in validation set\n",
        "      \n",
        "    TODO: Complete this method \n",
        "\n",
        "    '''\n",
        "    \n",
        "    ### Don't change this part\n",
        "    if external_loader and isinstance(external_loader, DataLoader):\n",
        "      loader = external_loader\n",
        "      print('An arbitrary loader is used instead of Validation loader')\n",
        "    else:\n",
        "      loader = self.valid_loader\n",
        "      \n",
        "    self.model.eval()\n",
        "    \n",
        "    '''\n",
        "    Write your code from here, using loader, self.model, self.loss_fn.\n",
        "    '''"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}