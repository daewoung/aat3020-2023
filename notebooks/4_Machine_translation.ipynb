{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "7193c298",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jdasam/aat3020-2023/blob/main/notebooks/4_Machine_translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "80c87a7a",
      "metadata": {
        "id": "80c87a7a"
      },
      "source": [
        "# Machine Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y6OfzjCACzNM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6OfzjCACzNM",
        "outputId": "0d6806bb-774c-43f9-857c-5c396a46df0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.1-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m104.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.1\n"
          ]
        }
      ],
      "source": [
        "# install HuggingFace\n",
        "!pip install transformers tokenizers\n",
        "\n",
        "# If you are not using Colab, install below to read xlsx file\n",
        "# !pip install openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "518b10da",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "518b10da",
        "outputId": "20f69b3d-7d3a-4a6f-9a2a-e85fa02b86bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (uriginal): https://drive.google.com/uc?id=1GMFWREWBVcD5vJdwNFadHmzVStclcyKf\n",
            "From (redirected): https://drive.google.com/uc?id=1GMFWREWBVcD5vJdwNFadHmzVStclcyKf&confirm=t&uuid=65827053-9650-47c0-bcea-16500524b0f4\n",
            "To: /home/teo/userdata/git_libraries/aat3020-2023/notebooks/nia-aihub-korean-english-txt.zip\n",
            "100%|████████████████████████████████████████| 449M/449M [00:55<00:00, 8.14MB/s]\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Download dataset (originally from NIA AI-Hub)\n",
        "'''\n",
        "\n",
        "# !gdown 1CpsqOuuuB3I_PG5DbuqH1ssCFVerU46g\n",
        "# !unzip -q nia-aihub-korean-english.zip\n",
        "\n",
        "!gdown 1GMFWREWBVcD5vJdwNFadHmzVStclcyKf\n",
        "!unzip -q nia-aihub-korean-english-txt.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "dWNSKjhdJ1jM",
      "metadata": {
        "id": "dWNSKjhdJ1jM"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ca202b9d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "ca202b9d",
        "outputId": "e415e08d-0eca-4d52-ecc3-414e60d7cff6",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\ndataset_dir = Path('nia_korean_english')\\ndata_list = sorted(list(dataset_dir.glob('*.xlsx')))\\nfor path in data_list:\\n  df = pd.read_excel(path)\\n  kor_text_path = path.parent / (path.stem+'_kor.txt') \\n  eng_text_path = path.parent / (path.stem+'_eng.txt') \\n  with open(kor_text_path, 'w', encoding='utf8') as f:\\n      f.write('\\n'.join(df['원문']))\\n  with open(eng_text_path, 'w', encoding='utf8') as f:\\n      f.write('\\n'.join(df['번역문']))\\n\""
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# You don't have to run this\n",
        "'''\n",
        "dataset_dir = Path('nia_korean_english')\n",
        "data_list = sorted(list(dataset_dir.glob('*.xlsx')))\n",
        "for path in data_list:\n",
        "  df = pd.read_excel(path)\n",
        "  kor_text_path = path.parent / (path.stem+'_kor.txt') \n",
        "  eng_text_path = path.parent / (path.stem+'_eng.txt') \n",
        "  with open(kor_text_path, 'w', encoding='utf8') as f:\n",
        "      f.write('\\n'.join(df['원문']))\n",
        "  with open(eng_text_path, 'w', encoding='utf8') as f:\n",
        "      f.write('\\n'.join(df['번역문']))\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1fbc2c73",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fbc2c73",
        "outputId": "f948c9f0-78dd-4ce2-b669-c2ed92bbc6e0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[PosixPath('nia_korean_english/1_구어체(1).xlsx'),\n",
              " PosixPath('nia_korean_english/1_구어체(2).xlsx'),\n",
              " PosixPath('nia_korean_english/2_대화체.xlsx'),\n",
              " PosixPath('nia_korean_english/3_문어체_뉴스(1)_200226.xlsx'),\n",
              " PosixPath('nia_korean_english/3_문어체_뉴스(2).xlsx'),\n",
              " PosixPath('nia_korean_english/3_문어체_뉴스(3).xlsx'),\n",
              " PosixPath('nia_korean_english/3_문어체_뉴스(4).xlsx'),\n",
              " PosixPath('nia_korean_english/4_문어체_한국문화.xlsx'),\n",
              " PosixPath('nia_korean_english/5_문어체_조례.xlsx'),\n",
              " PosixPath('nia_korean_english/6_문어체_지자체웹사이트.xlsx')]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_dir = Path('nia_korean_english')\n",
        "data_list = sorted(list(dataset_dir.glob('*.xlsx')))\n",
        "\n",
        "data_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "6c767468",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "6c767468",
        "outputId": "be64a757-971e-49f5-e0eb-458aeaedaf6f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/teo/.local/share/virtualenvs/aat3020-2023-E1AG9i7b/lib/python3.8/site-packages/openpyxl/styles/stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
            "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
            "/home/teo/.local/share/virtualenvs/aat3020-2023-E1AG9i7b/lib/python3.8/site-packages/openpyxl/styles/stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
            "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
            "/home/teo/.local/share/virtualenvs/aat3020-2023-E1AG9i7b/lib/python3.8/site-packages/openpyxl/styles/stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
            "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
            "/home/teo/.local/share/virtualenvs/aat3020-2023-E1AG9i7b/lib/python3.8/site-packages/openpyxl/styles/stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
            "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "dfs = [pd.read_excel(path) for path in data_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "2e9f316f",
      "metadata": {
        "id": "2e9f316f"
      },
      "outputs": [],
      "source": [
        "df = pd.concat(dfs, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gKyXWsJjjrLR",
      "metadata": {
        "id": "gKyXWsJjjrLR"
      },
      "outputs": [],
      "source": [
        "with open(\"nia_korean_english/1_구어체(1)_kor.txt\", 'w', encoding='utf8') as f:\n",
        "    f.write('\\n'.join(df['원문']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff8b8786",
      "metadata": {
        "id": "ff8b8786"
      },
      "outputs": [],
      "source": [
        "df['원문'][10000:10050], df['번역문'][10000:10050]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3df6e584",
      "metadata": {
        "id": "3df6e584"
      },
      "source": [
        "## Huggingface Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6c6f1769",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c6f1769",
        "outputId": "f1f0dfad-d93b-482d-8c92-d46bc4177580"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['hugging_eng_32000/vocab.txt']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tokenizers import BertWordPieceTokenizer\n",
        "tokenizer = BertWordPieceTokenizer(strip_accents=False, lowercase=False)\n",
        "\n",
        "num_files = len(data_list)\n",
        "\n",
        "corpus_file   =  [str(path.parent / (path.stem + '_kor.txt')) for path in data_list[:num_files]]\n",
        "# output_dir   = Path('hugging_kor_%d'%(vocab_size))\n",
        "en_corpus_file   =  [str(path.parent / (path.stem + '_eng.txt')) for path in data_list[:num_files]]\n",
        "# output_dir   = Path('hugging_eng_%d'%(vocab_size))\n",
        "\n",
        "vocab_size    = 32000  # Number of maximum size of the vocabulary\n",
        "limit_alphabet= 6000   \n",
        "output_dir    = Path('hugging_kor_%d'%(vocab_size))\n",
        "en_output_dir = Path('hugging_eng_%d'%(vocab_size))\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "en_output_dir.mkdir(exist_ok=True)\n",
        "min_frequency = 5 \n",
        "\n",
        "tokenizer.train(files=corpus_file,\n",
        "               vocab_size=vocab_size,\n",
        "               min_frequency=min_frequency,\n",
        "               limit_alphabet=limit_alphabet, \n",
        "               show_progress=True)\n",
        "\n",
        "tokenizer.save_model(str(output_dir))\n",
        "\n",
        "en_tokenizer = BertWordPieceTokenizer(strip_accents=False, lowercase=False)\n",
        "en_tokenizer.train(files=en_corpus_file,\n",
        "                vocab_size=vocab_size,\n",
        "                min_frequency=min_frequency,\n",
        "                limit_alphabet=limit_alphabet,\n",
        "                show_progress=True)\n",
        "en_tokenizer.save_model(str(en_output_dir))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "c2d0e85b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "  adding: nia_korean_english/ (stored 0%)\n",
            "  adding: nia_korean_english/5_문어체_조례.xlsx (deflated 3%)\n",
            "  adding: nia_korean_english/3_문어체_뉴스(2).xlsx (deflated 0%)\n",
            "  adding: nia_korean_english/2_대화체.xlsx (deflated 9%)\n",
            "  adding: nia_korean_english/1_구어체(1).xlsx (deflated 3%)\n",
            "  adding: nia_korean_english/1_구어체(1)_eng.txt (deflated 64%)\n",
            "  adding: nia_korean_english/3_문어체_뉴스(4).xlsx (deflated 0%)\n",
            "  adding: nia_korean_english/2_대화체_kor.txt (deflated 69%)\n",
            "  adding: nia_korean_english/4_문어체_한국문화_kor.txt (deflated 63%)\n",
            "  adding: nia_korean_english/5_문어체_조례_kor.txt (deflated 72%)\n",
            "  adding: nia_korean_english/6_문어체_지자체웹사이트_kor.txt (deflated 65%)\n",
            "  adding: nia_korean_english/1_구어체(2)_eng.txt (deflated 63%)\n",
            "  adding: nia_korean_english/4_문어체_한국문화_eng.txt (deflated 64%)\n",
            "  adding: nia_korean_english/3_문어체_뉴스(1)_200226.xlsx (deflated 0%)\n",
            "  adding: nia_korean_english/1_구어체(2)_kor.txt (deflated 66%)\n",
            "  adding: nia_korean_english/1_구어체(1)_kor.txt (deflated 68%)\n",
            "  adding: nia_korean_english/1_구어체(2).xlsx (deflated 20%)\n",
            "  adding: nia_korean_english/3_문어체_뉴스(3)_kor.txt (deflated 59%)\n",
            "  adding: nia_korean_english/5_문어체_조례_eng.txt (deflated 74%)\n",
            "  adding: nia_korean_english/3_문어체_뉴스(4)_eng.txt (deflated 62%)\n",
            "  adding: nia_korean_english/3_문어체_뉴스(4)_kor.txt (deflated 60%)\n",
            "  adding: nia_korean_english/3_문어체_뉴스(3)_eng.txt (deflated 62%)\n",
            "  adding: nia_korean_english/1_구어체(1).txt (deflated 68%)\n",
            "  adding: nia_korean_english/3_문어체_뉴스(2)_kor.txt (deflated 60%)\n",
            "  adding: nia_korean_english/3_문어체_뉴스(1)_200226_kor.txt (deflated 60%)\n",
            "  adding: nia_korean_english/6_문어체_지자체웹사이트.xlsx (deflated 4%)\n",
            "  adding: nia_korean_english/3_문어체_뉴스(3).xlsx (deflated 0%)\n",
            "  adding: nia_korean_english/3_문어체_뉴스(1)_200226_eng.txt (deflated 62%)\n",
            "  adding: nia_korean_english/2_대화체_eng.txt (deflated 67%)\n",
            "  adding: nia_korean_english/6_문어체_지자체웹사이트_eng.txt (deflated 67%)\n",
            "  adding: nia_korean_english/3_문어체_뉴스(2)_eng.txt (deflated 62%)\n",
            "  adding: nia_korean_english/4_문어체_한국문화.xlsx (deflated 4%)\n",
            "  adding: hugging_kor_32000/ (stored 0%)\n",
            "  adding: hugging_kor_32000/vocab.txt (deflated 54%)\n",
            "  adding: hugging_eng_32000/ (stored 0%)\n",
            "  adding: hugging_eng_32000/vocab.txt (deflated 51%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r nia-aihub-korean-english-vocab.zip nia_korean_english hugging_kor_32000 hugging_eng_32000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "410eb97a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "410eb97a",
        "outputId": "6598a1c1-1356-447d-975b-6d92303aaba9",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "나는 친구에게 그 철학자의 책을 선물해 주겠다고 말했습니다.\n",
            "[CLS] 나는 친구에게 그 철학자의 책을 선물해 주겠다고 말했습니다. [SEP]\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizerFast\n",
        "\n",
        "tokenizer_src = BertTokenizerFast.from_pretrained('hugging_kor_32000',\n",
        "                                                       strip_accents=False,\n",
        "                                                       lowercase=False) \n",
        "tokenizer_tgt = BertTokenizerFast.from_pretrained('hugging_eng_32000',\n",
        "                                                       strip_accents=False,\n",
        "                                                       lowercase=False) \n",
        "\n",
        "tokenized_data = tokenizer_src(df['원문'].iloc[10])\n",
        "print(df['원문'].iloc[10])\n",
        "print(tokenizer_src.decode(tokenized_data['input_ids']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q9GJ7cifk5tg",
      "metadata": {
        "id": "q9GJ7cifk5tg"
      },
      "outputs": [],
      "source": [
        "tokenized_data['input_ids']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "340f55c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "340f55c0",
        "outputId": "ccdfb5b5-0263-48d9-a8b6-a7d54ad9e731"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[2, 3715, 1079, 2604, 12638, 12008, 3]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_ids = tokenizer_src(\"나는 서강대학교에 다닙니다\")['input_ids']\n",
        "tokenized_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "5b3ea831",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "5b3ea831",
        "outputId": "ae85eb77-670f-4df2-8d3b-4ab3ad5695e8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[CLS] 나는 서강대학교에 다닙니다 [SEP]'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer_src.decode(tokenized_ids)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "26964dcb",
      "metadata": {
        "id": "26964dcb"
      },
      "source": [
        "## Divide Train / Validate/ Test Set\n",
        "- using `np.random.choice`\n",
        "    - To always get the same random shuffling result, you have to use `np.random.seed()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "B2zJgvXPlh95",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "B2zJgvXPlh95",
        "outputId": "33f24480-2a88-418e-d95d-b01f5b6050d6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d659cf52-06c5-4062-a100-2c32cbe09de2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>지자체</th>\n",
              "      <th>원문</th>\n",
              "      <th>번역문</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>261811</td>\n",
              "      <td>경기도</td>\n",
              "      <td>\"경기도가 말산업 육성을 위해 총예산 245,193천원으로 2013년 경기도 용인시...</td>\n",
              "      <td>\"The Gyeonggi provincial government announced ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>409852</td>\n",
              "      <td>경기도</td>\n",
              "      <td>\"경기도가 주최하고 경기FTA활용지원센터와 코트라가 주관한 이번 시장개척단은 지난 ...</td>\n",
              "      <td>\"Organized by Gyeonggi provincial government a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>352671</td>\n",
              "      <td>경기도</td>\n",
              "      <td>\"경기도가 주최하고 경기도비정규직지원센터가 주관한 이번 교육은 공공 부문이 직·간접...</td>\n",
              "      <td>\"Organized by Gyeonggi provincial government a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>308191</td>\n",
              "      <td>경기도</td>\n",
              "      <td>\"경기도가 주최하고 사단법인 한국장애인복지시설협회 경기도협회(대표자:정권)에서 주관...</td>\n",
              "      <td>\"Organized by Gyeonggi provincial government a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>185424</td>\n",
              "      <td>경기도</td>\n",
              "      <td>\"경기도가 주최하고 인구보건복지협회 경기지회, 아이낳기좋은세상 경기운동본부가 주관하...</td>\n",
              "      <td>\"Hosted by Gyeonggi provincial government, org...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100082</th>\n",
              "      <td>73961</td>\n",
              "      <td>서울시 중구</td>\n",
              "      <td>힐튼호텔 건너편에서 회현동주민센터로 내려가는 길에는 칙칙한 회색의 석축옹벽이 이어져...</td>\n",
              "      <td>Across from the Hilton Hotel, the path to Hoeh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100083</th>\n",
              "      <td>462950</td>\n",
              "      <td>경기도</td>\n",
              "      <td>힘든 분들이 많이 계시지만 조금이나마 어렵지 않도록 도움이 될 수 있도록 저희가 최...</td>\n",
              "      <td>There are a lot of people who are having a har...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100084</th>\n",
              "      <td>333006</td>\n",
              "      <td>경기도</td>\n",
              "      <td>힘든 역사 속에서 대한민국을 불과 50여년 만에 빛나는 나라로 만들고 업적을 만들 ...</td>\n",
              "      <td>It was possible to make the Republic of Korea ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100085</th>\n",
              "      <td>454399</td>\n",
              "      <td>경기도</td>\n",
              "      <td>힘든 일을 하는 데는 무엇보다 정부가 큰 관심을 갖고 있다는 자부심을 갖도록 해야 한다.</td>\n",
              "      <td>As for doing hard work, most of all, they shou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100086</th>\n",
              "      <td>507679</td>\n",
              "      <td>경기도</td>\n",
              "      <td>힘을 합쳐 세계 일류통일강국으로 대한민국을 우뚝 세우는 역할을 이 자리의 모든 지도...</td>\n",
              "      <td>I would like to thank all the leaders here for...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100087 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d659cf52-06c5-4062-a100-2c32cbe09de2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d659cf52-06c5-4062-a100-2c32cbe09de2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d659cf52-06c5-4062-a100-2c32cbe09de2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "            ID     지자체                                                 원문  \\\n",
              "0       261811     경기도  \"경기도가 말산업 육성을 위해 총예산 245,193천원으로 2013년 경기도 용인시...   \n",
              "1       409852     경기도  \"경기도가 주최하고 경기FTA활용지원센터와 코트라가 주관한 이번 시장개척단은 지난 ...   \n",
              "2       352671     경기도  \"경기도가 주최하고 경기도비정규직지원센터가 주관한 이번 교육은 공공 부문이 직·간접...   \n",
              "3       308191     경기도  \"경기도가 주최하고 사단법인 한국장애인복지시설협회 경기도협회(대표자:정권)에서 주관...   \n",
              "4       185424     경기도  \"경기도가 주최하고 인구보건복지협회 경기지회, 아이낳기좋은세상 경기운동본부가 주관하...   \n",
              "...        ...     ...                                                ...   \n",
              "100082   73961  서울시 중구  힐튼호텔 건너편에서 회현동주민센터로 내려가는 길에는 칙칙한 회색의 석축옹벽이 이어져...   \n",
              "100083  462950     경기도  힘든 분들이 많이 계시지만 조금이나마 어렵지 않도록 도움이 될 수 있도록 저희가 최...   \n",
              "100084  333006     경기도  힘든 역사 속에서 대한민국을 불과 50여년 만에 빛나는 나라로 만들고 업적을 만들 ...   \n",
              "100085  454399     경기도  힘든 일을 하는 데는 무엇보다 정부가 큰 관심을 갖고 있다는 자부심을 갖도록 해야 한다.   \n",
              "100086  507679     경기도  힘을 합쳐 세계 일류통일강국으로 대한민국을 우뚝 세우는 역할을 이 자리의 모든 지도...   \n",
              "\n",
              "                                                      번역문  \n",
              "0       \"The Gyeonggi provincial government announced ...  \n",
              "1       \"Organized by Gyeonggi provincial government a...  \n",
              "2       \"Organized by Gyeonggi provincial government a...  \n",
              "3       \"Organized by Gyeonggi provincial government a...  \n",
              "4       \"Hosted by Gyeonggi provincial government, org...  \n",
              "...                                                   ...  \n",
              "100082  Across from the Hilton Hotel, the path to Hoeh...  \n",
              "100083  There are a lot of people who are having a har...  \n",
              "100084  It was possible to make the Republic of Korea ...  \n",
              "100085  As for doing hard work, most of all, they shou...  \n",
              "100086  I would like to thank all the leaders here for...  \n",
              "\n",
              "[100087 rows x 4 columns]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "boOZlijtlpgv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boOZlijtlpgv",
        "outputId": "9ce04f9d-ee41-43a0-9463-c1d444496e01"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1602418"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "NdDy0EHNlv60",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdDy0EHNlv60",
        "outputId": "3180be90-6287-4e22-b28f-887e43166aab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ID                                                507679\n",
              "지자체                                                  경기도\n",
              "원문     힘을 합쳐 세계 일류통일강국으로 대한민국을 우뚝 세우는 역할을 이 자리의 모든 지도...\n",
              "번역문    I would like to thank all the leaders here for...\n",
              "Name: 100086, dtype: object"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.iloc[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "AGA1i06oMD1N",
      "metadata": {
        "id": "AGA1i06oMD1N"
      },
      "outputs": [],
      "source": [
        "df = pd.read_excel(\"/content/nia_korean_english/1_구어체(1).xlsx\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ba4a138a",
      "metadata": {
        "id": "ba4a138a"
      },
      "source": [
        "## Define Dataset\n",
        "- Each datasample has to return source sentence and target sentence\n",
        "- You need a Tokenizer to get the tokenized result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "b088f662",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b088f662",
        "outputId": "147be13b-5b25-4793-d55d-a7fc9377b11a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([    2,    11,    70,  4665,  5209, 13306,    71, 12901,  9565, 12435,\n",
            "           11,  3546, 14567,  4325,  8934,  8407,  7400,  4154,  3252,  6420,\n",
            "        12985,  4996,  3397,  6461,    18,     3])\n",
            "tensor([    2, 26268, 23067,    11,  1056,    69, 23067,  2803,  1067,  5155,\n",
            "         1117,  1042,  2405,  4024,  5520,  1039,  1023, 26268,    18])\n",
            "tensor([26268, 23067,    11,  1056,    69, 23067,  2803,  1067,  5155,  1117,\n",
            "         1042,  2405,  4024,  5520,  1039,  1023, 26268,    18,     3])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "class Dataset:\n",
        "  def __init__(self, df, src_tokenizer, tgt_tokenizer):\n",
        "    self.data = df\n",
        "    self.src_tokenizer = src_tokenizer\n",
        "    self.tgt_tokenizer = tgt_tokenizer\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    selected_row = self.data.iloc[idx]\n",
        "    source = selected_row['원문']\n",
        "    target = selected_row['번역문']\n",
        "\n",
        "    source_enc = self.src_tokenizer(source)['input_ids']\n",
        "    target_enc = self.tgt_tokenizer(target)['input_ids']\n",
        "\n",
        "    return torch.LongTensor(source_enc), torch.LongTensor(target_enc[:-1]), torch.LongTensor(target_enc[1:])\n",
        "  \n",
        "dataset = Dataset(df, tokenizer_src, tokenizer_tgt)\n",
        "\n",
        "source, target, shifted_target = dataset[0]\n",
        "print(source)\n",
        "print(target)\n",
        "print(shifted_target)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "kMaE7cK2EurA",
      "metadata": {
        "id": "kMaE7cK2EurA"
      },
      "source": [
        "## Split Dataset\n",
        "- using ``torch.utils.data.random_split(dataset)``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "Lb8fmwGbEhU1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lb8fmwGbEhU1",
        "outputId": "d189ec91-804c-4dad-e631-02cfeaca02b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1281934 160241 160243\n"
          ]
        }
      ],
      "source": [
        "num_data = len(dataset)\n",
        "# num_data\n",
        "num_train = int(num_data * 0.8)\n",
        "num_valid = int(num_data * 0.1)\n",
        "# num_test = int(num_data * 0.1)\n",
        "num_test = num_data - (num_train + num_valid)\n",
        "\n",
        "train_set, valid_set, test_set = torch.utils.data.random_split(dataset, [num_train, num_valid, num_test])\n",
        "print(len(train_set), len(valid_set), len(test_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "76933a49",
      "metadata": {
        "id": "76933a49"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "oTmu4Yy_N7vt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTmu4Yy_N7vt",
        "outputId": "26fc0c48-97db-4dad-883f-f1e123e98bf8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[tensor([[    2,  3318,  4506,  5072,  4326,  7277, 10197, 14775,  7277,  8680,\n",
              "           8472,  7973, 25836, 14094, 13079,  4331,  6426,   104, 13246,  7227,\n",
              "          13768,  6705,  9246, 12502,  6482, 12822,  3318,  4506,  5072,  8878,\n",
              "           6476, 14332, 12319,    18,     3]]),\n",
              " tensor([[   2, 1023, 2673, 6269, 1067, 1023, 4139, 1106,   69, 1495,   17, 1949,\n",
              "          4139, 1644,   69, 5867, 1039, 1334, 8757, 1045, 1787, 1970, 1319, 1039,\n",
              "          1334, 6679, 1045, 7405,   16, 1169, 1106, 8088, 1392, 2151, 1089, 1106,\n",
              "            69, 1495,   17, 1949, 4139, 1142, 2709,   17, 4430,   18]]),\n",
              " tensor([[1023, 2673, 6269, 1067, 1023, 4139, 1106,   69, 1495,   17, 1949, 4139,\n",
              "          1644,   69, 5867, 1039, 1334, 8757, 1045, 1787, 1970, 1319, 1039, 1334,\n",
              "          6679, 1045, 7405,   16, 1169, 1106, 8088, 1392, 2151, 1089, 1106,   69,\n",
              "          1495,   17, 1949, 4139, 1142, 2709,   17, 4430,   18,    3]])]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_loader = DataLoader(train_set, batch_size=1, shuffle=True)\n",
        "\n",
        "batch = next(iter(train_loader))\n",
        "batch"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f4a9f0ec",
      "metadata": {
        "id": "f4a9f0ec"
      },
      "source": [
        "## Define Collate function\n",
        "- After implementing Dataset, we have to declare DataLoader that groups several training samples as a single batch\n",
        "- However, we cannot batchify the melodies in straightforward way, because the length of each melody is different\n",
        "- In this problem, you will learn about how to handle sequences of different length as a batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "456ff9f2",
      "metadata": {
        "id": "456ff9f2"
      },
      "outputs": [],
      "source": [
        "\n",
        "'''\n",
        "This cell will make error, because the length of each sample is different to each other\n",
        "'''\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=8)\n",
        "batch = next(iter(train_loader))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afc533f7",
      "metadata": {
        "id": "afc533f7"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "To handle that problem, you have to make your collate function \n",
        "'''\n",
        "def your_collate_function(raw_batch):\n",
        "  '''\n",
        "  You can make your own function to handle the batch\n",
        "  '''\n",
        "  \n",
        "  return raw_batch[0] # This returns the first melody of each batch. So it will avoid the error, but it doesn't do proper batchifying\n",
        "\n",
        "batch_size = 8\n",
        "raw_batch = [train_set[i] for i in range(batch_size)] # This is the input for the collate function\n",
        "batch = your_collate_function(raw_batch)\n",
        "\n",
        "'''\n",
        "This is what the 'collate_fn' does in DataLoader\n",
        "'''\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, collate_fn=your_collate_function)\n",
        "batch_by_loader = next(iter(train_loader))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "22db0716",
      "metadata": {
        "id": "22db0716"
      },
      "source": [
        "#### Pad Sequence and Pack Sequence\n",
        "In PyTorch, there are two ways to batchify a group of sequence with different length.\n",
        "- `torch.nn.utils.rnn.pad_sequence`\n",
        "    - This function takes list of tensors with different length and return padded sequence\n",
        "    - Padding is adding some constant number as a PAD token to match the length of short sequence to the maximum length\n",
        "        - e.g. If there are sequence of length (3,7,4), we can add 4 zeros to sequence with length 3, 3 zeros to sequence with length 4 to make them length 7\n",
        "    - In default, we use 0 for padding (zero padding)\n",
        "    - The result \n",
        "- `torch.nn.utils.rnn.pack_sequence`\n",
        "    - pad_sequence "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "1c8cce88",
      "metadata": {
        "id": "1c8cce88"
      },
      "source": [
        "Cells below show the example of `pad_sequence`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "fe229f85",
      "metadata": {
        "id": "fe229f85"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 3., 27., 15.],\n",
              "        [ 2., 26., 14.],\n",
              "        [ 1., 25., 13.],\n",
              "        [ 0., 24., 12.],\n",
              "        [ 0., 23., 11.],\n",
              "        [ 0., 22., 10.],\n",
              "        [ 0., 21.,  0.],\n",
              "        [ 0., 20.,  0.]])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence, pack_sequence, PackedSequence\n",
        "short = torch.arange(3, -1, -1).float() # [3, 2, 1, 0]\n",
        "long = torch.arange(27,19, -1).float()\n",
        "middle = torch.arange(15,9, -1).float()\n",
        "\n",
        "pad_sequence([short, long, middle], batch_first=False)  # T x N "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b2a0920",
      "metadata": {
        "id": "0b2a0920"
      },
      "outputs": [],
      "source": [
        "# Default value of batch_first in pad_sequence is False.\n",
        "# So you have to always be careful not to miss batch_first=True in pad_sequence, if you use batch_first=True for your RNN layer.\n",
        "pad_sequence([short, long, middle], batch_first=True)  # N x T "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "4fb5e0dd",
      "metadata": {
        "id": "4fb5e0dd"
      },
      "source": [
        "1) However, the problem is that you can't figure out whether the 0 at the end of each sequence is a padded one, or was included in the original sequence\n",
        "- e.g. `[2, 3, 4, 3, 0]` becomes `[ 2,  3,  4,  5,  0,  0,  0]`. Now we don't know how many zeros were added for padding\n",
        "\n",
        "2) Also, if you run RNN for this padded sequence, RNN will calculate for the padded part also.\n",
        "- RNN doesn't know whether it is padded data, or existing data\n",
        "- This makes computation slower\n",
        "\n",
        "3) If you want to use bi-directional, which also reads the sequence from backward, paddings can make the result different.\n",
        "\n",
        "To solve this issue, we use PackedSequence, by using `pack_sequence`/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69086d43",
      "metadata": {
        "id": "69086d43"
      },
      "outputs": [],
      "source": [
        "packed_sequence = pack_sequence([short, long, middle], enforce_sorted=False)\n",
        "packed_sequence"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "4cd50747",
      "metadata": {
        "id": "4cd50747"
      },
      "source": [
        "`PackedSequence` has `data` and `batch_sizes`\n",
        "- `data` contains the flattened value of given batch\n",
        "    - To optimize the computation, the sequences have to be sorted by descending of length\n",
        "- `batch_sizes` represents how many valid batch sample exists for each time step\n",
        "    - `[3, 3, 3, 2, 2, 1, 1]` means that there are 3 sequences for first three time steps, and then 2 sequences for next two steps, and then only 1 sequence for next two steps.\n",
        "- `sorted_indices` shows how the sorted sequences can be converted to original order.\n",
        "    - `[1,2,0]` means that \n",
        "        - the 0th sequence in the sorted sequences (the longest one) was indexed as 1 in the original input batch\n",
        "        - the 1st sequence in the sorted sequences (`middle`) was indexed as 2 in the original input batch\n",
        "        - the 2nd sequence in the sorted sequences (`short`) was index as 0 in the original input batch\n",
        "- `unsorted_indices` shows how the original sequences are sorted.\n",
        "    - `[2,0,1]` means that\n",
        "        - the 0th sequence in the original input was sorted as 2nd in the sorted sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0500f200",
      "metadata": {
        "id": "0500f200"
      },
      "outputs": [],
      "source": [
        "rnn_layer = nn.GRU(1, 1)\n",
        "packed_sequence = pack_sequence([short.unsqueeze(1), long.unsqueeze(1), middle.unsqueeze(1)], enforce_sorted=False)\n",
        "out, last_hidden = rnn_layer(packed_sequence)\n",
        "\n",
        "print(f\"Type of output of RNN for PackedSequence: {type(out)}\")\n",
        "print(f\"Type of last_hidden of RNN for PackedSequence: {type(last_hidden)}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "8f2e3e44",
      "metadata": {
        "id": "8f2e3e44"
      },
      "source": [
        "- RNN or its family of PyTorch can automatically handle `PackedSequence`\n",
        "- However, other layers like `nn.Embedding` or `nn.Linear` cannot take `PackedSequence` as its input\n",
        "- There are two ways to feed `PackedSequence` to these layers\n",
        "    - First, convert PackedSequence to ordinary torch.Tensor by `torch.nn.utils.rnn.pad_packed_sequence`\n",
        "        - This will convert PackedSequence to a tensor of sequneces with same length but different padding\n",
        "    - The other way is to feed only PackedSequence.data, and then declaring new PackedSequence with the output as `data`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b08620c8",
      "metadata": {
        "id": "b08620c8"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This will make error, because other layers cannot handle PackedSequence\n",
        "'''\n",
        "test_linear_layer = nn.Linear(in_features=1, out_features=2)\n",
        "test_linear_layer(packed_sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac9efa09",
      "metadata": {
        "id": "ac9efa09"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "One way to to this is using torch.nn.utils.rnn.pad_packed_sequence to convert PackedSequence to ordinary tensor\n",
        "'''\n",
        "\n",
        "from torch.nn.utils.rnn import pad_packed_sequence\n",
        "padded_sequence, batch_lengths = pad_packed_sequence(packed_sequence)\n",
        "print(f'The padded sequence generated from packed sequence (squeezed for printing): \\n {padded_sequence.squeeze()}')\n",
        "print(f'\"pad_packed_sequence\" also returns \"batch_lengths\", to clarify the original length before the padding: \\n {batch_lengths}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e302fe4",
      "metadata": {
        "id": "8e302fe4"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Now you can feed padded sequence to linear layer.\n",
        "'''\n",
        "\n",
        "linear_output = test_linear_layer(padded_sequence)\n",
        "print(f\"Output of feeding padded_sequence to a linear layer: {linear_output}\")\n",
        "print(\"Caution that it returns non-zero values for timestep with zero padding, because linear layer has a bias\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1c0e150",
      "metadata": {
        "id": "b1c0e150"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "You can make the output as a PackedSequence, by using torch.nn.utils.rnn.pack_padded_sequence\n",
        "'''\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "re_packed_sequence = pack_padded_sequence(linear_output, batch_lengths, enforce_sorted=False)\n",
        "re_packed_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22117c41",
      "metadata": {
        "id": "22117c41"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Another way to do it is using PackedSequence.data\n",
        "'''\n",
        "\n",
        "linear_out_pack = test_linear_layer(packed_sequence.data)\n",
        "packed_sequence_after_linear = PackedSequence(linear_out_pack, packed_sequence.batch_sizes, packed_sequence.sorted_indices, packed_sequence.unsorted_indices)\n",
        "packed_sequence_after_linear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "81dd119c",
      "metadata": {
        "id": "81dd119c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(PackedSequence(data=tensor([    2,     2,     2,     2,     2,     2,     2,     2,  7211,  3536,\n",
              "          8201,  7203,  6587,  6651,  6528,  7458,   145,  7007, 13314,  6977,\n",
              "          4177, 13846,  3228, 11457,  3453, 10221,  4313, 30734, 14375,  7516,\n",
              "         29408,  4330,  5618, 12647, 25349,  9928, 10639,  7327, 16802,  6435,\n",
              "          4396, 23625,  6468,  4531,  6422,  6803, 21664,  6639,   146,  6723,\n",
              "            82, 11507, 22937,  2185,  2396,  7135,    16,  7375,  4856,  4330,\n",
              "         28107,  3578, 10831, 17205,   145, 28360,  4496,  6435,  8861, 17765,\n",
              "          3252,  4436, 28943,  7486,  4464, 16118, 10727, 10261,  4432,  3252,\n",
              "         28943, 31367,  4330,  4325, 13043,  6678,  4387,  6507, 28943, 10874,\n",
              "          7249, 11031,    16,  3304, 24513,    18,   146, 17005,  4562, 10566,\n",
              "         22046, 13655,    18,     3,    16, 12945, 28906,    16, 17689,    18,\n",
              "             3,   145, 18515, 12678,  9468, 14760,     3,  8703,    16,  4387,\n",
              "          4322,  6435, 24557, 18515,  8213, 12376,  2870,  4320,  7019, 10869,\n",
              "          7827,  4947,   146,  6670,  4345,  7343,  9126,    16,  2981, 13493,\n",
              "          6803,  2361,   145,  7075,  4355,  6653,  7304, 14189,  4546, 26921,\n",
              "          6780,    18,   146,    16,  6416,    18,     3,  2651,  3300,    18,\n",
              "             3, 21399,  7080,     3,  4409,  9997, 18324, 24081,  4325, 18515,\n",
              "          6508, 17418,  4953,  3821,  8789,  8391,  6509, 20685,  9851, 10707,\n",
              "         17060,  6838, 13709,  6416,  4421,    18,  7122,     3,  6466, 15171,\n",
              "           145,    87,  4856, 17414,  4630,    81,  4494,   146,  2849, 10374,\n",
              "         17786, 15572,  4387,  2361,  6424,    18,     3]), batch_sizes=tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 7, 6, 5, 5, 5, 5, 5, 5, 5, 5, 4,\n",
              "         3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1]), sorted_indices=tensor([3, 7, 5, 4, 1, 2, 0, 6]), unsorted_indices=tensor([6, 4, 5, 0, 3, 2, 7, 1])),\n",
              " PackedSequence(data=tensor([    2,     2,     2,     2,     2,     2,     2,     2,  2704,  1352,\n",
              "          1138,  1258,  1407,  1039,    77,  1023,  1042,  2956, 11241,  1159,\n",
              "            77,  1200,  1592,  1759,    69,    16,  1023,  1257,    11,  2289,\n",
              "          1102,  1034,  5756,  1172,  2525,  5731,    81,    16, 30598,  1200,\n",
              "         16094,  1165,  1883,  1039,  2124,  4919,  1113,  6043,  4464,  1257,\n",
              "          1039,  1023,    16, 12107, 19120,  1195,  1113,  6255,  1637,  2274,\n",
              "            77,  1042,  2277,  1314,  9790,  1113,  1042,  1068,   267,  4794,\n",
              "          8408,  1065,    16,  7728, 11379, 21588,    81,  1067,  1392,  5271,\n",
              "            69,  9403,  2816,    69,  1161,    82,    69,    18,  2138,  1310,\n",
              "          2543,  1767,  6984,   795,  2298,  1034,  1090,  1045,  1042,  1099,\n",
              "           809,  3480,  4359,    11,  1913,  2599,  1023,  1414,    18,    18,\n",
              "          6982,  1039,  6619,  7419,  1112,    26,    11,  9041,  1034,  1045,\n",
              "          5384,  1854,    16, 11918,  1838, 21765,  5699,  1414,    11,  1045,\n",
              "            16,  1089,  1039,  1106,  4315,  1042,  1209,  1407,    69,  2705,\n",
              "            11,  6796,  1331,  1089,  2032,  1042,    16, 16010,  4138,   267,\n",
              "            11,  2323,    11,  1132,  1042,    87,    87, 25903,  2625,  9041,\n",
              "          4904,  8940,  2760,    16,  8171, 11918,    69,    18,    18,  1467,\n",
              "            11,   267,  7661,  1023,    16,    87,  1061,  2845,    11,  2957,\n",
              "          1200,    17,    77,  1045,  2048,  2191,  1515,  2372,    18, 25903,\n",
              "            11,    18,    16,    16,  1023, 11068,  6290,  1042,  1045,  7458,\n",
              "          5032,  4279,  1034,  1441, 25903,  1334,    11,  1401,  3730,  2893,\n",
              "            16,    11,  1045,  5259,  5157,  1181,  1948,    11,  1156,    18,\n",
              "         13885,  2414,  1042, 24011,  1051,  2845,    18]), batch_sizes=tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 7, 7, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
              "         4, 4, 4, 4, 4, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1,\n",
              "         1, 1, 1]), sorted_indices=tensor([7, 3, 1, 4, 2, 5, 0, 6]), unsorted_indices=tensor([6, 2, 4, 1, 3, 5, 7, 0])),\n",
              " PackedSequence(data=tensor([ 2704,  1352,  1138,  1258,  1407,  1039,    77,  1023,  1042,  2956,\n",
              "         11241,  1159,    77,  1200,  1592,  1759,    69,    16,  1023,  1257,\n",
              "            11,  2289,  1102,  1034,  5756,  1172,  2525,  5731,    81,    16,\n",
              "         30598,  1200, 16094,  1165,  1883,  1039,  2124,  4919,  1113,  6043,\n",
              "          4464,  1257,  1039,  1023,    16, 12107, 19120,  1195,  1113,  6255,\n",
              "          1637,  2274,    77,  1042,  2277,  1314,  9790,  1113,  1042,  1068,\n",
              "           267,  4794,  8408,  1065,    16,  7728, 11379, 21588,    81,  1067,\n",
              "          1392,  5271,    69,  9403,  2816,    69,  1161,    82,    69,    18,\n",
              "          2138,  1310,  2543,  1767,  6984,   795,  2298,     3,  1034,  1090,\n",
              "          1045,  1042,  1099,   809,  3480,  4359,    11,  1913,  2599,  1023,\n",
              "          1414,    18,    18,  6982,  1039,  6619,  7419,  1112,     3,    26,\n",
              "            11,  9041,  1034,  1045,  5384,  1854,    16, 11918,  1838, 21765,\n",
              "          5699,  1414,    11,  1045,    16,  1089,  1039,  1106,  4315,  1042,\n",
              "          1209,  1407,    69,  2705,    11,  6796,  1331,  1089,  2032,  1042,\n",
              "            16, 16010,  4138,   267,    11,  2323,    11,  1132,  1042,    87,\n",
              "            87, 25903,  2625,  9041,  4904,  8940,  2760,    16,  8171, 11918,\n",
              "            69,    18,    18,  1467,    11,   267,  7661,     3,     3,  1023,\n",
              "            16,    87,  1061,  2845,    11,  2957,  1200,    17,    77,  1045,\n",
              "          2048,  2191,  1515,  2372,    18, 25903,    11,    18,     3,    16,\n",
              "            16,     3,  1023, 11068,  6290,  1042,  1045,  7458,  5032,  4279,\n",
              "          1034,  1441, 25903,  1334,    11,  1401,  3730,  2893,    16,    11,\n",
              "          1045,  5259,  5157,  1181,  1948,    11,  1156,    18, 13885,     3,\n",
              "          2414,  1042, 24011,  1051,  2845,    18,     3]), batch_sizes=tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 7, 7, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
              "         4, 4, 4, 4, 4, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1,\n",
              "         1, 1, 1]), sorted_indices=tensor([7, 3, 1, 4, 2, 5, 0, 6]), unsorted_indices=tensor([6, 2, 4, 1, 3, 5, 7, 0])))"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def pack_collate(raw_batch):\n",
        "  source, target, shifted_target = zip(*raw_batch)\n",
        "  return pack_sequence(source, enforce_sorted=False), pack_sequence(target, enforce_sorted=False), pack_sequence(shifted_target, enforce_sorted=False)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=8, collate_fn=pack_collate)\n",
        "batch = next(iter(train_loader))\n",
        "batch"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "357c61f4",
      "metadata": {
        "id": "357c61f4"
      },
      "source": [
        "## Define Model\n",
        "![image](https://raw.githubusercontent.com/tensorflow/nmt/master/nmt/g3doc/img/seq2seq.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "0af0ca29",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0af0ca29",
        "outputId": "c99b310d-221e-4354-ca9c-c0d2c770b233"
      },
      "outputs": [],
      "source": [
        "class Seq2seq(nn.Module):\n",
        "  def __init__(self, enc_vocab, dec_vocab, hidden_size):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(enc_vocab, hidden_size)\n",
        "    self.decoder = Decoder(dec_vocab, hidden_size)\n",
        "\n",
        "  def forward(self, src, tgt):\n",
        "    enc_out = self.encoder(src)\n",
        "    dec_out = self.decoder(tgt, enc_out)\n",
        "    return dec_out \n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, num_vocab, hidden_size, num_layers=2):\n",
        "    super().__init__()\n",
        "    self.emb = nn.Embedding(num_vocab, hidden_size)\n",
        "    self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True, num_layers=num_layers)\n",
        "    # batch_first True: it takes (Num_samples_in_batch, num_timesteps, num_dim)\n",
        "    # batch_first False: it takes (num_timesteps, Num_samples_in_batch, num_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    if isinstance(x, PackedSequence):\n",
        "      emb = PackedSequence(self.emb(x.data), batch_sizes=x.batch_sizes, sorted_indices=x.sorted_indices, unsorted_indices=x.unsorted_indices)\n",
        "    else:\n",
        "      emb = self.emb(x)\n",
        "    out, last_hidden = self.rnn(emb)\n",
        "    return last_hidden\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, num_vocab, hidden_size, num_layers=2):\n",
        "    super().__init__()\n",
        "    self.emb = nn.Embedding(num_vocab, hidden_size)\n",
        "    self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True, num_layers=num_layers)\n",
        "    self.proj = nn.Linear(hidden_size, num_vocab)\n",
        "\n",
        "\n",
        "  def forward(self, x, enc_output):\n",
        "    if isinstance(x, PackedSequence):\n",
        "      emb = PackedSequence(self.emb(x.data), batch_sizes=x.batch_sizes, sorted_indices=x.sorted_indices, unsorted_indices=x.unsorted_indices)\n",
        "      out, last_hidden = self.rnn(emb, enc_output)\n",
        "      logit = PackedSequence(self.proj(out.data), batch_sizes=out.batch_sizes, sorted_indices=out.sorted_indices, unsorted_indices=out.unsorted_indices)\n",
        "    else:\n",
        "      emb = self.emb(x)\n",
        "      out, last_hidden = self.rnn(emb, enc_output)\n",
        "      logit = self.proj(out)\n",
        "    return logit\n",
        "\n",
        "hidden_size = 64\n",
        "model = Seq2seq(tokenizer_src.vocab_size, tokenizer_tgt.vocab_size, hidden_size)\n",
        "encoder = Encoder(tokenizer_src.vocab_size, hidden_size)\n",
        "decoder = Decoder(tokenizer_tgt.vocab_size, hidden_size)\n",
        "\n",
        "src, tgt, shifted_tgt = batch\n",
        "\n",
        "enc_out = encoder(src)\n",
        "dec_out = decoder(tgt, enc_out)\n",
        "\n",
        "logit = model(src, tgt)\n",
        "# logit.shape, logit.softmax(dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "WMGUwt10Whf5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMGUwt10Whf5",
        "outputId": "53bf8946-490b-41cd-ca60-8eaa32b28fe9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([1, 10, 30679]),\n",
              " torch.Size([1, 10]),\n",
              " tensor([[  76,   10,  746,  810,  906,  335,  412, 2590,   17,    3]]))"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "logit.shape, shifted_tgt.shape, shifted_tgt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "WgvICWU0Wg0j",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgvICWU0Wg0j",
        "outputId": "07da1b70-660d-44b1-acdb-2cf0edcd84cc"
      },
      "outputs": [],
      "source": [
        "def nll_loss(pred, target, eps=1e-8):\n",
        "  if pred.ndim == 3:\n",
        "    pred = pred.flatten(0, 1)\n",
        "  if target.ndim == 2:\n",
        "    target = target.flatten(0, 1)\n",
        "  assert pred.ndim == 2\n",
        "  assert target.ndim == 1\n",
        "\n",
        "  \n",
        "  return -torch.log(pred[torch.arange(len(target)), target] + eps).mean()\n",
        "\n",
        "# loss = nll_loss(logit.softmax(dim=-1), shifted_tgt)\n",
        "# loss, loss.shape, shifted_tgt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "13ffb21b",
      "metadata": {
        "id": "13ffb21b"
      },
      "source": [
        "## Define Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "64a3758c",
      "metadata": {
        "id": "64a3758c"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "class Trainer:\n",
        "  def __init__(self, model, optimizer, loss_fn, train_loader, valid_loader, device):\n",
        "    self.model = model\n",
        "    self.optimizer = optimizer\n",
        "    self.loss_fn = loss_fn\n",
        "    self.train_loader = train_loader\n",
        "    self.valid_loader = valid_loader\n",
        "    \n",
        "    self.model.to(device)\n",
        "    \n",
        "    self.best_valid_accuracy = 0\n",
        "    self.device = device\n",
        "    \n",
        "    self.training_loss = []\n",
        "    self.validation_loss = []\n",
        "    self.validation_acc = []\n",
        "\n",
        "  def save_model(self, path='kor_eng_translator_model.pt'):\n",
        "    torch.save({'model':self.model.state_dict(), 'optim':self.optimizer.state_dict()}, path)\n",
        "    \n",
        "  def train_by_num_epoch(self, num_epochs):\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "      self.model.train()\n",
        "      for batch in tqdm(self.train_loader, leave=False):\n",
        "        loss_value = self._train_by_single_batch(batch)\n",
        "        self.training_loss.append(loss_value)\n",
        "      self.model.eval()\n",
        "      validation_loss, validation_acc = self.validate()\n",
        "      self.validation_loss.append(validation_loss)\n",
        "      self.validation_acc.append(validation_acc)\n",
        "      \n",
        "      if validation_acc > self.best_valid_accuracy:\n",
        "        print(f\"Saving the model with best validation accuracy: Epoch {epoch+1}, Acc: {validation_acc:.4f} \")\n",
        "        self.save_model('kor_eng_translator_model_best.pt')\n",
        "      else:\n",
        "        self.save_model('kor_eng_translator_model_last.pt')\n",
        "      self.best_valid_accuracy = max(validation_acc, self.best_valid_accuracy)\n",
        "\n",
        "      \n",
        "  def _train_by_single_batch(self, batch):\n",
        "    '''\n",
        "    This method updates self.model's parameter with a given batch\n",
        "    \n",
        "    batch (tuple): (batch_of_input_text, batch_of_label)\n",
        "    \n",
        "    You have to use variables below:\n",
        "    \n",
        "    self.model (SentimentModel/torch.nn.Module): A neural network model\n",
        "    self.optimizer (torch.optim.adam.Adam): Adam optimizer that optimizes model's parameter\n",
        "    self.loss_fn (function): function for calculating BCE loss for a given prediction and target\n",
        "    self.device (str): 'cuda' or 'cpu'\n",
        "\n",
        "    output: loss (float): Mean binary cross entropy value for every sample in the training batch\n",
        "    The model's parameters, optimizer's steps has to be updated inside this method\n",
        "\n",
        "    TODO: Complete this method \n",
        "    '''\n",
        "    src, tgt, shifted_tgt = batch\n",
        "    src = src.to(self.device)\n",
        "    tgt = tgt.to(self.device)\n",
        "    shifted_tgt = shifted_tgt.to(self.device)\n",
        "\n",
        "    logit = self.model(src, tgt)\n",
        "\n",
        "    if isinstance(logit, PackedSequence):\n",
        "      prob = logit.data.softmax(dim=-1)\n",
        "      loss = self.loss_fn(prob, shifted_tgt.data)\n",
        "    else:\n",
        "      loss = self.loss_fn(prob, shifted_tgt)\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "    self.optimizer.zero_grad\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "    \n",
        "  def validate(self, external_loader=None):\n",
        "    '''\n",
        "    This method calculates accuracy and loss for given data loader.\n",
        "    It can be used for validation step, or to get test set result\n",
        "    \n",
        "    input:\n",
        "      data_loader: If there is no data_loader given, use self.valid_loader as default.\n",
        "      \n",
        "    \n",
        "    output: \n",
        "      validation_loss (float): Mean Binary Cross Entropy value for every sample in validation set\n",
        "      validation_accuracy (float): Mean Accuracy value for every sample in validation set\n",
        "      \n",
        "    TODO: Complete this method \n",
        "\n",
        "    '''\n",
        "    \n",
        "    ### Don't change this part\n",
        "    if external_loader and isinstance(external_loader, DataLoader):\n",
        "      loader = external_loader\n",
        "      print('An arbitrary loader is used instead of Validation loader')\n",
        "    else:\n",
        "      loader = self.valid_loader\n",
        "      \n",
        "    self.model.eval()\n",
        "    \n",
        "    '''\n",
        "    Write your code from here, using loader, self.model, self.loss_fn.\n",
        "    '''\n",
        "\n",
        "hidden_size = 128\n",
        "model = Seq2seq(tokenizer_src.vocab_size, tokenizer_tgt.vocab_size, hidden_size)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_fn = nll_loss\n",
        "train_loader = DataLoader(train_set, batch_size=64, shuffle=True, collate_fn=pack_collate)\n",
        "valid_loader = DataLoader(valid_set, batch_size=128, shuffle=False, collate_fn=pack_collate)\n",
        "device = 'cuda'\n",
        "\n",
        "trainer = Trainer(model, optimizer, loss_fn, train_loader, valid_loader, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "-zNWE0znYwwf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zNWE0znYwwf",
        "outputId": "06c9a0c2-c5b5-43a7-ed71-fcd4b5caa521"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[10.386933326721191, 10.32657241821289, 10.268647193908691, 10.210345268249512, 10.149768829345703, 10.084954261779785, 10.013751029968262, 9.933724403381348, 9.842076301574707, 9.735479354858398, 9.609955787658691, 9.460831642150879, 9.283170700073242, 9.072586059570312, 8.827017784118652, 8.548736572265625, 8.245342254638672, 7.928269863128662, 7.609827041625977, 7.30034065246582, 7.006803512573242, 6.732766628265381, 6.479230880737305, 6.245960235595703, 6.0324931144714355, 5.8385162353515625, 5.663750171661377, 5.507776737213135, 5.369976997375488, 5.249426364898682, 5.144853115081787, 5.054716110229492, 4.977362632751465, 4.911262035369873, 4.85506534576416, 4.807579517364502, 4.767537593841553, 4.733421802520752, 4.70345401763916, 4.676103115081787, 4.650460720062256, 4.626137733459473, 4.6028008460998535, 4.580265998840332, 4.558821201324463, 4.539508819580078, 4.523560523986816, 4.511585235595703, 4.503292083740234, 4.497927665710449, 4.4949469566345215, 4.494128704071045, 4.495345592498779, 4.498366832733154, 4.502806663513184, 4.508152484893799, 4.513828754425049, 4.519322395324707, 4.524385452270508, 4.529078960418701, 4.533546447753906, 4.537725925445557, 4.5412163734436035, 4.543583393096924, 4.544646263122559, 4.544658660888672, 4.544276237487793, 4.544242858886719, 4.545022964477539, 4.5467000007629395, 4.549046993255615, 4.5516510009765625, 4.554001331329346, 4.55544900894165, 4.555344581604004, 4.553051471710205, 4.548077583312988, 4.540017127990723, 4.528607368469238, 4.513759136199951, 4.495539665222168, 4.474140644073486, 4.449749946594238, 4.422365188598633, 4.391932010650635, 4.35894775390625, 4.324995994567871, 4.2922515869140625, 4.262956142425537, 4.2383713722229, 4.2186713218688965, 4.203605651855469, 4.192801475524902, 4.185894966125488, 4.182514667510986, 4.182295799255371, 4.184702396392822, 4.189136981964111, 4.195082664489746, 4.2021050453186035]\n"
          ]
        }
      ],
      "source": [
        "loss_records = []\n",
        "\n",
        "for i in range(100):\n",
        "  loss = trainer._train_by_single_batch(batch)\n",
        "  loss_records.append(loss)\n",
        "print(loss_records)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mQx4K_3DZS-I",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "7e0c64845c244720b529a238fb7f03fc",
            "2ce4571ca8c3486db82cf5111b49b979",
            "d01cea5d86be431fa519656159469c88",
            "50f476d7d8e84e8a81539a9a3549c1d4",
            "601a23ba484145af8eec1946779a53d8",
            "0eb7936f98394185a72a8eb807798fe4",
            "4f4102090d5d4f079c0a23a218648049",
            "87be8f11d7304eea98f48b2f49448a24",
            "e718ea3aae714c89aaee1f1f79374cab",
            "4d0a96248ad549d6989d9cc4a58bc10f",
            "ccdb06e71d874d1f8210776f1f74ea28",
            "5daec47eff3040788002690b9d9ff559",
            "fa45e83d15cb442fb7890f9ccedac956",
            "a1f02abecc824b7b9f94ba2546ea7041",
            "3e01a4e4ef104f42a255d072e865d0ec",
            "496ec14697464ddf922fb72560efeabc",
            "bb4843917abc42d68b3df2fbb3d90519",
            "29011e908581492ebcb84b39c77fcc06",
            "91e2fea28b8e4386903bd5dbd39c9a2f",
            "f63757eee857429c83bf8bba81a36ff6",
            "c202c1da2c3549b1b759e9bc8609ffcd",
            "11f296f0327c43378b8e2cca2ac83ce8"
          ]
        },
        "id": "mQx4K_3DZS-I",
        "outputId": "efdf1f48-9588-49c3-9b80-c6e4a1cdb54d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e0c64845c244720b529a238fb7f03fc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5daec47eff3040788002690b9d9ff559",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/160000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer.train_by_num_epoch(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d79bbee3",
      "metadata": {},
      "outputs": [],
      "source": [
        "!gdown 10PR89HB2h2dKiXw-4GWQyhGvAOds3WKZ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1916a074",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = Seq2seq(tokenizer_src.vocab_size, tokenizer_tgt.vocab_size, hidden_size, num_layers=3)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0eb7936f98394185a72a8eb807798fe4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11f296f0327c43378b8e2cca2ac83ce8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29011e908581492ebcb84b39c77fcc06": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ce4571ca8c3486db82cf5111b49b979": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0eb7936f98394185a72a8eb807798fe4",
            "placeholder": "​",
            "style": "IPY_MODEL_4f4102090d5d4f079c0a23a218648049",
            "value": "  0%"
          }
        },
        "3e01a4e4ef104f42a255d072e865d0ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c202c1da2c3549b1b759e9bc8609ffcd",
            "placeholder": "​",
            "style": "IPY_MODEL_11f296f0327c43378b8e2cca2ac83ce8",
            "value": " 14326/160000 [02:02&lt;19:28, 124.68it/s]"
          }
        },
        "496ec14697464ddf922fb72560efeabc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d0a96248ad549d6989d9cc4a58bc10f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f4102090d5d4f079c0a23a218648049": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "50f476d7d8e84e8a81539a9a3549c1d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d0a96248ad549d6989d9cc4a58bc10f",
            "placeholder": "​",
            "style": "IPY_MODEL_ccdb06e71d874d1f8210776f1f74ea28",
            "value": " 0/1 [00:00&lt;?, ?it/s]"
          }
        },
        "5daec47eff3040788002690b9d9ff559": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fa45e83d15cb442fb7890f9ccedac956",
              "IPY_MODEL_a1f02abecc824b7b9f94ba2546ea7041",
              "IPY_MODEL_3e01a4e4ef104f42a255d072e865d0ec"
            ],
            "layout": "IPY_MODEL_496ec14697464ddf922fb72560efeabc"
          }
        },
        "601a23ba484145af8eec1946779a53d8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e0c64845c244720b529a238fb7f03fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2ce4571ca8c3486db82cf5111b49b979",
              "IPY_MODEL_d01cea5d86be431fa519656159469c88",
              "IPY_MODEL_50f476d7d8e84e8a81539a9a3549c1d4"
            ],
            "layout": "IPY_MODEL_601a23ba484145af8eec1946779a53d8"
          }
        },
        "87be8f11d7304eea98f48b2f49448a24": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91e2fea28b8e4386903bd5dbd39c9a2f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1f02abecc824b7b9f94ba2546ea7041": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91e2fea28b8e4386903bd5dbd39c9a2f",
            "max": 160000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f63757eee857429c83bf8bba81a36ff6",
            "value": 14326
          }
        },
        "bb4843917abc42d68b3df2fbb3d90519": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c202c1da2c3549b1b759e9bc8609ffcd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccdb06e71d874d1f8210776f1f74ea28": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d01cea5d86be431fa519656159469c88": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87be8f11d7304eea98f48b2f49448a24",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e718ea3aae714c89aaee1f1f79374cab",
            "value": 0
          }
        },
        "e718ea3aae714c89aaee1f1f79374cab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f63757eee857429c83bf8bba81a36ff6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fa45e83d15cb442fb7890f9ccedac956": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb4843917abc42d68b3df2fbb3d90519",
            "placeholder": "​",
            "style": "IPY_MODEL_29011e908581492ebcb84b39c77fcc06",
            "value": "  9%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
