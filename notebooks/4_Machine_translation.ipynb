{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jdasam/aat3020-2023/blob/main/notebooks/4_Machine_translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80c87a7a",
      "metadata": {
        "id": "80c87a7a"
      },
      "source": [
        "# Machine Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "518b10da",
      "metadata": {
        "id": "518b10da",
        "outputId": "92ae893c-94b5-4e2e-b9c6-6b631d415190",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CpsqOuuuB3I_PG5DbuqH1ssCFVerU46g\n",
            "To: /content/nia-aihub-korean-english.zip\n",
            "100% 290M/290M [00:06<00:00, 44.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Download dataset (originally from NIA AI-Hub)\n",
        "'''\n",
        "\n",
        "!gdown 1CpsqOuuuB3I_PG5DbuqH1ssCFVerU46g"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "192d9799",
      "metadata": {
        "id": "192d9799"
      },
      "outputs": [],
      "source": [
        "!unzip -q nia-aihub-korean-english.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ca202b9d",
      "metadata": {
        "scrolled": true,
        "id": "ca202b9d",
        "outputId": "e4876b92-6f77-42a9-8db0-ecee70cbbfb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-16e2164faec5>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'*.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'*.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0mkor_text_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_kor.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0meng_text_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_eng.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m         data = io.parse(\n\u001b[0m\u001b[1;32m    491\u001b[0m             \u001b[0msheet_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msheet_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, sheet_name, header, names, index_col, usecols, squeeze, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds)\u001b[0m\n\u001b[1;32m   1732\u001b[0m             \u001b[0mDataFrame\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mExcel\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1733\u001b[0m         \"\"\"\n\u001b[0;32m-> 1734\u001b[0;31m         return self._reader.parse(\n\u001b[0m\u001b[1;32m   1735\u001b[0m             \u001b[0msheet_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msheet_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1736\u001b[0m             \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, sheet_name, header, names, index_col, usecols, squeeze, dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0mfile_rows_needed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calc_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiprows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sheet_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msheet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_rows_needed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msheet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"close\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                 \u001b[0;31m# pyxlsb opens two TemporaryFiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_openpyxl.py\u001b[0m in \u001b[0;36mget_sheet_data\u001b[0;34m(self, sheet, convert_float, file_rows_needed)\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0mlast_row_with_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msheet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m             \u001b[0mconverted_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_float\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mconverted_row\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconverted_row\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m                 \u001b[0;31m# trim trailing empty elements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_openpyxl.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0mlast_row_with_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msheet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m             \u001b[0mconverted_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_float\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mconverted_row\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconverted_row\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m                 \u001b[0;31m# trim trailing empty elements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_openpyxl.py\u001b[0m in \u001b[0;36m_convert_cell\u001b[0;34m(self, cell, convert_float)\u001b[0m\n\u001b[1;32m    589\u001b[0m         )\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"\"\u001b[0m  \u001b[0;31m# compat with xlrd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTYPE_ERROR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openpyxl/cell/read_only.py\u001b[0m in \u001b[0;36mvalue\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "dataset_dir = Path('nia_korean_english')\n",
        "data_list = sorted(list(dataset_dir.glob('*.xlsx')))\n",
        "for path in data_list[:1]:\n",
        "  df = pd.read_excel(path)\n",
        "  kor_text_path = path.parent / (path.stem+'_kor.txt') \n",
        "  eng_text_path = path.parent / (path.stem+'_eng.txt') \n",
        "  with open(kor_text_path, 'w', encoding='utf8') as f:\n",
        "      f.write('\\n'.join(df['원문']))\n",
        "  with open(eng_text_path, 'w', encoding='utf8') as f:\n",
        "      f.write('\\n'.join(df['번역문']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1fbc2c73",
      "metadata": {
        "id": "1fbc2c73",
        "outputId": "f7cee7f6-e4d6-4acc-ebc0-ad7c867817d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('nia_korean_english/1_구어체(1).xlsx')]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "data_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6c767468",
      "metadata": {
        "id": "6c767468"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "dfs = [pd.read_excel(path) for path in data_list[:1]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2e9f316f",
      "metadata": {
        "id": "2e9f316f"
      },
      "outputs": [],
      "source": [
        "df = pd.concat(dfs, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"nia_korean_english/1_구어체(1)_kor.txt\", 'w', encoding='utf8') as f:\n",
        "    f.write('\\n'.join(df['원문']))\n"
      ],
      "metadata": {
        "id": "gKyXWsJjjrLR"
      },
      "id": "gKyXWsJjjrLR",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ff8b8786",
      "metadata": {
        "id": "ff8b8786",
        "outputId": "5aad7beb-6593-4054-fd47-00972008f706",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000          개, 돌고래류, 원숭이, 앵무새 일련의 음성 명령 또는 단어를 배울 수 있다.\n",
              " 10001                     개가 계속 딸꾹질을 한다면 문제가 있는 것이 틀림없습니다.\n",
              " 10002                                      개가 그걸 좋아할 것 같아.\n",
              " 10003                                      개가 정말 예쁘게 생겼네요.\n",
              " 10004                            개가 지저분하게 해 놓은 것을 청소할 거예요.\n",
              " 10005                           개가 짖는 소리 때문에 나는 방금 잠에서 깼어.\n",
              " 10006    개가 하는 행동의 의미를 알고 있는 것은 반려견과 서로를 이해할 수 있는 하나의 방...\n",
              " 10007         개개인들이 가까이 서로 붙어있으면, 그들은 서로 소통하고 많은 정보를 나눕니다.\n",
              " 10008    개개인의 사망 시점을 예측하기는 어려워도 어느 집단에서 일정 기간의 평균 사망자 수...\n",
              " 10009      개개인의 존재가 존중된 후에 제대로 된 society가 이루어질 수 있다고 생각해요.\n",
              " 10010    개교 이래 100년 만에 DJ들과 빠른 비트의 EDM 음악, 현란한 조명들은 저에게...\n",
              " 10011                     개구리는 뱀이 개구리를 먹는다는 것을 알고 어떻게 했나요?\n",
              " 10012                         개구리로 변한 그는 사람들로부터 괴롭힘을 당했어요.\n",
              " 10013    개구쟁이 성격을 지닌 캐릭터로 연령대가 10대 후반에서 20대 초반으로 사춘기를 겪...\n",
              " 10014                 개구쟁이처럼만 지낼 것 같아서 걱정이었는데 사진 보니 자랑스러워.\n",
              " 10015                                개국 기념일 날에는 불꾳 놀이를 해요.\n",
              " 10016         개나 소나 말고, 보고 싶은 것만 보고 보여주고 싶은 사람에게만 보여주고 살자.\n",
              " 10017               개나리 아파트에서는 배우 김윤석의 와이어 액션 장면만 촬영되었습니다.\n",
              " 10018                               개나리는 봄에 피는 꽃의 대표주자입니다.\n",
              " 10019         개념, 범주, 연상 의미 수준에서는, 모두 기타 오류가 가장 많이 산출했습니다.\n",
              " 10020                            개념을 검토해 보시고 의견 주시면 좋겠습니다.\n",
              " 10021                        개념을 설명한 후에 그 개념과 관련된 문제를 푸세요.\n",
              " 10022               개는 고양이와 달리 여기저기 숨지 않고 높은 곳에 올라가지 않습니다.\n",
              " 10023                              개는 산책하러 나가면 냄새를 계속 맡아요.\n",
              " 10024                 개는 상대방에게 자신의 우월감을 드러낼 때 꼬리를 곧게 세웁니다.\n",
              " 10025                             개는 신나게 뛰어놀다가 모래사장에 누워있어.\n",
              " 10026                             개는 엘리베이터 안에서 똥을 쌀 수도 있다.\n",
              " 10027    개는 자신이 가지고 있는 고기의 소중함을 생각하지 않고 다른 고기를 얻기 위해서 욕...\n",
              " 10028                            개는 충성심이 강해서 많은 사람들이 좋아해요.\n",
              " 10029                               개는 털갈이를 하므로 몸에 털이 빠져요.\n",
              " 10030                              개는 호의적이어서 사람 들을 잘 따릅니다.\n",
              " 10031                  개당 $110이며, 이동용 가방을 포함해서 주실 수 있으신지요?\n",
              " 10032                                 개당 납품 단가는 1.26달러입니다.\n",
              " 10033                               개도 사람처럼 의사소통을 할 수 있어요.\n",
              " 10034                             개들은 인간을 그들의 가족으로서 인식합니다.\n",
              " 10035                              개들은 종류가 다양하며 크기도 다양합니다.\n",
              " 10036                                개들은 좋은 후각 신경을 가지고 있다.\n",
              " 10037                     개들은 주인 들을 잘 따르고 우리에게 많은 도움을 줍니다.\n",
              " 10038                                  개들은 주인을 잘 따르는 것 같아.\n",
              " 10039                                    개떵이의 얼음 먹방이 재밌더라.\n",
              " 10040                            개를 데리고 산책하러 나가시면 얼마나 걸려요?\n",
              " 10041     개막 전시회가 전 세계적으로 경쟁력을 갖추면서, 이를 통해 기술 교류가 활발해졌습니다.\n",
              " 10042                          개미 무리는 많은 어려운 일들을 할 수 있습니다.\n",
              " 10043                          개미, 벌, 비둘기, 청어의 공통점이 무엇인가요?\n",
              " 10044                            개미는 무리지어 다니는 동물의 좋은 예입니다.\n",
              " 10045                             개미는 배설을 담당하는 기관이 배에 있어요.\n",
              " 10046                 개미는 자기에게 기대되어 진 모든 일을 하면서 삶을 살 것입니다.\n",
              " 10047                        개미는 절대로 자신의 길을 적에게로 돌리지 않습니다.\n",
              " 10048                          개미들의 경우에는 리더가 없는 것 처럼 보입니다.\n",
              " 10049             개미와 비둘기는 서로 도움을 줬을 때 은혜를 갚을 줄 알고 고마워했어요.\n",
              " Name: 원문, dtype: object,\n",
              " 10000    Dogs, like dolphins and apes and parrots, can ...\n",
              " 10001    If the dog still keeps on hiccuping, then it m...\n",
              " 10002                                The dog will love it.\n",
              " 10003                                He's a beautiful dog.\n",
              " 10004                  I'm going to clean up the dog mess.\n",
              " 10005    I just woke up from my luggage because of the ...\n",
              " 10006    Knowing the meanings of the behaviors of pet d...\n",
              " 10007    When individuals stay closely together, they c...\n",
              " 10008    It is difficult to predict when each individua...\n",
              " 10009    I believe a true society can be achieved only ...\n",
              " 10010    For the first time in 100years since its incep...\n",
              " 10011    What did frog do after knowing that a snake ea...\n",
              " 10012     He turned into a frog and was bullied by people.\n",
              " 10013    The character is a naughty personality who is ...\n",
              " 10014    I thought he was always going get in trouble, ...\n",
              " 10015    There are fireworks on the national foundation...\n",
              " 10016    Not anyone or anything, look what you want to ...\n",
              " 10017    The only scene filmed at Gaenari Apartment was...\n",
              " 10018    The forsythia is a representative flower for t...\n",
              " 10019    In concept, extent, and associated meaning lev...\n",
              " 10020    Please review the concept and give us your opi...\n",
              " 10021    After explaining the concept, solve the proble...\n",
              " 10022    Dogs, different from cats, do not hide and cli...\n",
              " 10023    When the dogs go out for a walk, they keep sniff.\n",
              " 10024    Dogs usually straighten their tails to show th...\n",
              " 10025    The dog was running around and lying on the sand.\n",
              " 10026                     Dogs might shit in the elevator.\n",
              " 10027    The dog didn't think about the value of the me...\n",
              " 10028    Many people love dogs because it has strong lo...\n",
              " 10029             Dogs lose their hair because they do it.\n",
              " 10030         Dogs docile so they follow human with favor.\n",
              " 10031    It's $110 per unit and could you give them to ...\n",
              " 10032              The price of each unit is 1.26 dollars.\n",
              " 10033                    Dogs can communicate like humans.\n",
              " 10034           Dogs see humans as part of their families.\n",
              " 10035    There are many different kinds of dogs that va...\n",
              " 10036                     Dogs have good olfactory nerves.\n",
              " 10037    Dogs are loyal to their owners and give them l...\n",
              " 10038               Dogs seem to follow their owners well.\n",
              " 10039                The Gaeddeong's ice meokbang was fun.\n",
              " 10040               How long do you spend walking the dog?\n",
              " 10041    As opening exhibitions became competitive all ...\n",
              " 10042    Groups of ants can accomplish many difficult t...\n",
              " 10043    What do ants, bees, pigeons and herring all ha...\n",
              " 10044        Ants a good example of such bandying animals.\n",
              " 10045    The ants have the excretion organs in the abdo...\n",
              " 10046    Ants will live their lives, doing all the thin...\n",
              " 10047            Ants never bend their course to an enemy.\n",
              " 10048    In the case of ants, there appears to be no le...\n",
              " 10049    The ant and the dove appreciated each others f...\n",
              " Name: 번역문, dtype: object)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "df['원문'][10000:10050], df['번역문'][10000:10050]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3df6e584",
      "metadata": {
        "id": "3df6e584"
      },
      "source": [
        "## Huggingface Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers tokenizers"
      ],
      "metadata": {
        "id": "xreS5uZdibIl",
        "outputId": "79de1110-4be5-4317-d466-fd6b2a9d84e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "xreS5uZdibIl",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.13.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 transformers-4.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "6c6f1769",
      "metadata": {
        "id": "6c6f1769",
        "outputId": "ea10c67f-d509-4dcb-b400-5a3e31df5ba7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hugging_eng_32000/vocab.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "from tokenizers import BertWordPieceTokenizer\n",
        "tokenizer = BertWordPieceTokenizer(strip_accents=False, lowercase=False)\n",
        "\n",
        "corpus_file   =  [str(path.parent / (path.stem + '_kor.txt')) for path in data_list[:2]]\n",
        "# output_dir   = Path('hugging_kor_%d'%(vocab_size))\n",
        "en_corpus_file   =  [str(path.parent / (path.stem + '_eng.txt')) for path in data_list[:2]]\n",
        "# output_dir   = Path('hugging_eng_%d'%(vocab_size))\n",
        "\n",
        "vocab_size    = 32000  # Number of maximum size of the vocabulary\n",
        "limit_alphabet= 6000   \n",
        "output_dir    = Path('hugging_kor_%d'%(vocab_size))\n",
        "en_output_dir = Path('hugging_eng_%d'%(vocab_size))\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "en_output_dir.mkdir(exist_ok=True)\n",
        "min_frequency = 5 \n",
        "\n",
        "tokenizer.train(files=corpus_file,\n",
        "               vocab_size=vocab_size,\n",
        "               min_frequency=min_frequency,\n",
        "               limit_alphabet=limit_alphabet, \n",
        "               show_progress=True)\n",
        "\n",
        "tokenizer.save_model(str(output_dir))\n",
        "\n",
        "en_tokenizer = BertWordPieceTokenizer(strip_accents=False, lowercase=False)\n",
        "en_tokenizer.train(files=en_corpus_file,\n",
        "                vocab_size=vocab_size,\n",
        "                min_frequency=min_frequency,\n",
        "                limit_alphabet=limit_alphabet,\n",
        "                show_progress=True)\n",
        "en_tokenizer.save_model(str(en_output_dir))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_file"
      ],
      "metadata": {
        "id": "szHQEAeBjZia",
        "outputId": "c5cf230f-619b-4cfd-ff14-e36371a9cced",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "szHQEAeBjZia",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['nia_korean_english/1_구어체(1)_kor.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "410eb97a",
      "metadata": {
        "scrolled": true,
        "id": "410eb97a",
        "outputId": "bafa4b35-49e2-49c1-f44a-b0b6109a0c9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] 나는 친구에게 그 철학자의 책을 선물해 주겠다고 말했습니다. [SEP]\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizerFast\n",
        "\n",
        "tokenizer_src = BertTokenizerFast.from_pretrained('hugging_kor_32000',\n",
        "                                                       strip_accents=False,\n",
        "                                                       lowercase=False) \n",
        "tokenizer_tgt = BertTokenizerFast.from_pretrained('hugging_eng_32000',\n",
        "                                                       strip_accents=False,\n",
        "                                                       lowercase=False) \n",
        "\n",
        "tokenized_data = tokenizer_src(df['원문'].iloc[10])\n",
        "print(tokenizer_src.decode(tokenized_data['input_ids']))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data['input_ids']"
      ],
      "metadata": {
        "id": "q9GJ7cifk5tg",
        "outputId": "49d14311-88b0-4c15-cc3a-96b4bd33ad54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "q9GJ7cifk5tg",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 3390, 5817, 245, 12734, 4135, 3972, 15943, 26023, 6168, 15, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "340f55c0",
      "metadata": {
        "id": "340f55c0",
        "outputId": "9fbdb331-bdc9-48fc-ee7f-64aaab7e7004",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 3390, 974, 2346, 11185, 10758, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "tokenized_ids = tokenizer_src(\"나는 서강대학교에 다닙니다\")['input_ids']\n",
        "tokenized_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "5b3ea831",
      "metadata": {
        "id": "5b3ea831",
        "outputId": "49c56cb6-d02e-48f8-9044-635073a1c67b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] 나는 서강대학교에 다닙니다 [SEP]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "tokenizer_src.decode(tokenized_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26964dcb",
      "metadata": {
        "id": "26964dcb"
      },
      "source": [
        "## Divide Train / Validate/ Test Set\n",
        "- using `np.random.choice`\n",
        "    - To always get the same random shuffling result, you have to use `np.random.seed()`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "B2zJgvXPlh95",
        "outputId": "4239de02-ca5b-4fb5-c1f1-84dc6784de19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        }
      },
      "id": "B2zJgvXPlh95",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           SID                                                 원문  \\\n",
              "0            1  'Bible Coloring'은 성경의 아름다운 이야기를 체험 할 수 있는 컬러링 ...   \n",
              "1            2                                       씨티은행에서 일하세요?   \n",
              "2            3              푸리토의 베스트셀러는 해외에서 입소문만으로 4차 완판을 기록하였다.   \n",
              "3            4   11장에서는 예수님이 이번엔 나사로를 무덤에서 불러내어 죽은 자 가운데서 살리셨습니다.   \n",
              "4            5     6.5, 7, 8 사이즈가 몇 개나 더 재입고 될지 제게 알려주시면 감사하겠습니다.   \n",
              "...        ...                                                ...   \n",
              "199995  199996                               나는 먼저 청소기로 바닥을 밀었어요.   \n",
              "199996  199997                             나는 먼저 팀 과제를 하고 놀러 갔어요.   \n",
              "199997  199998                              나는 비 같은 멋진 연예인을 좋아해요.   \n",
              "199998  199999                           나는 멋진 자연 경치를 보고 눈물을 흘렸어.   \n",
              "199999  200000                               나는 멋진 중학교 생활을 기대합니다.   \n",
              "\n",
              "                                                      번역문  \n",
              "0       Bible Coloring' is a coloring application that...  \n",
              "1                             Do you work at a City bank?  \n",
              "2       PURITO's bestseller, which recorded 4th rough ...  \n",
              "3       In Chapter 11 Jesus called Lazarus from the to...  \n",
              "4       I would feel grateful to know how many stocks ...  \n",
              "...                                                   ...  \n",
              "199995                First of all, I vacuumed the floor.  \n",
              "199996  I did the team assignment first and went out t...  \n",
              "199997                 I like cool entertainer like Rain.  \n",
              "199998                I cried seeing the amazing scenery.  \n",
              "199999  I look forward to a great middle school experi...  \n",
              "\n",
              "[200000 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-01663799-264f-476a-8fe9-83420e973950\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SID</th>\n",
              "      <th>원문</th>\n",
              "      <th>번역문</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>'Bible Coloring'은 성경의 아름다운 이야기를 체험 할 수 있는 컬러링 ...</td>\n",
              "      <td>Bible Coloring' is a coloring application that...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>씨티은행에서 일하세요?</td>\n",
              "      <td>Do you work at a City bank?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>푸리토의 베스트셀러는 해외에서 입소문만으로 4차 완판을 기록하였다.</td>\n",
              "      <td>PURITO's bestseller, which recorded 4th rough ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>11장에서는 예수님이 이번엔 나사로를 무덤에서 불러내어 죽은 자 가운데서 살리셨습니다.</td>\n",
              "      <td>In Chapter 11 Jesus called Lazarus from the to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>6.5, 7, 8 사이즈가 몇 개나 더 재입고 될지 제게 알려주시면 감사하겠습니다.</td>\n",
              "      <td>I would feel grateful to know how many stocks ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199995</th>\n",
              "      <td>199996</td>\n",
              "      <td>나는 먼저 청소기로 바닥을 밀었어요.</td>\n",
              "      <td>First of all, I vacuumed the floor.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199996</th>\n",
              "      <td>199997</td>\n",
              "      <td>나는 먼저 팀 과제를 하고 놀러 갔어요.</td>\n",
              "      <td>I did the team assignment first and went out t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199997</th>\n",
              "      <td>199998</td>\n",
              "      <td>나는 비 같은 멋진 연예인을 좋아해요.</td>\n",
              "      <td>I like cool entertainer like Rain.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199998</th>\n",
              "      <td>199999</td>\n",
              "      <td>나는 멋진 자연 경치를 보고 눈물을 흘렸어.</td>\n",
              "      <td>I cried seeing the amazing scenery.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199999</th>\n",
              "      <td>200000</td>\n",
              "      <td>나는 멋진 중학교 생활을 기대합니다.</td>\n",
              "      <td>I look forward to a great middle school experi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200000 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-01663799-264f-476a-8fe9-83420e973950')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-01663799-264f-476a-8fe9-83420e973950 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-01663799-264f-476a-8fe9-83420e973950');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(df)"
      ],
      "metadata": {
        "id": "boOZlijtlpgv",
        "outputId": "9b938fbc-6d5f-4d43-b04e-fd6f57df8bea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "boOZlijtlpgv",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200000"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[1]"
      ],
      "metadata": {
        "id": "NdDy0EHNlv60",
        "outputId": "5f1314d2-e9e9-4b00-f5e1-5f8a252be2d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "NdDy0EHNlv60",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SID                              2\n",
              "원문                    씨티은행에서 일하세요?\n",
              "번역문    Do you work at a City bank?\n",
              "Name: 1, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "b088f662",
      "metadata": {
        "id": "b088f662",
        "outputId": "d6840236-f92a-443c-8adb-3438651d5c7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([2, 29361, 3393, 1274, 3621, 32, 3],\n",
              " [2, 336, 267, 425, 339, 68, 1619, 1601, 34],\n",
              " [336, 267, 425, 339, 68, 1619, 1601, 34, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "class Dataset:\n",
        "  def __init__(self, df, src_tokenizer, tgt_tokenizer):\n",
        "    self.data = df\n",
        "    self.src_tokenizer = src_tokenizer\n",
        "    self.tgt_tokenizer = tgt_tokenizer\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    selected_row = self.data.iloc[idx]\n",
        "    source = selected_row['원문']\n",
        "    target = selected_row['번역문']\n",
        "\n",
        "    source_enc = self.src_tokenizer(source)['input_ids']\n",
        "    target_enc = self.tgt_tokenizer(target)['input_ids']\n",
        "\n",
        "    return source_enc, target_enc[:-1], target_enc[1:]\n",
        "  \n",
        "dataset = Dataset(df, tokenizer_src, tokenizer_tgt)\n",
        "\n",
        "dataset[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba4a138a",
      "metadata": {
        "id": "ba4a138a"
      },
      "source": [
        "## Define Dataset\n",
        "- Each datasample has to return source sentence and target sentence\n",
        "- You need a Tokenizer to get the tokenized result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76933a49",
      "metadata": {
        "id": "76933a49"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4a9f0ec",
      "metadata": {
        "id": "f4a9f0ec"
      },
      "source": [
        "## Define Collate function\n",
        "- After implementing Dataset, we have to declare DataLoader that groups several training samples as a single batch\n",
        "- However, we cannot batchify the melodies in straightforward way, because the length of each melody is different\n",
        "- In this problem, you will learn about how to handle sequences of different length as a batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "456ff9f2",
      "metadata": {
        "id": "456ff9f2"
      },
      "outputs": [],
      "source": [
        "\n",
        "'''\n",
        "This cell will make error, because the length of each sample is different to each other\n",
        "'''\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=8)\n",
        "batch = next(iter(train_loader))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afc533f7",
      "metadata": {
        "id": "afc533f7"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "To handle that problem, you have to make your collate function \n",
        "'''\n",
        "def your_collate_function(raw_batch):\n",
        "  '''\n",
        "  You can make your own function to handle the batch\n",
        "  '''\n",
        "  \n",
        "  return raw_batch[0] # This returns the first melody of each batch. So it will avoid the error, but it doesn't do proper batchifying\n",
        "\n",
        "batch_size = 8\n",
        "raw_batch = [train_set[i] for i in range(batch_size)] # This is the input for the collate function\n",
        "batch = your_collate_function(raw_batch)\n",
        "\n",
        "'''\n",
        "This is what the 'collate_fn' does in DataLoader\n",
        "'''\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, collate_fn=your_collate_function)\n",
        "batch_by_loader = next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22db0716",
      "metadata": {
        "id": "22db0716"
      },
      "source": [
        "#### Pad Sequence and Pack Sequence\n",
        "In PyTorch, there are two ways to batchify a group of sequence with different length.\n",
        "- `torch.nn.utils.rnn.pad_sequence`\n",
        "    - This function takes list of tensors with different length and return padded sequence\n",
        "    - Padding is adding some constant number as a PAD token to match the length of short sequence to the maximum length\n",
        "        - e.g. If there are sequence of length (3,7,4), we can add 4 zeros to sequence with length 3, 3 zeros to sequence with length 4 to make them length 7\n",
        "    - In default, we use 0 for padding (zero padding)\n",
        "    - The result \n",
        "- `torch.nn.utils.rnn.pack_sequence`\n",
        "    - pad_sequence "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c8cce88",
      "metadata": {
        "id": "1c8cce88"
      },
      "source": [
        "Cells below show the example of `pad_sequence`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe229f85",
      "metadata": {
        "id": "fe229f85"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence, pack_sequence, PackedSequence\n",
        "short = torch.arange(3, -1, -1).float() # [3, 2, 1, 0]\n",
        "long = torch.arange(27,19, -1).float()\n",
        "middle = torch.arange(15,9, -1).float()\n",
        "\n",
        "pad_sequence([short, long, middle], batch_first=False)  # T x N "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b2a0920",
      "metadata": {
        "id": "0b2a0920"
      },
      "outputs": [],
      "source": [
        "# Default value of batch_first in pad_sequence is False.\n",
        "# So you have to always be careful not to miss batch_first=True in pad_sequence, if you use batch_first=True for your RNN layer.\n",
        "pad_sequence([short, long, middle], batch_first=True)  # N x T "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fb5e0dd",
      "metadata": {
        "id": "4fb5e0dd"
      },
      "source": [
        "1) However, the problem is that you can't figure out whether the 0 at the end of each sequence is a padded one, or was included in the original sequence\n",
        "- e.g. `[2, 3, 4, 3, 0]` becomes `[ 2,  3,  4,  5,  0,  0,  0]`. Now we don't know how many zeros were added for padding\n",
        "\n",
        "2) Also, if you run RNN for this padded sequence, RNN will calculate for the padded part also.\n",
        "- RNN doesn't know whether it is padded data, or existing data\n",
        "- This makes computation slower\n",
        "\n",
        "3) If you want to use bi-directional, which also reads the sequence from backward, paddings can make the result different.\n",
        "\n",
        "To solve this issue, we use PackedSequence, by using `pack_sequence`/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69086d43",
      "metadata": {
        "id": "69086d43"
      },
      "outputs": [],
      "source": [
        "packed_sequence = pack_sequence([short, long, middle], enforce_sorted=False)\n",
        "packed_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cd50747",
      "metadata": {
        "id": "4cd50747"
      },
      "source": [
        "`PackedSequence` has `data` and `batch_sizes`\n",
        "- `data` contains the flattened value of given batch\n",
        "    - To optimize the computation, the sequences have to be sorted by descending of length\n",
        "- `batch_sizes` represents how many valid batch sample exists for each time step\n",
        "    - `[3, 3, 3, 2, 2, 1, 1]` means that there are 3 sequences for first three time steps, and then 2 sequences for next two steps, and then only 1 sequence for next two steps.\n",
        "- `sorted_indices` shows how the sorted sequences can be converted to original order.\n",
        "    - `[1,2,0]` means that \n",
        "        - the 0th sequence in the sorted sequences (the longest one) was indexed as 1 in the original input batch\n",
        "        - the 1st sequence in the sorted sequences (`middle`) was indexed as 2 in the original input batch\n",
        "        - the 2nd sequence in the sorted sequences (`short`) was index as 0 in the original input batch\n",
        "- `unsorted_indices` shows how the original sequences are sorted.\n",
        "    - `[2,0,1]` means that\n",
        "        - the 0th sequence in the original input was sorted as 2nd in the sorted sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0500f200",
      "metadata": {
        "id": "0500f200"
      },
      "outputs": [],
      "source": [
        "rnn_layer = nn.GRU(1, 1)\n",
        "packed_sequence = pack_sequence([short.unsqueeze(1), long.unsqueeze(1), middle.unsqueeze(1)], enforce_sorted=False)\n",
        "out, last_hidden = rnn_layer(packed_sequence)\n",
        "\n",
        "print(f\"Type of output of RNN for PackedSequence: {type(out)}\")\n",
        "print(f\"Type of last_hidden of RNN for PackedSequence: {type(last_hidden)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f2e3e44",
      "metadata": {
        "id": "8f2e3e44"
      },
      "source": [
        "- RNN or its family of PyTorch can automatically handle `PackedSequence`\n",
        "- However, other layers like `nn.Embedding` or `nn.Linear` cannot take `PackedSequence` as its input\n",
        "- There are two ways to feed `PackedSequence` to these layers\n",
        "    - First, convert PackedSequence to ordinary torch.Tensor by `torch.nn.utils.rnn.pad_packed_sequence`\n",
        "        - This will convert PackedSequence to a tensor of sequneces with same length but different padding\n",
        "    - The other way is to feed only PackedSequence.data, and then declaring new PackedSequence with the output as `data`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b08620c8",
      "metadata": {
        "id": "b08620c8"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This will make error, because other layers cannot handle PackedSequence\n",
        "'''\n",
        "test_linear_layer = nn.Linear(in_features=1, out_features=2)\n",
        "test_linear_layer(packed_sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac9efa09",
      "metadata": {
        "id": "ac9efa09"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "One way to to this is using torch.nn.utils.rnn.pad_packed_sequence to convert PackedSequence to ordinary tensor\n",
        "'''\n",
        "\n",
        "from torch.nn.utils.rnn import pad_packed_sequence\n",
        "padded_sequence, batch_lengths = pad_packed_sequence(packed_sequence)\n",
        "print(f'The padded sequence generated from packed sequence (squeezed for printing): \\n {padded_sequence.squeeze()}')\n",
        "print(f'\"pad_packed_sequence\" also returns \"batch_lengths\", to clarify the original length before the padding: \\n {batch_lengths}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e302fe4",
      "metadata": {
        "id": "8e302fe4"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Now you can feed padded sequence to linear layer.\n",
        "'''\n",
        "\n",
        "linear_output = test_linear_layer(padded_sequence)\n",
        "print(f\"Output of feeding padded_sequence to a linear layer: {linear_output}\")\n",
        "print(\"Caution that it returns non-zero values for timestep with zero padding, because linear layer has a bias\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1c0e150",
      "metadata": {
        "id": "b1c0e150"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "You can make the output as a PackedSequence, by using torch.nn.utils.rnn.pack_padded_sequence\n",
        "'''\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "re_packed_sequence = pack_padded_sequence(linear_output, batch_lengths, enforce_sorted=False)\n",
        "re_packed_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22117c41",
      "metadata": {
        "id": "22117c41"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Another way to do it is using PackedSequence.data\n",
        "'''\n",
        "\n",
        "linear_out_pack = test_linear_layer(packed_sequence.data)\n",
        "packed_sequence_after_linear = PackedSequence(linear_out_pack, packed_sequence.batch_sizes, packed_sequence.sorted_indices, packed_sequence.unsorted_indices)\n",
        "packed_sequence_after_linear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81dd119c",
      "metadata": {
        "id": "81dd119c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "357c61f4",
      "metadata": {
        "id": "357c61f4"
      },
      "source": [
        "## Define Model\n",
        "![image](https://raw.githubusercontent.com/tensorflow/nmt/master/nmt/g3doc/img/seq2seq.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0af0ca29",
      "metadata": {
        "id": "0af0ca29"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "13ffb21b",
      "metadata": {
        "id": "13ffb21b"
      },
      "source": [
        "## Define Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64a3758c",
      "metadata": {
        "id": "64a3758c"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "class Trainer:\n",
        "  def __init__(self, model, optimizer, loss_fn, train_loader, valid_loader, device):\n",
        "    self.model = model\n",
        "    self.optimizer = optimizer\n",
        "    self.loss_fn = loss_fn\n",
        "    self.train_loader = train_loader\n",
        "    self.valid_loader = valid_loader\n",
        "    \n",
        "    self.model.to(device)\n",
        "    \n",
        "    self.best_valid_accuracy = 0\n",
        "    self.device = device\n",
        "    \n",
        "    self.training_loss = []\n",
        "    self.validation_loss = []\n",
        "    self.validation_acc = []\n",
        "\n",
        "  def save_model(self, path='imdb_sentiment_model.pt'):\n",
        "    torch.save({'model':self.model.state_dict(), 'optim':self.optimizer.state_dict()}, path)\n",
        "    \n",
        "  def train_by_num_epoch(self, num_epochs):\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "      self.model.train()\n",
        "      for batch in self.train_loader:\n",
        "        loss_value = self._train_by_single_batch(batch)\n",
        "        self.training_loss.append(loss_value)\n",
        "      self.model.eval()\n",
        "      validation_loss, validation_acc = self.validate()\n",
        "      self.validation_loss.append(validation_loss)\n",
        "      self.validation_acc.append(validation_acc)\n",
        "      \n",
        "      if validation_acc > self.best_valid_accuracy:\n",
        "        print(f\"Saving the model with best validation accuracy: Epoch {epoch+1}, Acc: {validation_acc:.4f} \")\n",
        "        self.save_model('imdb_sentiment_model_best.pt')\n",
        "      else:\n",
        "        self.save_model('imdb_sentiment_model_last.pt')\n",
        "      self.best_valid_accuracy = max(validation_acc, self.best_valid_accuracy)\n",
        "\n",
        "      \n",
        "  def _train_by_single_batch(self, batch):\n",
        "    '''\n",
        "    This method updates self.model's parameter with a given batch\n",
        "    \n",
        "    batch (tuple): (batch_of_input_text, batch_of_label)\n",
        "    \n",
        "    You have to use variables below:\n",
        "    \n",
        "    self.model (SentimentModel/torch.nn.Module): A neural network model\n",
        "    self.optimizer (torch.optim.adam.Adam): Adam optimizer that optimizes model's parameter\n",
        "    self.loss_fn (function): function for calculating BCE loss for a given prediction and target\n",
        "    self.device (str): 'cuda' or 'cpu'\n",
        "\n",
        "    output: loss (float): Mean binary cross entropy value for every sample in the training batch\n",
        "    The model's parameters, optimizer's steps has to be updated inside this method\n",
        "\n",
        "    TODO: Complete this method \n",
        "    '''\n",
        "\n",
        "    \n",
        "    return\n",
        "\n",
        "    \n",
        "  def validate(self, external_loader=None):\n",
        "    '''\n",
        "    This method calculates accuracy and loss for given data loader.\n",
        "    It can be used for validation step, or to get test set result\n",
        "    \n",
        "    input:\n",
        "      data_loader: If there is no data_loader given, use self.valid_loader as default.\n",
        "      \n",
        "    \n",
        "    output: \n",
        "      validation_loss (float): Mean Binary Cross Entropy value for every sample in validation set\n",
        "      validation_accuracy (float): Mean Accuracy value for every sample in validation set\n",
        "      \n",
        "    TODO: Complete this method \n",
        "\n",
        "    '''\n",
        "    \n",
        "    ### Don't change this part\n",
        "    if external_loader and isinstance(external_loader, DataLoader):\n",
        "      loader = external_loader\n",
        "      print('An arbitrary loader is used instead of Validation loader')\n",
        "    else:\n",
        "      loader = self.valid_loader\n",
        "      \n",
        "    self.model.eval()\n",
        "    \n",
        "    '''\n",
        "    Write your code from here, using loader, self.model, self.loss_fn.\n",
        "    '''"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}